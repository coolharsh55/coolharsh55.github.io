[{"id": 34, "title": "Setting up notifications via Telegram", "authors": "1", "date_created": "2018-02-28 00:08:41", "date_published": "2018-02-28 00:08:47", "date_updated": "2018-03-03 23:35:55", "is_published": "1", "short_description": "Automating notification of new appointments through the Telegram app", "tags": "86,137,206,204,207,82,208,205", "slug": "setting-up-notifications-via-telegram", "body_type": "markdown", "body_text": "<p>This post details how the <a href=\"https://harshp.com/dev/utils/gnib-appointments/\">GNIB/VISA appointment utility</a> was developed to display appointments on the website as well as how this was extended to create a <a href=\"https://harshp.com/dev/projects/gnib-appointments/appointment-notifications-using-telegram-app/\">Telegram bot</a> to get notifications on all platforms. This can be roughly partitioned into three parts - efficient retrieval of appointments data, storing them into the database, building the bot for notifications</p>\n<h2 id=\"retrieving-notifications-using-async\">Retrieving notifications using <code>async</code></h2>\n<blockquote>\n<p>This describes the approach to the code in <a href=\"https://github.com/coolharsh55/harshp.com/blob/dev/harshp_com/scripts/gnib_appointments_async.py\">gnib_appointments_async.py</a></p>\n</blockquote>\n<p>Getting the <code>json</code> response and the appointment dates is pretty easy based on the scripts, however, to do this efficiently requires some thought. Since there are a lot of requests here, executing them in parallel is the best way to go about it. Threads are one way, but they are expensive to manage and run. Since python now features asynchronous co-routines, these are the best candidates for the task. The basic idea is, by declaring the part as a co-routine, it will be run with other co-routines in a time efficient manner. The system will intermittently check if the responses have returned results, and if not, will execute other code in the interim. This allows many tasks that are waiting to be run in parallel. Based on this, our basic model is:</p>\n<pre class=\"codehilite\"><code class=\"language-python\">async def task(url):\n    download(url)\n\nasync def download_urls():\n    urls = [ ... ]\n    completed, pending = await asyncio.wait(\n        [task(url) for url in urls])\n    results = [task.result() for task in completed]\n    return results\n\nevent_loop = asyncio.get_event_loop()\nevent_loop.run_until_complete(download_urls())\nevent_loop.close()</code></pre>\n\n\n<p>This gets an event loop which is like a thread that keeps tracks of co-routines and their results and will download all urls in asynchronous fashion, which makes them appear as if they are executing in parallel. That's one way of doing this. Another is to wait on the actual download instead of the function where the download is happening. This is done as:</p>\n<pre class=\"codehilite\"><code class=\"language-python\">async def fetch(url, params):\n    async with ClientSession() as session:\n        async with session.get(url, params) as response:\n            data = await response.text()\n            return data</code></pre>\n\n\n<p>The important bit is to breakdown the task into components that can be run in parallel and then to call them as separate functions. In the case here, if all urls to be downloaded were within one function, then it cannot be run in parallel as each url will be wait to be downloaded as the previous one hasn't been completed. Instead, if these are called in separate statements as part of a function, then they can be run as separate function calls in parallel, or in this case as <code>async-await</code> calls.</p>\n<h2 id=\"database-models\">Database models</h2>\n<p>To persist the appointments, some form of storage is required. In earlier iterations, I simply saved them in memory, or wrote them to a file. However, for per-minute appointments, it isn't feasible, and some form of a database system is better. I already use PostgreSQL for my website, and Redis for a job queue. So I used both in this case for the purposes of storing appointments.</p>\n<p>The first time the appointments are retrieved, they are stored in Redis (key-value) so that other jobs and processes can immediately see the result. This allows the website to display the latest updates while other jobs are processed in the background. Storing stuff in redis is pretty simple, you simply set the value of a key, and that is it. To make things easier, the data is stored and retrieved as <code>JSON</code>. Retrieving it is simple as well.</p>\n<pre class=\"codehilite\"><code class=\"language-python\"> gnib_appointments = {\n        'gnib_Study_New': json.loads(kvstore.get('gnib_Study_New')),\n        'gnib_Study_Renewal': json.loads(kvstore.get('gnib_Study_Renewal')),\n        'gnib_Work_New': json.loads(kvstore.get('gnib_Work_New')),\n        'gnib_Work_Renewal': json.loads(kvstore.get('gnib_Work_Renewal')),\n        'gnib_Other_New': json.loads(kvstore.get('gnib_Other_New')),\n        'gnib_Other_Renewal': json.loads(kvstore.get('gnib_Other_Renewal')),\n        'visa_I': json.loads(kvstore.get('visa_I')),\n        'visa_F': json.loads(kvstore.get('visa_F')),\n        'last_update': kvstore.get('gnib_last_run'),\n}</code></pre>\n\n\n<p>For longer persistence, the appointments are stored to the PostgreSQL database as instance of the <code>GNIBAppointment</code> and <code>VisaAppointment</code> classes.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">class GNIBAppointment(AppointmentSlot):\n    '''GNIB Appointment'''\n    CATEGORY_STUDY = 'Study'\n    CATEGORY_WORK = 'Work'\n    CATEGORY_OTHER = 'Other'\n    CATEGORIES = (\n        (CATEGORY_STUDY, 'GNIB Study Appointment'),\n        (CATEGORY_WORK, 'GNIB Work Appointment'),\n        (CATEGORY_OTHER, 'GNIB Other Appointment'),\n    )\n    CATEGORY_TYPE_NEW = 'New'\n    CATEGORY_TYPE_RENEWAL = 'Renewal'\n    CATEGORY_TYPES = (\n        (CATEGORY_TYPE_NEW, 'GNIB New Appointment'),\n        (CATEGORY_TYPE_RENEWAL, 'GNIB Renewal Appointment'),\n    )\n\n    category = models.CharField(\n        max_length=8, db_index=True, choices=CATEGORIES)\n    category_type = models.CharField(\n        max_length=8, db_index=True, choices=CATEGORY_TYPES)\n\nclass VisaAppointment(AppointmentSlot):\n    '''Visa Appointment'''\n    CATEGORY_INDIVIDUAL = 'I'\n    CATEGORY_FAMILY = 'F'\n    CATEGORIES = (\n        (CATEGORY_INDIVIDUAL, 'Visa Individual Appointment'),\n        (CATEGORY_FAMILY, 'Visa Family Appointment'),\n    )\n\n    category = models.CharField(\n        max_length=8, db_index=True, choices=CATEGORIES)</code></pre>\n\n\n<p>To keep the responses, all appointment data retrieved from the servers is stored in to the database as well.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">class APIResponse(models.Model):\n    '''Stores raw JSON response from GNIB API'''\n    added_on = models.DateTimeField(auto_now_add=True)\n    json = JSONField()</code></pre>\n\n\n<h2 id=\"telegram-bot\">Telegram Bot</h2>\n<blockquote>\n<p>This describes code in <a href=\"https://github.com/coolharsh55/harshp.com/blob/dev/harshp_com/apps/jobs/gnib_telegram_bot.py\">gnib_telegram_bot.py</a></p>\n</blockquote>\n<p>Telegram makes it super simple to build bots. You get an API key and then you interact with your bot by passing the key as part of your request. Then you register your message handlers, and keep checking for messages every once in a while. I used the <a href=\"https://python-telegram-bot.org/\"><code>python telegram bot</code></a> library to interact with Telegram. Their documentation was easy to pick up on, and the bot itself is straightforward to start with. Additionally, it features a job queue so I can add jobs as well.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">updater = Updater(token=TELEGRAM_API_KEY)\ndispatcher = updater.dispatcher\ndispatcher.add_handler(CommandHandler('start', start, pass_job_queue=True))\nupdater.start_polling()\nupdater.idle()</code></pre>\n\n\n<p>Telegram makes it so that bots can only respond to conversations that have already been started by someone. To store the data, I persist a chat in a database model along with notification preferences such as date filters for GNIB and Visa appointments.</p>\n<blockquote>\n<p>This describes code for <a href=\"https://github.com/coolharsh55/harshp.com/blob/dev/harshp_com/apps/models/gnib_telegram_bot.py\">database models</a></p>\n</blockquote>\n<pre class=\"codehilite\"><code class=\"language-python\">class TelegramUser(models.Model):\n    '''Telegram User\n    This is the user that interacts with the bot.\n    '''\n    chat_id = models.CharField(max_length=64, db_index=True, primary_key=True)\n    appointment_gnib = models.CharField(\n        max_length=64, db_index=True, choices=GNIB_APPOINTMENT_TYPES,\n        blank=True, null=True)\n    gnib_filter_date_start = models.DateField(blank=True, null=True)\n    gnib_filter_date_end = models.DateField(blank=True, null=True)\n    appointment_visa = models.CharField(\n        max_length=64, db_index=True, choices=VISA_APPOINTMENT_TYPES,\n        blank=True, null=True)\n    visa_filter_date_start = models.DateField(blank=True, null=True)\n    visa_filter_date_end = models.DateField(blank=True, null=True)</code></pre>\n\n\n<p>To actually send the notifications, after each retrieval, it's a simple database call;</p>\n<blockquote>\n<p>Code in <a href=\"https://github.com/coolharsh55/harshp.com/blob/dev/harshp_com/apps/jobs/gnib_telegram_notifications.py\">gnib_telegram_notifications.py</a></p>\n</blockquote>\n<pre class=\"codehilite\"><code class=\"language-python\">async def job():\n    # retrieve added appointment for every type from database\n    gnib_appointments = {\n        ...\n    }\n    visa_appointments = {\n        ...\n    }\n    last_update = kvstore.get('gnib_last_run')\n\n    # retrieve users that want notifications for this appointment type\n    for type, available in gnib_appointments.items():\n        if available:\n            users = retrieve_users_for_gnib_appointments(type, available)\n            try:\n                await send_notification_to_users(users, 'gnib', type, last_update)\n            except Exception as E:\n                logger.error(E, exc_info=True)\n    for type, available in visa_appointments.items():\n        if available:\n            users = retrieve_users_for_visa_appointments(type, available)\n            await send_notification_to_users(users, 'visa', type, last_update)</code></pre>\n\n\n<p>To set this up as a process, I use <code>supervisor</code> and start this as a separate worker process with django's management commands.</p>\n<pre class=\"codehilite\"><code class=\"language-python\"># ./manage.py  gnib_telegram_bot\nfrom apps.jobs.gnib_telegram_bot import run\nclass Command(BaseCommand):\n    def handle(self, *args, **kwargs):\n         run()</code></pre>", "body": "This post details how the [GNIB/VISA appointment utility](https://harshp.com/dev/utils/gnib-appointments/) was developed to display appointments on the website as well as how this was extended to create a [Telegram bot](https://harshp.com/dev/projects/gnib-appointments/appointment-notifications-using-telegram-app/) to get notifications on all platforms. This can be roughly partitioned into three parts - efficient retrieval of appointments data, storing them into the database, building the bot for notifications\r\n\r\n## Retrieving notifications using `async`\r\n\r\n> This describes the approach to the code in [gnib_appointments_async.py](https://github.com/coolharsh55/harshp.com/blob/dev/harshp_com/scripts/gnib_appointments_async.py)\r\n\r\nGetting the `json` response and the appointment dates is pretty easy based on the scripts, however, to do this efficiently requires some thought. Since there are a lot of requests here, executing them in parallel is the best way to go about it. Threads are one way, but they are expensive to manage and run. Since python now features asynchronous co-routines, these are the best candidates for the task. The basic idea is, by declaring the part as a co-routine, it will be run with other co-routines in a time efficient manner. The system will intermittently check if the responses have returned results, and if not, will execute other code in the interim. This allows many tasks that are waiting to be run in parallel. Based on this, our basic model is:\r\n\r\n```python\r\nasync def task(url):\r\n\tdownload(url)\r\n\t\r\nasync def download_urls():\r\n\turls = [ ... ]\r\n\tcompleted, pending = await asyncio.wait(\r\n\t\t[task(url) for url in urls])\r\n\tresults = [task.result() for task in completed]\r\n\treturn results\r\n\r\nevent_loop = asyncio.get_event_loop()\r\nevent_loop.run_until_complete(download_urls())\r\nevent_loop.close()\r\n```\r\n\r\nThis gets an event loop which is like a thread that keeps tracks of co-routines and their results and will download all urls in asynchronous fashion, which makes them appear as if they are executing in parallel. That's one way of doing this. Another is to wait on the actual download instead of the function where the download is happening. This is done as:\r\n\r\n```python\r\nasync def fetch(url, params):\r\n\tasync with ClientSession() as session:\r\n\t\tasync with session.get(url, params) as response:\r\n\t\t\tdata = await response.text()\r\n\t\t\treturn data\r\n```\r\n\r\nThe important bit is to breakdown the task into components that can be run in parallel and then to call them as separate functions. In the case here, if all urls to be downloaded were within one function, then it cannot be run in parallel as each url will be wait to be downloaded as the previous one hasn't been completed. Instead, if these are called in separate statements as part of a function, then they can be run as separate function calls in parallel, or in this case as `async-await` calls.\r\n\r\n## Database models\r\n\r\nTo persist the appointments, some form of storage is required. In earlier iterations, I simply saved them in memory, or wrote them to a file. However, for per-minute appointments, it isn't feasible, and some form of a database system is better. I already use PostgreSQL for my website, and Redis for a job queue. So I used both in this case for the purposes of storing appointments.\r\n\r\nThe first time the appointments are retrieved, they are stored in Redis (key-value) so that other jobs and processes can immediately see the result. This allows the website to display the latest updates while other jobs are processed in the background. Storing stuff in redis is pretty simple, you simply set the value of a key, and that is it. To make things easier, the data is stored and retrieved as `JSON`. Retrieving it is simple as well.\r\n\r\n```python\r\n gnib_appointments = {\r\n        'gnib_Study_New': json.loads(kvstore.get('gnib_Study_New')),\r\n        'gnib_Study_Renewal': json.loads(kvstore.get('gnib_Study_Renewal')),\r\n        'gnib_Work_New': json.loads(kvstore.get('gnib_Work_New')),\r\n        'gnib_Work_Renewal': json.loads(kvstore.get('gnib_Work_Renewal')),\r\n        'gnib_Other_New': json.loads(kvstore.get('gnib_Other_New')),\r\n        'gnib_Other_Renewal': json.loads(kvstore.get('gnib_Other_Renewal')),\r\n        'visa_I': json.loads(kvstore.get('visa_I')),\r\n        'visa_F': json.loads(kvstore.get('visa_F')),\r\n        'last_update': kvstore.get('gnib_last_run'),\r\n}\r\n```\r\n\r\nFor longer persistence, the appointments are stored to the PostgreSQL database as instance of the `GNIBAppointment` and `VisaAppointment` classes.\r\n\r\n```python\r\nclass GNIBAppointment(AppointmentSlot):\r\n    '''GNIB Appointment'''\r\n    CATEGORY_STUDY = 'Study'\r\n    CATEGORY_WORK = 'Work'\r\n    CATEGORY_OTHER = 'Other'\r\n    CATEGORIES = (\r\n        (CATEGORY_STUDY, 'GNIB Study Appointment'),\r\n        (CATEGORY_WORK, 'GNIB Work Appointment'),\r\n        (CATEGORY_OTHER, 'GNIB Other Appointment'),\r\n    )\r\n    CATEGORY_TYPE_NEW = 'New'\r\n    CATEGORY_TYPE_RENEWAL = 'Renewal'\r\n    CATEGORY_TYPES = (\r\n        (CATEGORY_TYPE_NEW, 'GNIB New Appointment'),\r\n        (CATEGORY_TYPE_RENEWAL, 'GNIB Renewal Appointment'),\r\n    )\r\n\r\n    category = models.CharField(\r\n        max_length=8, db_index=True, choices=CATEGORIES)\r\n    category_type = models.CharField(\r\n        max_length=8, db_index=True, choices=CATEGORY_TYPES)\r\n\r\nclass VisaAppointment(AppointmentSlot):\r\n    '''Visa Appointment'''\r\n    CATEGORY_INDIVIDUAL = 'I'\r\n    CATEGORY_FAMILY = 'F'\r\n    CATEGORIES = (\r\n        (CATEGORY_INDIVIDUAL, 'Visa Individual Appointment'),\r\n        (CATEGORY_FAMILY, 'Visa Family Appointment'),\r\n    )\r\n\r\n    category = models.CharField(\r\n        max_length=8, db_index=True, choices=CATEGORIES)\r\n```\r\n\r\nTo keep the responses, all appointment data retrieved from the servers is stored in to the database as well.\r\n\r\n```python\r\nclass APIResponse(models.Model):\r\n    '''Stores raw JSON response from GNIB API'''\r\n    added_on = models.DateTimeField(auto_now_add=True)\r\n    json = JSONField()\r\n```\r\n\r\n## Telegram Bot\r\n\r\n> This describes code in [gnib_telegram_bot.py](https://github.com/coolharsh55/harshp.com/blob/dev/harshp_com/apps/jobs/gnib_telegram_bot.py)\r\n\r\nTelegram makes it super simple to build bots. You get an API key and then you interact with your bot by passing the key as part of your request. Then you register your message handlers, and keep checking for messages every once in a while. I used the [`python telegram bot`](https://python-telegram-bot.org/) library to interact with Telegram. Their documentation was easy to pick up on, and the bot itself is straightforward to start with. Additionally, it features a job queue so I can add jobs as well.\r\n\r\n```python\r\nupdater = Updater(token=TELEGRAM_API_KEY)\r\ndispatcher = updater.dispatcher\r\ndispatcher.add_handler(CommandHandler('start', start, pass_job_queue=True))\r\nupdater.start_polling()\r\nupdater.idle()\r\n```\r\n\r\nTelegram makes it so that bots can only respond to conversations that have already been started by someone. To store the data, I persist a chat in a database model along with notification preferences such as date filters for GNIB and Visa appointments.\r\n\r\n> This describes code for [database models](https://github.com/coolharsh55/harshp.com/blob/dev/harshp_com/apps/models/gnib_telegram_bot.py)\r\n\r\n```python\r\nclass TelegramUser(models.Model):\r\n    '''Telegram User\r\n    This is the user that interacts with the bot.\r\n    '''\r\n    chat_id = models.CharField(max_length=64, db_index=True, primary_key=True)\r\n    appointment_gnib = models.CharField(\r\n        max_length=64, db_index=True, choices=GNIB_APPOINTMENT_TYPES,\r\n        blank=True, null=True)\r\n    gnib_filter_date_start = models.DateField(blank=True, null=True)\r\n    gnib_filter_date_end = models.DateField(blank=True, null=True)\r\n    appointment_visa = models.CharField(\r\n        max_length=64, db_index=True, choices=VISA_APPOINTMENT_TYPES,\r\n        blank=True, null=True)\r\n    visa_filter_date_start = models.DateField(blank=True, null=True)\r\n    visa_filter_date_end = models.DateField(blank=True, null=True)\r\n```\r\n\r\nTo actually send the notifications, after each retrieval, it's a simple database call;\r\n\r\n> Code in [gnib_telegram_notifications.py](https://github.com/coolharsh55/harshp.com/blob/dev/harshp_com/apps/jobs/gnib_telegram_notifications.py)\r\n\r\n```python\r\nasync def job():\r\n    # retrieve added appointment for every type from database\r\n    gnib_appointments = {\r\n        ...\r\n    }\r\n    visa_appointments = {\r\n        ...\r\n    }\r\n    last_update = kvstore.get('gnib_last_run')\r\n\r\n    # retrieve users that want notifications for this appointment type\r\n    for type, available in gnib_appointments.items():\r\n        if available:\r\n            users = retrieve_users_for_gnib_appointments(type, available)\r\n            try:\r\n                await send_notification_to_users(users, 'gnib', type, last_update)\r\n            except Exception as E:\r\n                logger.error(E, exc_info=True)\r\n    for type, available in visa_appointments.items():\r\n        if available:\r\n            users = retrieve_users_for_visa_appointments(type, available)\r\n            await send_notification_to_users(users, 'visa', type, last_update)\r\n```\r\n\r\nTo set this up as a process, I use `supervisor` and start this as a separate worker process with django's management commands.\r\n\r\n```python\r\n# ./manage.py  gnib_telegram_bot\r\nfrom apps.jobs.gnib_telegram_bot import run\r\nclass Command(BaseCommand):\r\n    def handle(self, *args, **kwargs):\r\n         run()\r\n```", "headerimage": "", "highlight": "0", "section": 3}, {"id": 35, "title": "Analysing GNIB appointment app survey responses", "authors": "1", "date_created": "2018-02-27 00:00:00", "date_published": "2018-02-27 00:00:00", "date_updated": "2018-03-01 11:08:18", "is_published": "1", "short_description": "Gathering requirements and making decisions based on the survey of 59 participants", "tags": "162,137", "slug": "analysing-gnib-appointment-app-survey-responses", "body_type": "markdown", "body_text": "<p>I posted a link on the GNIB appointment utility for a survey on an app for appointments. The survey ran from 7th November until 28th February and inquired about things like what features the app should have, the system people used, and what would be a fair price. This post presents an analysis of the survey results. The survey itself is available as a <a href=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/GNIB_appointment_app_survey.pdf\">PDF file</a>. The survey results can be found <a href=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/GNIB_appointment_app_survey_responses.csv\">here</a>. In total, the survey generated 59 responses.</p>\n<h2 id=\"phoneos\">Phone/OS</h2>\n<p><img alt=\"what operating system are you using?\" src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart__os.png\"></p>\n<p>Responses indicated that most people who answered the survey were using an iPhone (70.7%) while Android made up most of the rest (27.6%). Since iPhone users were the majority of the respondents, it was essential that the app should work on iOS devices. Similarly, Android users made up a sizable chunk of the responses, and therefore needed to be addressed to. Ideally, this would have resulted in two different apps. The cost of development for this approach is quite high in terms of learning and implementing two different apps. Additionally, putting such apps on their respective app stores requires a large amount of money (99$ for iOS and 25$ for Android) which would need to be recovered through the app. This required looking at a free solution  that was accessible to all platforms. Ideally this would have been Whatsapp, but this was not possible as the platform does not support bots. Facebook messenger exists and while I previously used it to provide a notifications bot, I did not want to tie my system down to Facebook because of privacy considerations. Alternatives like Slack or Telegram were good options where I can push messages and people can receive it without any of the issues above. I chose telegram since it is an open source and secure messaging app and building a bot was well documented.</p>\n<blockquote>\n<p>You can find the implementation report for building the notifications bot <a href=\"https://harshp.com/dev/projects/gnib-appointments/setting-up-notifications-via-telegram/\">here</a></p>\n</blockquote>\n<h2 id=\"features\">Features</h2>\n<p><img alt=\"what features would you like in the app?\" src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart_features.png\"></p>\n<p>Most respondents wanted to view the appointments as well as to receive notifications. This formed the core feature set of the app. Another large percentage wanted to book appointments as well, and though this is possible, this could potentially not be acceptable to the governmental agencies that handle appointments. Therefore I decided to raise this issue with them as an official matter before providing this as a feature. Filtering appointments by date had a few responses, but because it was trivial to implement and had potential uses for everyone, it was adopted as a core feature as well.</p>\n<p><img alt=\"would be you be okay with storing sensitive information in the app?\" src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart_sensitive_date.png\"></p>\n<p>People were cautious about this as there were an equal number of responses for not storing any data and only storing if the data had adequate security measures. A similar number of people responded in the affirmative that they were okay with storing sensitive data. Currently, the app does not store anything other than the user's telegram chat id which is required for sending notifications. In the future, these responses may reflect additional features and how they should be implemented.</p>\n<h2 id=\"paymentrevenue\">Payment/Revenue</h2>\n<p><img alt=\"how much would you be willing to pay for the app?\" src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart_payment.png\"></p>\n<p>Developing this app and the entire system is a task that I undertake in my spare time as a hobby project. As such, I have limited time to develop it. Receiving monetary compensation provides not only motivation, but also helps me with maintaining servers, adding more features, and fixing bugs. To that end, the survey responses indicate that most people (55%) would not be willing to pay for an app. Some people (11%) indicated that 5euros would be a fair price to pay. Other responses were few and varied in between 1 and 10 euros. This indicates that a large majority of the respondents want a free system that provided the required notifications, which is what I intend to implement.</p>\n<p><img alt=\"would you be okay if the app showed ads for revenue?\" src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart_generate_revenue.png\"></p>\n<p>The form also asked whether ads were an acceptable form of generating revenue. 55.4% answered yes, 17.9% answered maybe, and 26.8% answered no. While currently the system does not show any ads, I can safely that the response towards this is positive, and if the ads are non-intrusive, non-disruptive, and safe, then they can be showed to generate revenue.</p>", "body": "I posted a link on the GNIB appointment utility for a survey on an app for appointments. The survey ran from 7th November until 28th February and inquired about things like what features the app should have, the system people used, and what would be a fair price. This post presents an analysis of the survey results. The survey itself is available as a [PDF file](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/GNIB_appointment_app_survey.pdf). The survey results can be found [here](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/GNIB_appointment_app_survey_responses.csv). In total, the survey generated 59 responses.\r\n\r\n## Phone/OS\r\n![what operating system are you using?](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart__os.png)\r\n\r\nResponses indicated that most people who answered the survey were using an iPhone (70.7%) while Android made up most of the rest (27.6%). Since iPhone users were the majority of the respondents, it was essential that the app should work on iOS devices. Similarly, Android users made up a sizable chunk of the responses, and therefore needed to be addressed to. Ideally, this would have resulted in two different apps. The cost of development for this approach is quite high in terms of learning and implementing two different apps. Additionally, putting such apps on their respective app stores requires a large amount of money (99$ for iOS and 25$ for Android) which would need to be recovered through the app. This required looking at a free solution  that was accessible to all platforms. Ideally this would have been Whatsapp, but this was not possible as the platform does not support bots. Facebook messenger exists and while I previously used it to provide a notifications bot, I did not want to tie my system down to Facebook because of privacy considerations. Alternatives like Slack or Telegram were good options where I can push messages and people can receive it without any of the issues above. I chose telegram since it is an open source and secure messaging app and building a bot was well documented.\r\n\r\n> You can find the implementation report for building the notifications bot [here](https://harshp.com/dev/projects/gnib-appointments/setting-up-notifications-via-telegram/)\r\n\r\n## Features\r\n![what features would you like in the app?](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart_features.png)\r\n\r\nMost respondents wanted to view the appointments as well as to receive notifications. This formed the core feature set of the app. Another large percentage wanted to book appointments as well, and though this is possible, this could potentially not be acceptable to the governmental agencies that handle appointments. Therefore I decided to raise this issue with them as an official matter before providing this as a feature. Filtering appointments by date had a few responses, but because it was trivial to implement and had potential uses for everyone, it was adopted as a core feature as well.\r\n\r\n![would be you be okay with storing sensitive information in the app?](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart_sensitive_date.png)\r\n\r\nPeople were cautious about this as there were an equal number of responses for not storing any data and only storing if the data had adequate security measures. A similar number of people responded in the affirmative that they were okay with storing sensitive data. Currently, the app does not store anything other than the user's telegram chat id which is required for sending notifications. In the future, these responses may reflect additional features and how they should be implemented.\r\n\r\n## Payment/Revenue\r\n![how much would you be willing to pay for the app?](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart_payment.png)\r\n\r\nDeveloping this app and the entire system is a task that I undertake in my spare time as a hobby project. As such, I have limited time to develop it. Receiving monetary compensation provides not only motivation, but also helps me with maintaining servers, adding more features, and fixing bugs. To that end, the survey responses indicate that most people (55%) would not be willing to pay for an app. Some people (11%) indicated that 5euros would be a fair price to pay. Other responses were few and varied in between 1 and 10 euros. This indicates that a large majority of the respondents want a free system that provided the required notifications, which is what I intend to implement.\r\n\r\n![would you be okay if the app showed ads for revenue?](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/chart_generate_revenue.png)\r\n\r\nThe form also asked whether ads were an acceptable form of generating revenue. 55.4% answered yes, 17.9% answered maybe, and 26.8% answered no. While currently the system does not show any ads, I can safely that the response towards this is positive, and if the ads are non-intrusive, non-disruptive, and safe, then they can be showed to generate revenue.", "headerimage": "", "highlight": "0", "section": 3}, {"id": 33, "title": "Appointment notifications using Telegram app", "authors": "1", "date_created": "2018-03-01 06:00:00", "date_published": "2018-03-01 06:00:00", "date_updated": "2018-03-01 10:35:28", "is_published": "1", "short_description": "A how-to for signing up to receive appointment notifications for GNIB and VISA appointments", "tags": "137,204,205", "slug": "appointment-notifications-using-telegram-app", "body_type": "html", "body_text": "<p>Refer to <a href=\"https://harshp.com/dev/projects/gnib-appointments/analysing-gnib-appointment-app-survey-responses/\">this<a/> for an analysis of the GNIB appointment app survey and why I chose Telegram.</p>\r\n\r\n<ol>\r\n<li>Install <a href=\"https://telegram.org/\">Telegram</a> app for your device and sign-up/sign-in</li>\r\n<li>Send/Start a message and find the bot with the name \"<b>gnib_appointment_bot</b>\"</li>\r\n<li>Click/Send <i>start</i></li>\r\n<div class=\"w-50 w-25-m w-20-l pa2\"><img src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/gnib_telegram_bot_start.png\" class=\"db\" alt=\"\"></div>\r\n<li>You will receive a message with a link for customising notifications, click it</li>\r\n<li>This will open a browser page where you can select category and dates for notifications to receive</li>\r\n<div class=\"w-50 w-25-m w-20-l pa2\"><img src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/gnib_telegram_bot_preferences.png\" class=\"db\" alt=\"\"></div>\r\n<li>Select all relevant options and click on <i>save preferences</i></li>\r\n<ol>\r\n<br/>\r\n<p>You will now receive notifications whenever a new appointment is detected!</p>", "body": "<p>Refer to <a href=\"https://harshp.com/dev/projects/gnib-appointments/analysing-gnib-appointment-app-survey-responses/\">this<a/> for an analysis of the GNIB appointment app survey and why I chose Telegram.</p>\r\n\r\n<ol>\r\n<li>Install <a href=\"https://telegram.org/\">Telegram</a> app for your device and sign-up/sign-in</li>\r\n<li>Send/Start a message and find the bot with the name \"<b>gnib_appointment_bot</b>\"</li>\r\n<li>Click/Send <i>start</i></li>\r\n<div class=\"w-50 w-25-m w-20-l pa2\"><img src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/gnib_telegram_bot_start.png\" class=\"db\" alt=\"\"></div>\r\n<li>You will receive a message with a link for customising notifications, click it</li>\r\n<li>This will open a browser page where you can select category and dates for notifications to receive</li>\r\n<div class=\"w-50 w-25-m w-20-l pa2\"><img src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/gnib_telegram_bot_preferences.png\" class=\"db\" alt=\"\"></div>\r\n<li>Select all relevant options and click on <i>save preferences</i></li>\r\n<ol>\r\n<br/>\r\n<p>You will now receive notifications whenever a new appointment is detected!</p>", "headerimage": "", "highlight": "1", "section": 3}, {"id": 32, "title": "give & take lists", "authors": "1", "date_created": "2015-09-08 23:00:00", "date_published": "2015-09-09 15:17:10", "date_updated": "2017-11-12 16:28:37", "is_published": "1", "short_description": "comparing movies with friends", "tags": "40", "slug": "give-take-lists", "body_type": "markdown", "body_text": "<p>So far, using hdd-indexer, you can indentify what movies are present on\nthe disk, get their metadata, browse using this metadata. So you know\nwhat\u2019s the highest rated movie you have, or what\u2019s the latest movie you\nhave. Great. But what happens when you meet up with a friend, and\nthere\u2019s exchanging of movies? You both ask each other what movies are\npresent that you don\u2019t have, find the movie files amongst all the (if)\nrandomly named and arranged folders, and then copy them one by one. Too\nmuch repetition. So I decided to see if I can get hdd-indexer can do\nsomething about it.</p>\n<p>I came up with the simple concept of give/take lists, which show a\ntwo-column list of movies that</p>\n<ul>\n<li>you have to give to your friend, meaning he doesn\u2019t have these\n    movies on his disk, and</li>\n<li>you have to take from your friend, meaning you don\u2019t have these\n    movies on your disk</li>\n</ul>\n<p>If you\u2019ve studied (and remember!) a little bit of math (set theory, to\nbe precise) you can see that the the above two conditions are nothing\nbut an application of set difference. Let\u2019s say that X is the set of\nmovies on your disk, and Y is the set of movies on your friends\u2019 disk.\nThen by set theory, we have:</p>\n<ul>\n<li>X - Y: movies on your disk that are not on your friend\u2019s disk</li>\n<li>Y - X: movies on your friend\u2019s disk that are not on your disk</li>\n</ul>\n<p>The assumption here, is that all objects in X and Y are similar, or can\nbe compared to see if they exist in either set. Which means, that the\nformat in which movies are stored, or indexed on both disks must be the\nsame (or it will involve more work to first convert them both in to a\ncommon format). Once we get a set of movies, it\u2019s trivial to calculate\nthe set difference in any programming language. If you\u2019re not an aware\nprogrammer, I hope the pesudocode is self-explanatory. <span\nstyle=\"\\&quot;font-family:\" monospace;\\\"=\"\"></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\">function\nset_difference(X, Y): </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> var\ndifference = list() # empty list </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> for x\nin X: </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> if x\nnot in Y: </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\">\ndifference.add( x ) </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> return\ndifference</span></span></p>\n<p>Once we calculate this difference, we get a list of movies to give and\ntake. This eases a part of the transaction - finding which movies should\nbe exchanged. There\u2019s still the other parts that need to be addressed:\ncopying movie files. To make this possible, we need to understand what\nhappens when movies are exchanged. The files containing these movies are\ncopied directly to the disk, if available, or else to a temporary\nlocation (to be copied to the disk later). In both cases, it\u2019s a\ncopy-file operation that can be automated using a script.\\</p>\n<p>Since hdd-indexer is aware of the file paths of movies indexed in its\ndatabase, the creation of such a script is trivial.```</p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\">function\ncopy_script(movie_list, target_folder): </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> var\nscript = list() # empty list </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> for\nmovie in movie_list: </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> # cp\nfor *NIX, copy for WINDOWS </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> #\nshell command </span></span></p>\n<p><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\">\nscript.append(\u201ccopy movie_list to target_folder\u201d)</span></span></p>\n<p>Depending on the platform, the command to copy changes. Fortunately,\nthis trivial differences are taken care of because hdd-indexer uses\npython, which offers libraries that do the work on all platforms. So we\nmerely have to tell the script that we want a file copied from one path\nto another.</p>\n<p>After copying, the part that does not figure in the movie exchange -\nadding these files to hdd-indexer comes up. There\u2019s always the easiest\nway to simply copy the files in the movie folder on disk, and let\nhdd-indexer detect them and download their metadata. But to make things\nmore challenging, and better, we can export the metadata along with the\ncopied files. Hdd-indexer can simply create a file called, say,\nget_list.json that will contain a list of all movie files that have\nbeen copied, and their metadata. Now you can simply import this file in\nto hdd-indexer, and it knows what movie files to add along with their\nmetadata.</p>\n<p>There, a simple example of exchanging movies between friends gets\nanother feature added to hdd-indexer!</p>", "body": "So far, using hdd-indexer, you can indentify what movies are present on\r\nthe disk, get their metadata, browse using this metadata. So you know\r\nwhat\u2019s the highest rated movie you have, or what\u2019s the latest movie you\r\nhave. Great. But what happens when you meet up with a friend, and\r\nthere\u2019s exchanging of movies? You both ask each other what movies are\r\npresent that you don\u2019t have, find the movie files amongst all the (if)\r\nrandomly named and arranged folders, and then copy them one by one. Too\r\nmuch repetition. So I decided to see if I can get hdd-indexer can do\r\nsomething about it.\r\n\r\nI came up with the simple concept of give/take lists, which show a\r\ntwo-column list of movies that\r\n\r\n-   you have to give to your friend, meaning he doesn\u2019t have these\r\n    movies on his disk, and\r\n-   you have to take from your friend, meaning you don\u2019t have these\r\n    movies on your disk\r\n\r\nIf you\u2019ve studied (and remember!) a little bit of math (set theory, to\r\nbe precise) you can see that the the above two conditions are nothing\r\nbut an application of set difference. Let\u2019s say that X is the set of\r\nmovies on your disk, and Y is the set of movies on your friends\u2019 disk.\r\nThen by set theory, we have:\r\n\r\n-   X - Y: movies on your disk that are not on your friend\u2019s disk\r\n-   Y - X: movies on your friend\u2019s disk that are not on your disk\r\n\r\nThe assumption here, is that all objects in X and Y are similar, or can\r\nbe compared to see if they exist in either set. Which means, that the\r\nformat in which movies are stored, or indexed on both disks must be the\r\nsame (or it will involve more work to first convert them both in to a\r\ncommon format). Once we get a set of movies, it\u2019s trivial to calculate\r\nthe set difference in any programming language. If you\u2019re not an aware\r\nprogrammer, I hope the pesudocode is self-explanatory. <span\r\nstyle=\"\\&quot;font-family:\" monospace;\\\"=\"\"></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\">function\r\nset\\_difference(X, Y): </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> var\r\ndifference = list() \\# empty list </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> for x\r\nin X: </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> if x\r\nnot in Y: </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\">\r\ndifference.add( x ) </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> return\r\ndifference</span></span>\r\n\r\nOnce we calculate this difference, we get a list of movies to give and\r\ntake. This eases a part of the transaction - finding which movies should\r\nbe exchanged. There\u2019s still the other parts that need to be addressed:\r\ncopying movie files. To make this possible, we need to understand what\r\nhappens when movies are exchanged. The files containing these movies are\r\ncopied directly to the disk, if available, or else to a temporary\r\nlocation (to be copied to the disk later). In both cases, it\u2019s a\r\ncopy-file operation that can be automated using a script.\\\r\n\r\nSince hdd-indexer is aware of the file paths of movies indexed in its\r\ndatabase, the creation of such a script is trivial.\\`\\`\\`\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\">function\r\ncopy\\_script(movie\\_list, target\\_folder): </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> var\r\nscript = list() \\# empty list </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> for\r\nmovie in movie\\_list: </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> \\# cp\r\nfor \\*NIX, copy for WINDOWS </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\"> \\#\r\nshell command </span></span>\r\n\r\n<span style=\"\\&quot;font-family:\" monospace;\\\"=\"\"><span\r\nstyle=\"\\&quot;background-color:\" rgb(238,=\"\" 236,=\"\" 225);\\\"=\"\">\r\nscript.append(\u201ccopy movie\\_list to target\\_folder\u201d)</span></span>\r\n\r\nDepending on the platform, the command to copy changes. Fortunately,\r\nthis trivial differences are taken care of because hdd-indexer uses\r\npython, which offers libraries that do the work on all platforms. So we\r\nmerely have to tell the script that we want a file copied from one path\r\nto another.\r\n\r\nAfter copying, the part that does not figure in the movie exchange -\r\nadding these files to hdd-indexer comes up. There\u2019s always the easiest\r\nway to simply copy the files in the movie folder on disk, and let\r\nhdd-indexer detect them and download their metadata. But to make things\r\nmore challenging, and better, we can export the metadata along with the\r\ncopied files. Hdd-indexer can simply create a file called, say,\r\nget\\_list.json that will contain a list of all movie files that have\r\nbeen copied, and their metadata. Now you can simply import this file in\r\nto hdd-indexer, and it knows what movie files to add along with their\r\nmetadata.\r\n\r\nThere, a simple example of exchanging movies between friends gets\r\nanother feature added to hdd-indexer!", "headerimage": "", "highlight": "0", "section": 13}, {"id": 31, "title": "imdbnator - like this, but classier", "authors": "1", "date_created": "2015-08-11 23:00:00", "date_published": "2015-08-12 15:15:09", "date_updated": "2017-11-12 16:25:48", "is_published": "1", "short_description": "Found a website that already does something similar to HDD-indexer", "tags": "40", "slug": "imdbnator-like-this-but-classier", "body_type": "markdown", "body_text": "<p>I found out about imdbnator.com - it's a nifty little website that reads\na folder containing movie files, parses them, and presents a neat index\nof movie metadata from IMDb. That's exactly what HDD-indexer aims to do.\nOr does. Whatever.</p>\n<p>The point is, it's great that such a tool exists out there, doesn't\nrequire any form of installation, and serves up a pretty list of movies\non disk. It even involves movie posters! As of now, HDD-indexer has only\nthe basic movie metadata such as title, release date, and a few ratings.\nI do plan to eventually add more fields to it, but I think those are\nsecondary compared to finishing all the modules first.\u00a0</p>\n<p>Being aware of imdbnator.com has given me new aims and goals as to what\nI want to do with HDD-indexer. I emailed the developer, looking to\ncollaborate or get some sort of help, but that didn't happen. The\ndeveloper (great guy) informed me that he has no plans to open source\nhis project any time soon. Upon persisting, he did give me a few tips,\ntricks, and things to think about.</p>\n<p>Levenshtein distance - using this to get the most appropriate movie\ntitle based on filename. I had planned on using this anyhow. The idea\ncame to me when I was looking at the output of the metadata_by_title\nfunctions that try to identify a movie based on its filename. It gave\nvarious options for titles, and some of them were way off the mark. I\nthought about using string similarity to get the closest match possible,\nand assign that as the identified movie. This search led me to the\nLevenshtein algorithm, which is the best candidate. It is fast, it is\nsimple, and can be used directly on the database (SQL queries) if need\nbe.</p>\n<p>Offline database - here's where the tricky part comes in. As per the\nsuggestion, having an offline copy of the database allows much faster\nsearch and resolutions. Sounds great, I know it works, and is a great\nmodel for static servers. But HDD-indexer is a portable utility that\nmoves with the disk. So I need all that amount of data in a portable\ndatabase that can be easily loaded into the program. SQLite struggles\nwith huge amounts of data. So I'm in a pickle here as to how to go about\nit.\u00a0</p>\n<p>I only need the database for READ access, since I'm not going to change\nany information on it. All the writes and changes are going to be in\nanother database, which will be the user's database. This read-only\napproach makes most sense because SQLite has a global write lock, which\nmeans it locks the entire database when writing. Not good for\nperformance. Having a separate database with read-only access would\nallow concurrent multiple threads to access it without any problems.\nOnly the primary keys referring to rows in this IMDb database could be\nsaved in memory, and then later written to the user database in a single\nthread.\u00a0</p>\n<p>Integrating the database into Django seems to be possible. It supports\nmultiple databases which are separate and can adhere to different\nmodels. It also supports a (great) tool called inspectdb, that creates\nmodels based on existing database schemas. So theoretically, I can\nconstruct the IMDb database, use inspectdb to create models for it, and\nuse these in HDD-indexer. The only thing that is uncertain over here is\nthe kind of performance it would return.\u00a0</p>\n<p>Integrating such an offline flow is great, because Internet over here\nstill seems like a luxury rather than a commodity. How well this\napproach integrates with the online-metadata approach is something that\nI should be able to understand well as I develop this later on. For now,\nI'm more focused on the v0.3 release, which would introduce the Exporter\nand Organizer modules. I'm particularly excited about the Organizer,\nbecause it could (in theory) organize your entire movie collection like\niTunes does with the help of metadata. For starters, there will be basic\norganization capabilities like release year and alphabetically. Later\non, I plan to organize using complex conditions such as genre, series,\nand directors.</p>", "body": "I found out about imdbnator.com - it's a nifty little website that reads\r\na folder containing movie files, parses them, and presents a neat index\r\nof movie metadata from IMDb. That's exactly what HDD-indexer aims to do.\r\nOr does. Whatever.\r\n\r\nThe point is, it's great that such a tool exists out there, doesn't\r\nrequire any form of installation, and serves up a pretty list of movies\r\non disk. It even involves movie posters! As of now, HDD-indexer has only\r\nthe basic movie metadata such as title, release date, and a few ratings.\r\nI do plan to eventually add more fields to it, but I think those are\r\nsecondary compared to finishing all the modules first.\u00a0\r\n\r\nBeing aware of imdbnator.com has given me new aims and goals as to what\r\nI want to do with HDD-indexer. I emailed the developer, looking to\r\ncollaborate or get some sort of help, but that didn't happen. The\r\ndeveloper (great guy) informed me that he has no plans to open source\r\nhis project any time soon. Upon persisting, he did give me a few tips,\r\ntricks, and things to think about.\r\n\r\nLevenshtein distance - using this to get the most appropriate movie\r\ntitle based on filename. I had planned on using this anyhow. The idea\r\ncame to me when I was looking at the output of the metadata\\_by\\_title\r\nfunctions that try to identify a movie based on its filename. It gave\r\nvarious options for titles, and some of them were way off the mark. I\r\nthought about using string similarity to get the closest match possible,\r\nand assign that as the identified movie. This search led me to the\r\nLevenshtein algorithm, which is the best candidate. It is fast, it is\r\nsimple, and can be used directly on the database (SQL queries) if need\r\nbe.\r\n\r\nOffline database - here's where the tricky part comes in. As per the\r\nsuggestion, having an offline copy of the database allows much faster\r\nsearch and resolutions. Sounds great, I know it works, and is a great\r\nmodel for static servers. But HDD-indexer is a portable utility that\r\nmoves with the disk. So I need all that amount of data in a portable\r\ndatabase that can be easily loaded into the program. SQLite struggles\r\nwith huge amounts of data. So I'm in a pickle here as to how to go about\r\nit.\u00a0\r\n\r\nI only need the database for READ access, since I'm not going to change\r\nany information on it. All the writes and changes are going to be in\r\nanother database, which will be the user's database. This read-only\r\napproach makes most sense because SQLite has a global write lock, which\r\nmeans it locks the entire database when writing. Not good for\r\nperformance. Having a separate database with read-only access would\r\nallow concurrent multiple threads to access it without any problems.\r\nOnly the primary keys referring to rows in this IMDb database could be\r\nsaved in memory, and then later written to the user database in a single\r\nthread.\u00a0\r\n\r\nIntegrating the database into Django seems to be possible. It supports\r\nmultiple databases which are separate and can adhere to different\r\nmodels. It also supports a (great) tool called inspectdb, that creates\r\nmodels based on existing database schemas. So theoretically, I can\r\nconstruct the IMDb database, use inspectdb to create models for it, and\r\nuse these in HDD-indexer. The only thing that is uncertain over here is\r\nthe kind of performance it would return.\u00a0\r\n\r\nIntegrating such an offline flow is great, because Internet over here\r\nstill seems like a luxury rather than a commodity. How well this\r\napproach integrates with the online-metadata approach is something that\r\nI should be able to understand well as I develop this later on. For now,\r\nI'm more focused on the v0.3 release, which would introduce the Exporter\r\nand Organizer modules. I'm particularly excited about the Organizer,\r\nbecause it could (in theory) organize your entire movie collection like\r\niTunes does with the help of metadata. For starters, there will be basic\r\norganization capabilities like release year and alphabetically. Later\r\non, I plan to organize using complex conditions such as genre, series,\r\nand directors.", "headerimage": "", "highlight": "0", "section": 13}, {"id": 30, "title": "HDD-indexer v0.1", "authors": "1", "date_created": "2015-07-28 23:00:00", "date_published": "2015-07-29 15:13:41", "date_updated": "2017-11-12 16:24:43", "is_published": "1", "short_description": "First implementation", "tags": "40", "slug": "hdd-indexer-v01", "body_type": "markdown", "body_text": "<p>The first deployment of this utility focuses on basic operations -\nidentify movies on disk and download metadata. Along with these\noperations, the setup and browser interface are also made simple to\nunderstand and use. The setup takes care of installing dependencies and\nbasic configuratons. The setup is split into two parts - one run in the\nshell, and the second in the browser. The setup first installs\ndependencies, creates the database and all related tables and schema,\ncreates an admin account, starts the server and opens a browser window\nto complete the next steps. This allows the installation and usage to be\nthe same experience on all platforms.\u00a0</p>\n<p>The HDD Root is the mount directory (*NIX) or drive (Windows) where the\ndisk can be accessed at. This is an absolute path so that the disk can\nbe accessed on the system it is mounted on. The Movie Folder is the\nplace where all movie files are stored. Its path is stored relative to\nthe HDD Root. This is done so that the configuration remains the same\nacross systems. The organisation and placement of contents on disk\nremain the same irrespective of where it is attached to. The only thing\nthat changes is the place where the disk is accessed at. This is the HDD\nRoot path. Therefore, whenever the disk is attached to a system, the\nroot path needs to be configured. Sometimes, this path changes even when\nthe disk is attached to the same system on which the root path was\nconfigured. In the future, it possibly could be checked if the hdd is\nmounted on the same path or the root path has changed. This can be done\n(maybe) by checking the path of the server when it starts execution. If\nthe program is on the disk, the root path will be a substring of the\nserver execution path. Cases exist where this can be subverted with\ndirectories and symlinks made in such a manner so that to mimic a valid\ndisk path. But we can ignore this case since it is unlikely that someone\nwill pervert the configurations in this manner, and even if they do,\ntheir usage of the utility would be reduced, or possibly prevented.</p>\n<p>The only working modules at this time are browse, crawl, and load. The\nbrowse module allows viewing of movies already present in database. It\nuses django\u2019s handy admin interface, which presents objects stored in\nthe database in a nice and simple tabular layout. The results can be\nsorted or filtered based on columns or fields. In this case, the movies\ncan be filtered by their release date or based on actors and directors.\nThe columns can be sorted based on their contents. The movies section\nshows the title, release date, their relative path from the movie\nfolder, imdb rating, rotten tomatoes rating, and metascore. By default,\nmovies are sorted by their title. Filtering by release dates supports\nyears and also months. It\u2019s certainly handy to check which of the latest\nmovies are on the disk. Filtering based on an actor or director allows\nme to check which of their movies are on the disk. All of these suffice\nthe need and use of a basic organisation of movies based on their\nmetadata.</p>\n<p>The crawl module looks for movie files in the configured movie folder.\nIt identifies a video file as a movie based on its file extension (video\nfile) and file size (more than 300MB), and saves it to the database\nalong with its relative path. This relative path is calculated from the\nmovie folder. This abstraction allows the movie folder to be moved\naround, renamed, and copied while the movie paths remain consistent. To\nget the absolute path of any movie file, its relative path must be\nappended with the movie folder\u2019s relative path, which is then appended\nto the root path. While tedious to calculate and evaluate, this\nabstraction greatly simplifies the code when trying to get the absolute\npath of every file on different systems. The crawl module is usually\nfast and simple. Identified files are immediately added to the database.\nWhen viewed, they show only the filename as its title, and the relative\npath.</p>\n<p>The load module downloads the metadata from online source such as the\nOpen Movie Database and The Movie Database. If a movie entry in database\nhas an IMDb ID associated with it, this is used to retrieve its\nmetadata. If this ID is absent, a search is performed based on the\nfilename, which is assumed to be its title. If a movie is identified,\nits IMDb ID is retrieved, and used to retrieved metadata. If the movie\ncannot be identified by its filename, Open Subtitles is used to identify\nthe movie and retrieve its IMDb ID. Open Subtitles is an online\nsubtitles library/database that can identify a movie by calculating a\nhash of the movie file and matching it within its database.</p>\n<p>The crawl and load (or loader) module when started, creates a new\nnon-blocking asynchronous thread so as to keep the browser and other\noperations open for use. This thread initiates the necessary parameters\nfor accessing the online sources via their API. The actual downloading\nis done in individual threads, which currently number five (5). This\nnumber was selected as the best choice after testing with various number\nof threads (1,2,5,10,20). Low number of threads take more time, whereas\nlarge number of threads download the data much faster, but block the\ndatabase when writing. SQLite cannot handle this large load of\nwrite-locks and fails. To solve this, I only download the data in\nthreads, and when all threads have finished, save the movie data into\nthe database in a single thread. To simplify the object management,\nqueues are used to store objects as threads retrieve and operate on\nthem. If a large number of movie objects are in database, the queues and\ndownloaded data take up more memory, which causes the loader to become\nsluggish towards the latter half of operations. To prevent this, I put 5\nobjects in the queue at a time, the same number as there are threads.\nThis completes the operations in a faster time since each thread has\nless load to complete, and doesn\u2019t clog up the server or database\nmemory. The results and effects might (most likely will) change when a\ndifferent, dedicated database is used. What would be interesting is to\nuse a document database to store the movie data since it allows much\nmore freedom to store arbitrary fields. Using such a division of labor\nalso makes sure that if an error occurs that breaks the flow, the\nprevious objects are stored to database.\u00a0</p>\n<p>The status of the loader is constantly reflected in the browser via\nGET/POST methods that send the movies evaluated, metadata downloaded,\nand movies skipped. The user has control of the operation through the\nSTART/STOP button in the browser. Sometimes, due to (currently unchecked\nand unverified) some cause, the loader gets stuck and will not continue.\nIt continues to run without downloading metadata. I call this as\noperation freezing, since it appears to have stuck in some operation,\nwhile still running and responding. In such cases, the process can be\nstopped via the button in the browser, and it (looks to be) seems safe\nto stop it at \u00a0any time.</p>\n<p>The current version of HDD-indexer is certainly useful on its own, but\nthere are a lot of features and operations that are yet to be developed.\nI have several ideas as to what these could be, and how I can implement\nthem.</p>", "body": "The first deployment of this utility focuses on basic operations -\r\nidentify movies on disk and download metadata. Along with these\r\noperations, the setup and browser interface are also made simple to\r\nunderstand and use. The setup takes care of installing dependencies and\r\nbasic configuratons. The setup is split into two parts - one run in the\r\nshell, and the second in the browser. The setup first installs\r\ndependencies, creates the database and all related tables and schema,\r\ncreates an admin account, starts the server and opens a browser window\r\nto complete the next steps. This allows the installation and usage to be\r\nthe same experience on all platforms.\u00a0\r\n\r\nThe HDD Root is the mount directory (\\*NIX) or drive (Windows) where the\r\ndisk can be accessed at. This is an absolute path so that the disk can\r\nbe accessed on the system it is mounted on. The Movie Folder is the\r\nplace where all movie files are stored. Its path is stored relative to\r\nthe HDD Root. This is done so that the configuration remains the same\r\nacross systems. The organisation and placement of contents on disk\r\nremain the same irrespective of where it is attached to. The only thing\r\nthat changes is the place where the disk is accessed at. This is the HDD\r\nRoot path. Therefore, whenever the disk is attached to a system, the\r\nroot path needs to be configured. Sometimes, this path changes even when\r\nthe disk is attached to the same system on which the root path was\r\nconfigured. In the future, it possibly could be checked if the hdd is\r\nmounted on the same path or the root path has changed. This can be done\r\n(maybe) by checking the path of the server when it starts execution. If\r\nthe program is on the disk, the root path will be a substring of the\r\nserver execution path. Cases exist where this can be subverted with\r\ndirectories and symlinks made in such a manner so that to mimic a valid\r\ndisk path. But we can ignore this case since it is unlikely that someone\r\nwill pervert the configurations in this manner, and even if they do,\r\ntheir usage of the utility would be reduced, or possibly prevented.\r\n\r\nThe only working modules at this time are browse, crawl, and load. The\r\nbrowse module allows viewing of movies already present in database. It\r\nuses django\u2019s handy admin interface, which presents objects stored in\r\nthe database in a nice and simple tabular layout. The results can be\r\nsorted or filtered based on columns or fields. In this case, the movies\r\ncan be filtered by their release date or based on actors and directors.\r\nThe columns can be sorted based on their contents. The movies section\r\nshows the title, release date, their relative path from the movie\r\nfolder, imdb rating, rotten tomatoes rating, and metascore. By default,\r\nmovies are sorted by their title. Filtering by release dates supports\r\nyears and also months. It\u2019s certainly handy to check which of the latest\r\nmovies are on the disk. Filtering based on an actor or director allows\r\nme to check which of their movies are on the disk. All of these suffice\r\nthe need and use of a basic organisation of movies based on their\r\nmetadata.\r\n\r\nThe crawl module looks for movie files in the configured movie folder.\r\nIt identifies a video file as a movie based on its file extension (video\r\nfile) and file size (more than 300MB), and saves it to the database\r\nalong with its relative path. This relative path is calculated from the\r\nmovie folder. This abstraction allows the movie folder to be moved\r\naround, renamed, and copied while the movie paths remain consistent. To\r\nget the absolute path of any movie file, its relative path must be\r\nappended with the movie folder\u2019s relative path, which is then appended\r\nto the root path. While tedious to calculate and evaluate, this\r\nabstraction greatly simplifies the code when trying to get the absolute\r\npath of every file on different systems. The crawl module is usually\r\nfast and simple. Identified files are immediately added to the database.\r\nWhen viewed, they show only the filename as its title, and the relative\r\npath.\r\n\r\nThe load module downloads the metadata from online source such as the\r\nOpen Movie Database and The Movie Database. If a movie entry in database\r\nhas an IMDb ID associated with it, this is used to retrieve its\r\nmetadata. If this ID is absent, a search is performed based on the\r\nfilename, which is assumed to be its title. If a movie is identified,\r\nits IMDb ID is retrieved, and used to retrieved metadata. If the movie\r\ncannot be identified by its filename, Open Subtitles is used to identify\r\nthe movie and retrieve its IMDb ID. Open Subtitles is an online\r\nsubtitles library/database that can identify a movie by calculating a\r\nhash of the movie file and matching it within its database.\r\n\r\nThe crawl and load (or loader) module when started, creates a new\r\nnon-blocking asynchronous thread so as to keep the browser and other\r\noperations open for use. This thread initiates the necessary parameters\r\nfor accessing the online sources via their API. The actual downloading\r\nis done in individual threads, which currently number five (5). This\r\nnumber was selected as the best choice after testing with various number\r\nof threads (1,2,5,10,20). Low number of threads take more time, whereas\r\nlarge number of threads download the data much faster, but block the\r\ndatabase when writing. SQLite cannot handle this large load of\r\nwrite-locks and fails. To solve this, I only download the data in\r\nthreads, and when all threads have finished, save the movie data into\r\nthe database in a single thread. To simplify the object management,\r\nqueues are used to store objects as threads retrieve and operate on\r\nthem. If a large number of movie objects are in database, the queues and\r\ndownloaded data take up more memory, which causes the loader to become\r\nsluggish towards the latter half of operations. To prevent this, I put 5\r\nobjects in the queue at a time, the same number as there are threads.\r\nThis completes the operations in a faster time since each thread has\r\nless load to complete, and doesn\u2019t clog up the server or database\r\nmemory. The results and effects might (most likely will) change when a\r\ndifferent, dedicated database is used. What would be interesting is to\r\nuse a document database to store the movie data since it allows much\r\nmore freedom to store arbitrary fields. Using such a division of labor\r\nalso makes sure that if an error occurs that breaks the flow, the\r\nprevious objects are stored to database.\u00a0\r\n\r\nThe status of the loader is constantly reflected in the browser via\r\nGET/POST methods that send the movies evaluated, metadata downloaded,\r\nand movies skipped. The user has control of the operation through the\r\nSTART/STOP button in the browser. Sometimes, due to (currently unchecked\r\nand unverified) some cause, the loader gets stuck and will not continue.\r\nIt continues to run without downloading metadata. I call this as\r\noperation freezing, since it appears to have stuck in some operation,\r\nwhile still running and responding. In such cases, the process can be\r\nstopped via the button in the browser, and it (looks to be) seems safe\r\nto stop it at \u00a0any time.\r\n\r\nThe current version of HDD-indexer is certainly useful on its own, but\r\nthere are a lot of features and operations that are yet to be developed.\r\nI have several ideas as to what these could be, and how I can implement\r\nthem.", "headerimage": "", "highlight": "0", "section": 13}, {"id": 29, "title": "Choosing the right framework", "authors": "1", "date_created": "2015-06-30 23:00:00", "date_published": "2015-07-01 15:13:00", "date_updated": "2017-11-12 16:23:21", "is_published": "1", "short_description": "choosing the right framework for implementation", "tags": "40", "slug": "choosing-the-right-framework", "body_type": "markdown", "body_text": "<p>The main aim of the <strong>HDD-Indexer</strong> is to provide an easy to use, yet\npowerful and complete abstraction to the data on the disk. The main\npoint to focus on here is that this is an external disk. It can and\nprobably will be used on different systems. Therefore, the first\nrequirement of whatever technology we use is that it should\nbe\u00a0<strong>cross-platform</strong>.</p>\n<p>The first thing that comes up when someone says\u00a0<em>cross-platform</em>\u00a0is\njava, and for a good reason. Java has matured a great deal, and has\ndeveloped into a stable language that works great across any scale. But\nit isn't the most easy to develop with. That honor goes to languages\nlike python and ruby, that have a huge community of frameworks and\nlibraries that are just ready to deploy.\u00a0</p>\n<p>Along with the language, another major aspect is the GUI. The terminal\nis great, but to really have a visual usability of the data, we need a\nGUI. While there are quite a lot of frameworks and libraries that aid in\nmaking GUI apps, the best bet is to use something that every user has\ninstalled on their system - their browser. The browser provides the\nideal environment, because it is readily available and is pretty much\nconsistent across platforms. The variations are in individual standards\nand procedures adopted by the different browsers, which is quite\nacceptable.</p>\n<p>Deploying to a browser requires either statically compiled pages (html\nfiles) or a local web server. Static compiled pages are great, except\nthat they cannot change, or interact with the data outside of the\nprogrammed links. A local web server is much more powerful, because it\ncan accept requests, respond to them, and make changes (to the data on\ndisk) as required. Python readily comes up with a nifty web server\n(SimpleHTTPServer) that works great for local deployment of files. But\nthere are other powerful options that save development efforts and make\ndeployment easier.</p>\n<p>Web frameworks such as Django, Flask, and Pyramid allow creation of web\napps while saving a great deal of developer time. Django especially has\nbecome a lot popular due to its large number of features, while Flash\noffers a lightweight, barebones and quick method of deploying something.\nI'm going to choose <strong>Django</strong> simply because I want to finish and\niterate as quickly as possible.\u00a0Using a local web server also (kind of)\nimplies a local database. There's no better choice than <strong>SQLite</strong>\n(although there are a lot many NoSQL options coming up) to store the\ndata on disk. At most, it should take up a few MBs worth of space.\u00a0</p>\n<p>Deploying brings up issues of dependencies. What if python is not\ninstalled? Or the libraries are not installed? And so on. To address\nsuch issues, it would be better to use a portable version of python on\ndisk, and then download the dependencies as statically stored\nlibraries.</p>", "body": "The main aim of the **HDD-Indexer** is to provide an easy to use, yet\r\npowerful and complete abstraction to the data on the disk. The main\r\npoint to focus on here is that this is an external disk. It can and\r\nprobably will be used on different systems. Therefore, the first\r\nrequirement of whatever technology we use is that it should\r\nbe\u00a0**cross-platform**.\r\n\r\nThe first thing that comes up when someone says\u00a0*cross-platform*\u00a0is\r\njava, and for a good reason. Java has matured a great deal, and has\r\ndeveloped into a stable language that works great across any scale. But\r\nit isn't the most easy to develop with. That honor goes to languages\r\nlike python and ruby, that have a huge community of frameworks and\r\nlibraries that are just ready to deploy.\u00a0\r\n\r\nAlong with the language, another major aspect is the GUI. The terminal\r\nis great, but to really have a visual usability of the data, we need a\r\nGUI. While there are quite a lot of frameworks and libraries that aid in\r\nmaking GUI apps, the best bet is to use something that every user has\r\ninstalled on their system - their browser. The browser provides the\r\nideal environment, because it is readily available and is pretty much\r\nconsistent across platforms. The variations are in individual standards\r\nand procedures adopted by the different browsers, which is quite\r\nacceptable.\r\n\r\nDeploying to a browser requires either statically compiled pages (html\r\nfiles) or a local web server. Static compiled pages are great, except\r\nthat they cannot change, or interact with the data outside of the\r\nprogrammed links. A local web server is much more powerful, because it\r\ncan accept requests, respond to them, and make changes (to the data on\r\ndisk) as required. Python readily comes up with a nifty web server\r\n(SimpleHTTPServer) that works great for local deployment of files. But\r\nthere are other powerful options that save development efforts and make\r\ndeployment easier.\r\n\r\nWeb frameworks such as Django, Flask, and Pyramid allow creation of web\r\napps while saving a great deal of developer time. Django especially has\r\nbecome a lot popular due to its large number of features, while Flash\r\noffers a lightweight, barebones and quick method of deploying something.\r\nI'm going to choose **Django** simply because I want to finish and\r\niterate as quickly as possible.\u00a0Using a local web server also (kind of)\r\nimplies a local database. There's no better choice than **SQLite**\r\n(although there are a lot many NoSQL options coming up) to store the\r\ndata on disk. At most, it should take up a few MBs worth of space.\u00a0\r\n\r\nDeploying brings up issues of dependencies. What if python is not\r\ninstalled? Or the libraries are not installed? And so on. To address\r\nsuch issues, it would be better to use a portable version of python on\r\ndisk, and then download the dependencies as statically stored\r\nlibraries.", "headerimage": "", "highlight": "0", "section": 13}, {"id": 28, "title": "checking if time difference between stations are the same", "authors": "1", "date_created": "2015-09-16 23:00:00", "date_published": "2015-09-17 15:05:46", "date_updated": "2017-11-12 16:17:37", "is_published": "1", "short_description": "checking if the time differences between each train run and station is the same", "tags": "68", "slug": "checking-if-time-difference-between-stations-are-the-same", "body_type": "html", "body_text": "<p>When creating a system that automatically populates the timings for each station, it is vital to know whether timings remain constant for all the trains that run on that route. What I'm talking about is: does the time taken to get from station A to station B remain the same for all trains going from A to B ?</p>\r\n<p><br></p>\r\n<p>To check this, I scraped the time table from <a href=\\\"http://www.punediary.com/html/upside_local.html\\\" target=\\\"_blank\\\">punediary.com</a> using a tool called <code>scrapy</code>. The following is the output of the script that parses the scraped data, cleans it, and calculates the time difference for every station along with the average.</p>\r\n<p><br></p>\r\n<p>The code and data files are hosted at: <a href=\\\"https://github.com/coolharsh55/scrape-train-timings\\\" target=\\\"_blank\\\">scrape-train-timings (github.com/coolharsh55)</a></p>\r\n<p><br></p>\r\n<h3>---- UP LOCALS ----</h3>\r\n<ul>\r\n  <li>A: PUNE STATION</li>\r\n  <li>B: SHIVAJINAGAR</li>\r\n  <li>C: KHADKI</li>\r\n  <li>D: DAPODI</li>\r\n  <li>E: KASARWADI</li>\r\n  <li>F: PIMPRI</li>\r\n  <li>G: CHINCHWAD</li>\r\n  <li>H: AKURDI</li>\r\n  <li>I: DEHU ROAD</li>\r\n  <li>J: BEGDEWADI</li>\r\n  <li>K: GORAWADI</li>\r\n  <li>L: TALEGAON</li>\r\n  <li>M: WADGAON</li>\r\n  <li>N: KANHE</li>\r\n  <li>O: KAMSHET</li>\r\n  <li>P: MALWALI</li>\r\n  <li>Q: LONAWALA<span></span></li>\r\n</ul>\r\n<p><span style=\\\"font-family: monospace;\\\">train timings for up side locals</span></p>\r\n<table class=\"w-100 mw8 center\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>    </td>\r\n      <td>        A    </td>\r\n      <td>        B    </td>\r\n      <td>        C    </td>\r\n      <td>        D    </td>\r\n      <td>        E    </td>\r\n      <td>        F    </td>\r\n      <td>        G    </td>\r\n      <td>        H    </td>\r\n      <td>        I    </td>\r\n      <td>        J    </td>\r\n      <td>        K    </td>\r\n      <td>        L    </td>\r\n      <td>        M    </td>\r\n      <td>        N    </td>\r\n      <td>        O    </td>\r\n      <td>        P    </td>\r\n      <td>        Q    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">1    </td>\r\n      <td>00:10    </td>\r\n      <td>00:16    </td>\r\n      <td>00:21    </td>\r\n      <td>00:26    </td>\r\n      <td>00:30    </td>\r\n      <td>00:33    </td>\r\n      <td>00:37    </td>\r\n      <td>00:42    </td>\r\n      <td>00:47    </td>\r\n      <td>00:51    </td>\r\n      <td>00:54    </td>\r\n      <td>00:59    </td>\r\n      <td>01:04    </td>\r\n      <td>01:09    </td>\r\n      <td>01:13    </td>\r\n      <td>01:21    </td>\r\n      <td>01:35    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">2    </td>\r\n      <td>04:45    </td>\r\n      <td>04:51    </td>\r\n      <td>04:56    </td>\r\n      <td>05:01    </td>\r\n      <td>05:05    </td>\r\n      <td>05:08    </td>\r\n      <td>05:12    </td>\r\n      <td>05:17    </td>\r\n      <td>05:22    </td>\r\n      <td>05:26    </td>\r\n      <td>05:29    </td>\r\n      <td>05:34    </td>\r\n      <td>05:39    </td>\r\n      <td>05:44    </td>\r\n      <td>05:48    </td>\r\n      <td>05:56    </td>\r\n      <td>06:10    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">3    </td>\r\n      <td>05:45    </td>\r\n      <td>05:51    </td>\r\n      <td>05:56    </td>\r\n      <td>06:01    </td>\r\n      <td>06:05    </td>\r\n      <td>06:08    </td>\r\n      <td>06:12    </td>\r\n      <td>06:17    </td>\r\n      <td>06:22    </td>\r\n      <td>06:26    </td>\r\n      <td>06:29    </td>\r\n      <td>06:34    </td>\r\n      <td>06:39    </td>\r\n      <td>06:44    </td>\r\n      <td>06:48    </td>\r\n      <td>06:56    </td>\r\n      <td>07:10    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">4    </td>\r\n      <td>06:30    </td>\r\n      <td>06:36    </td>\r\n      <td>06:41    </td>\r\n      <td>06:46    </td>\r\n      <td>06:50    </td>\r\n      <td>06:53    </td>\r\n      <td>06:57    </td>\r\n      <td>07:02    </td>\r\n      <td>07:07    </td>\r\n      <td>07:11    </td>\r\n      <td>07:14    </td>\r\n      <td>07:19    </td>\r\n      <td>07:23    </td>\r\n      <td>07:29    </td>\r\n      <td>07:33    </td>\r\n      <td>07:41    </td>\r\n      <td>07:55    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">5    </td>\r\n      <td>06:50    </td>\r\n      <td>06:56    </td>\r\n      <td>07:01    </td>\r\n      <td>07:06    </td>\r\n      <td>07:10    </td>\r\n      <td>07:13    </td>\r\n      <td>07:17    </td>\r\n      <td>07:22    </td>\r\n      <td>07:27    </td>\r\n      <td>07:31    </td>\r\n      <td>07:34    </td>\r\n      <td>07:39    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">6    </td>\r\n      <td>08:05    </td>\r\n      <td>08:11    </td>\r\n      <td>08:16    </td>\r\n      <td>08:21    </td>\r\n      <td>08:25    </td>\r\n      <td>08:28    </td>\r\n      <td>08:32    </td>\r\n      <td>08:37    </td>\r\n      <td>08:42    </td>\r\n      <td>08:47    </td>\r\n      <td>08:49    </td>\r\n      <td>08:54    </td>\r\n      <td>08:59    </td>\r\n      <td>09:04    </td>\r\n      <td>09:08    </td>\r\n      <td>09:16    </td>\r\n      <td>09:30    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">7    </td>\r\n      <td>08:57    </td>\r\n      <td>09:03    </td>\r\n      <td>09:08    </td>\r\n      <td>09:13    </td>\r\n      <td>09:17    </td>\r\n      <td>09:20    </td>\r\n      <td>09:24    </td>\r\n      <td>09:29    </td>\r\n      <td>09:34    </td>\r\n      <td>09:38    </td>\r\n      <td>09:41    </td>\r\n      <td>09:46    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">8    </td>\r\n      <td>09:55    </td>\r\n      <td>10:01    </td>\r\n      <td>10:06    </td>\r\n      <td>10:11    </td>\r\n      <td>10:15    </td>\r\n      <td>10:18    </td>\r\n      <td>10:22    </td>\r\n      <td>10:27    </td>\r\n      <td>10:32    </td>\r\n      <td>10:36    </td>\r\n      <td>10:39    </td>\r\n      <td>10:45    </td>\r\n      <td>10:50    </td>\r\n      <td>10:54    </td>\r\n      <td>10:59    </td>\r\n      <td>11:07    </td>\r\n      <td>11:20    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">9    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td>10:50    </td>\r\n      <td>10:56    </td>\r\n      <td>11:01    </td>\r\n      <td>11:05    </td>\r\n      <td>11:08    </td>\r\n      <td>11:12    </td>\r\n      <td>11:17    </td>\r\n      <td>11:22    </td>\r\n      <td>11:26    </td>\r\n      <td>11:29    </td>\r\n      <td>11:35    </td>\r\n      <td>11:40    </td>\r\n      <td>11:44    </td>\r\n      <td>11:49    </td>\r\n      <td>11:57    </td>\r\n      <td>12:10    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">10    </td>\r\n      <td>11:55    </td>\r\n      <td>12:01    </td>\r\n      <td>12:06    </td>\r\n      <td>12:11    </td>\r\n      <td>12:15    </td>\r\n      <td>12:18    </td>\r\n      <td>12:22    </td>\r\n      <td>12:27    </td>\r\n      <td>12:32    </td>\r\n      <td>12:36    </td>\r\n      <td>12:39    </td>\r\n      <td>12:44    </td>\r\n      <td>12:49    </td>\r\n      <td>12:54    </td>\r\n      <td>12:59    </td>\r\n      <td>13:06    </td>\r\n      <td>13:20    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">11    </td>\r\n      <td>13:00    </td>\r\n      <td>13:06    </td>\r\n      <td>13:11    </td>\r\n      <td>13:16    </td>\r\n      <td>13:20    </td>\r\n      <td>13:23    </td>\r\n      <td>13:27    </td>\r\n      <td>13:32    </td>\r\n      <td>13:37    </td>\r\n      <td>13:41    </td>\r\n      <td>13:44    </td>\r\n      <td>13:49    </td>\r\n      <td>13:54    </td>\r\n      <td>13:59    </td>\r\n      <td>14:03    </td>\r\n      <td>14:12    </td>\r\n      <td>14:26    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">12    </td>\r\n      <td>15:00    </td>\r\n      <td>15:06    </td>\r\n      <td>15:11    </td>\r\n      <td>15:16    </td>\r\n      <td>15:20    </td>\r\n      <td>15:23    </td>\r\n      <td>15:27    </td>\r\n      <td>15:32    </td>\r\n      <td>15:37    </td>\r\n      <td>15:41    </td>\r\n      <td>15:44    </td>\r\n      <td>15:49    </td>\r\n      <td>15:54    </td>\r\n      <td>15:59    </td>\r\n      <td>16:03    </td>\r\n      <td>16:12    </td>\r\n      <td>16:26    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">13    </td>\r\n      <td>15:40    </td>\r\n      <td>15:46    </td>\r\n      <td>15:51    </td>\r\n      <td>15:56    </td>\r\n      <td>16:00    </td>\r\n      <td>16:03    </td>\r\n      <td>16:07    </td>\r\n      <td>16:12    </td>\r\n      <td>16:17    </td>\r\n      <td>16:21    </td>\r\n      <td>16:24    </td>\r\n      <td>16:29    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">14    </td>\r\n      <td>16:25    </td>\r\n      <td>16:31    </td>\r\n      <td>16:36    </td>\r\n      <td>16:41    </td>\r\n      <td>16:45    </td>\r\n      <td>16:47    </td>\r\n      <td>16:52    </td>\r\n      <td>16:57    </td>\r\n      <td>17:02    </td>\r\n      <td>17:06    </td>\r\n      <td>17:09    </td>\r\n      <td>17:14    </td>\r\n      <td>17:19    </td>\r\n      <td>17:24    </td>\r\n      <td>17:28    </td>\r\n      <td>17:36    </td>\r\n      <td>17:50    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">15    </td>\r\n      <td>17:10    </td>\r\n      <td>17:16    </td>\r\n      <td>17:21    </td>\r\n      <td>17:26    </td>\r\n      <td>17:30    </td>\r\n      <td>17:33    </td>\r\n      <td>17:37    </td>\r\n      <td>17:42    </td>\r\n      <td>17:47    </td>\r\n      <td>17:51    </td>\r\n      <td>17:54    </td>\r\n      <td>17:59    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">16    </td>\r\n      <td>18:02    </td>\r\n      <td>18:08    </td>\r\n      <td>18:13    </td>\r\n      <td>18:18    </td>\r\n      <td>18:22    </td>\r\n      <td>18:25    </td>\r\n      <td>18:29    </td>\r\n      <td>18:34    </td>\r\n      <td>18:39    </td>\r\n      <td>18:43    </td>\r\n      <td>18:46    </td>\r\n      <td>18:51    </td>\r\n      <td>18:56    </td>\r\n      <td>19:01    </td>\r\n      <td>19:05    </td>\r\n      <td>19:14    </td>\r\n      <td>19:28    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">17    </td>\r\n      <td>19:05    </td>\r\n      <td>19:11    </td>\r\n      <td>19:16    </td>\r\n      <td>19:21    </td>\r\n      <td>19:25    </td>\r\n      <td>19:28    </td>\r\n      <td>19:32    </td>\r\n      <td>19:37    </td>\r\n      <td>19:42    </td>\r\n      <td>19:46    </td>\r\n      <td>19:49    </td>\r\n      <td>19:54    </td>\r\n      <td>19:59    </td>\r\n      <td>20:04    </td>\r\n      <td>20:08    </td>\r\n      <td>20:16    </td>\r\n      <td>20:30    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">18    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td>19:35    </td>\r\n      <td>19:39    </td>\r\n      <td>19:44    </td>\r\n      <td>19:48    </td>\r\n      <td>19:51    </td>\r\n      <td>19:55    </td>\r\n      <td>20:00    </td>\r\n      <td>20:05    </td>\r\n      <td>20:10    </td>\r\n      <td>20:13    </td>\r\n      <td>20:18    </td>\r\n      <td>20:23    </td>\r\n      <td>20:28    </td>\r\n      <td>20:32    </td>\r\n      <td>20:40    </td>\r\n      <td>20:54    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">19    </td>\r\n      <td>20:00    </td>\r\n      <td>20:06    </td>\r\n      <td>20:11    </td>\r\n      <td>20:16    </td>\r\n      <td>20:20    </td>\r\n      <td>20:23    </td>\r\n      <td>20:27    </td>\r\n      <td>20:32    </td>\r\n      <td>20:37    </td>\r\n      <td>20:41    </td>\r\n      <td>20:44    </td>\r\n      <td>20:49    </td>\r\n      <td>20:54    </td>\r\n      <td>20:59    </td>\r\n      <td>21:03    </td>\r\n      <td>21:11    </td>\r\n      <td>21:25    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">20    </td>\r\n      <td>21:05    </td>\r\n      <td>21:11    </td>\r\n      <td>21:16    </td>\r\n      <td>21:21    </td>\r\n      <td>21:25    </td>\r\n      <td>21:27    </td>\r\n      <td>21:32    </td>\r\n      <td>21:37    </td>\r\n      <td>21:42    </td>\r\n      <td>21:46    </td>\r\n      <td>21:49    </td>\r\n      <td>21:54    </td>\r\n      <td>21:59    </td>\r\n      <td>22:04    </td>\r\n      <td>22:08    </td>\r\n      <td>22:16    </td>\r\n      <td>22:30    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">21    </td>\r\n      <td>22:10    </td>\r\n      <td>22:16    </td>\r\n      <td>22:21    </td>\r\n      <td>22:26    </td>\r\n      <td>22:30    </td>\r\n      <td>22:33    </td>\r\n      <td>22:37    </td>\r\n      <td>22:42    </td>\r\n      <td>22:47    </td>\r\n      <td>22:51    </td>\r\n      <td>22:54    </td>\r\n      <td>22:59    </td>\r\n      <td>23:04    </td>\r\n      <td>23:09    </td>\r\n      <td>23:13    </td>\r\n      <td>23:21    </td>\r\n      <td>23:35    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">22    </td>\r\n      <td>23:00    </td>\r\n      <td>23:06    </td>\r\n      <td>23:11    </td>\r\n      <td>23:16    </td>\r\n      <td>23:20    </td>\r\n      <td>23:23    </td>\r\n      <td>23:27    </td>\r\n      <td>23:32    </td>\r\n      <td>23:37    </td>\r\n      <td>23:41    </td>\r\n      <td>23:44    </td>\r\n      <td>23:49    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<p><span style=\\\"font-family: monospace;\\\">time difference between stations for locals going from Pune to Lonavala</span></p>\r\n<table class=\\\"table table-hover table-striped table-condensed table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>    </td>\r\n      <td>1    </td>\r\n      <td>2    </td>\r\n      <td>3    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n      <td>7    </td>\r\n      <td>8    </td>\r\n      <td>9    </td>\r\n      <td>10    </td>\r\n      <td>11    </td>\r\n      <td>12    </td>\r\n      <td>13    </td>\r\n      <td>14    </td>\r\n      <td>15    </td>\r\n      <td>16    </td>\r\n      <td>17    </td>\r\n      <td>18    </td>\r\n      <td>19    </td>\r\n      <td>20    </td>\r\n      <td>21    </td>\r\n      <td>22    </td>\r\n      <td>    AVG    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">A    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">B    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>0    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>0    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">C    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">D    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">E    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">F    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>2    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>2    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">G    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">H    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">I    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">J    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">K    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>2    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">L    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">M    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">N    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">O    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">P    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>0    </td>\r\n      <td>8    </td>\r\n      <td>0    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>7    </td>\r\n      <td>9    </td>\r\n      <td>9    </td>\r\n      <td>0    </td>\r\n      <td>8    </td>\r\n      <td>0    </td>\r\n      <td>9    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>0    </td>\r\n      <td>9    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">Q    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>13    </td>\r\n      <td>13    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>14    </td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<p><br></p>\r\n<p><span style=\\\"font-family: monospace;\\\">average time difference for trains going from Pune to Lonavala</span></p>\r\n<table class=\\\"table table-hover table-striped table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>FROM</td>\r\n      <td>TO</td>\r\n      <td>AVERAGE TIME</td>\r\n    </tr>\r\n    <tr>\r\n      <td>PUNESTATION</td>\r\n      <td>SHIVAJINAGAR</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td>SHIVAJINAGAR</td>\r\n      <td>KHADKI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KHADKI</td>\r\n      <td>DAPODI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>DAPODI</td>\r\n      <td>KASARWADI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KASARWADI</td>\r\n      <td>PIMPRI</td>\r\n      <td>3</td>\r\n    </tr>\r\n    <tr>\r\n      <td>PIMPRI</td>\r\n      <td>CHINCHWAD</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>CHINCHWAD</td>\r\n      <td>AKURDI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>AKURDI</td>\r\n      <td>DEHU</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>DEHU</td>\r\n      <td>ROAD</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>BEGDEWADI</td>\r\n      <td>GORAWADI</td>\r\n      <td>3</td>\r\n    </tr>\r\n    <tr>\r\n      <td>GORAWADI</td>\r\n      <td>TALEGAON</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td>TALEGAON</td>\r\n      <td>WADGAON</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>WADGAON</td>\r\n      <td>KANHE</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KANHE</td>\r\n      <td>KAMSHET</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KAMSHET</td>\r\n      <td>MALWALI</td>\r\n      <td>9</td>\r\n    </tr>\r\n    <tr>\r\n      <td>MALWALI</td>\r\n      <td>LONAWALA</td>\r\n      <td>4</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<span style=\\\"font-family: monospace;\\\">train timings for down side locals</span>\r\n<table class=\\\"table table-hover table-striped table-condensed table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>A</td>\r\n      <td>B</td>\r\n      <td>C</td>\r\n      <td>D</td>\r\n      <td>E</td>\r\n      <td>F</td>\r\n      <td>G</td>\r\n      <td>H</td>\r\n      <td>I</td>\r\n      <td>J</td>\r\n      <td>K</td>\r\n      <td>L</td>\r\n      <td>M</td>\r\n      <td>N</td>\r\n      <td>O</td>\r\n      <td>P</td>\r\n      <td>Q</td>\r\n      <td>AVG</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">1</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>00:05</td>\r\n      <td>00:09</td>\r\n      <td>00:13</td>\r\n      <td>00:17</td>\r\n      <td>00:21</td>\r\n      <td>00:26</td>\r\n      <td>00:30</td>\r\n      <td>00:33</td>\r\n      <td>00:37</td>\r\n      <td>00:41</td>\r\n      <td>00:46</td>\r\n      <td>00:58</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">2</td>\r\n      <td>05:20</td>\r\n      <td>05:29</td>\r\n      <td>05:37</td>\r\n      <td>05:41</td>\r\n      <td>05:46</td>\r\n      <td>05:52</td>\r\n      <td>05:56</td>\r\n      <td>06:00</td>\r\n      <td>06:04</td>\r\n      <td>06:08</td>\r\n      <td>06:13</td>\r\n      <td>06:17</td>\r\n      <td>06:20</td>\r\n      <td>06:24</td>\r\n      <td>06:28</td>\r\n      <td>06:33</td>\r\n      <td>06:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">3</td>\r\n      <td>06:20</td>\r\n      <td>06:29</td>\r\n      <td>06:37</td>\r\n      <td>06:41</td>\r\n      <td>06:46</td>\r\n      <td>06:52</td>\r\n      <td>06:56</td>\r\n      <td>07:00</td>\r\n      <td>07:04</td>\r\n      <td>07:08</td>\r\n      <td>07:13</td>\r\n      <td>07:17</td>\r\n      <td>07:20</td>\r\n      <td>07:24</td>\r\n      <td>07:28</td>\r\n      <td>07:33</td>\r\n      <td>07:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">4</td>\r\n      <td>07:20</td>\r\n      <td>07:29</td>\r\n      <td>07:37</td>\r\n      <td>07:41</td>\r\n      <td>07:46</td>\r\n      <td>07:52</td>\r\n      <td>07:56</td>\r\n      <td>08:00</td>\r\n      <td>08:04</td>\r\n      <td>08:08</td>\r\n      <td>08:13</td>\r\n      <td>08:17</td>\r\n      <td>08:20</td>\r\n      <td>08:24</td>\r\n      <td>08:28</td>\r\n      <td>08:33</td>\r\n      <td>08:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">5</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>07:50</td>\r\n      <td>07:54</td>\r\n      <td>07:58</td>\r\n      <td>08:02</td>\r\n      <td>08:06</td>\r\n      <td>08:11</td>\r\n      <td>08:15</td>\r\n      <td>08:18</td>\r\n      <td>08:22</td>\r\n      <td>08:26</td>\r\n      <td>08:31</td>\r\n      <td>08:43</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">6</td>\r\n      <td>08:20</td>\r\n      <td>08:29</td>\r\n      <td>08:37</td>\r\n      <td>08:41</td>\r\n      <td>08:46</td>\r\n      <td>08:52</td>\r\n      <td>08:56</td>\r\n      <td>09:00</td>\r\n      <td>09:04</td>\r\n      <td>09:08</td>\r\n      <td>09:13</td>\r\n      <td>09:17</td>\r\n      <td>09:20</td>\r\n      <td>09:24</td>\r\n      <td>09:28</td>\r\n      <td>09:33</td>\r\n      <td>09:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">7</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>09:57</td>\r\n      <td>10:01</td>\r\n      <td>10:08</td>\r\n      <td>10:12</td>\r\n      <td>10:16</td>\r\n      <td>10:21</td>\r\n      <td>10:25</td>\r\n      <td>10:28</td>\r\n      <td>10:32</td>\r\n      <td>10:36</td>\r\n      <td>10:41</td>\r\n      <td>XX:XX</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">8</td>\r\n      <td>09:35</td>\r\n      <td>09:44</td>\r\n      <td>09:52</td>\r\n      <td>09:56</td>\r\n      <td>10:01</td>\r\n      <td>10:07</td>\r\n      <td>10:11</td>\r\n      <td>10:15</td>\r\n      <td>10:19</td>\r\n      <td>10:23</td>\r\n      <td>10:28</td>\r\n      <td>10:32</td>\r\n      <td>10:35</td>\r\n      <td>10:39</td>\r\n      <td>10:43</td>\r\n      <td>10:48</td>\r\n      <td>11:00</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">9</td>\r\n      <td>11:20</td>\r\n      <td>11:29</td>\r\n      <td>11:37</td>\r\n      <td>11:41</td>\r\n      <td>11:46</td>\r\n      <td>11:52</td>\r\n      <td>11:56</td>\r\n      <td>12:00</td>\r\n      <td>12:04</td>\r\n      <td>12:08</td>\r\n      <td>12:13</td>\r\n      <td>12:17</td>\r\n      <td>12:20</td>\r\n      <td>12:24</td>\r\n      <td>12:28</td>\r\n      <td>12:33</td>\r\n      <td>12:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">10</td>\r\n      <td>14:00</td>\r\n      <td>14:09</td>\r\n      <td>14:17</td>\r\n      <td>14:21</td>\r\n      <td>14:26</td>\r\n      <td>14:32</td>\r\n      <td>14:36</td>\r\n      <td>14:40</td>\r\n      <td>14:44</td>\r\n      <td>14:48</td>\r\n      <td>14:53</td>\r\n      <td>14:56</td>\r\n      <td>15:00</td>\r\n      <td>15:04</td>\r\n      <td>15:08</td>\r\n      <td>15:13</td>\r\n      <td>15:25</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">11</td>\r\n      <td>14:55</td>\r\n      <td>15:04</td>\r\n      <td>15:12</td>\r\n      <td>15:16</td>\r\n      <td>15:22</td>\r\n      <td>15:28</td>\r\n      <td>15:32</td>\r\n      <td>15:36</td>\r\n      <td>15:40</td>\r\n      <td>15:44</td>\r\n      <td>15:49</td>\r\n      <td>15:53</td>\r\n      <td>15:57</td>\r\n      <td>16:02</td>\r\n      <td>16:06</td>\r\n      <td>16:09</td>\r\n      <td>16:21</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">12</td>\r\n      <td>15:45</td>\r\n      <td>15:54</td>\r\n      <td>16:02</td>\r\n      <td>16:06</td>\r\n      <td>16:11</td>\r\n      <td>16:17</td>\r\n      <td>16:21</td>\r\n      <td>16:25</td>\r\n      <td>16:29</td>\r\n      <td>16:33</td>\r\n      <td>16:38</td>\r\n      <td>16:42</td>\r\n      <td>16:45</td>\r\n      <td>16:49</td>\r\n      <td>16:53</td>\r\n      <td>16:58</td>\r\n      <td>17:10</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">13</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>16:40</td>\r\n      <td>16:44</td>\r\n      <td>16:48</td>\r\n      <td>16:52</td>\r\n      <td>16:56</td>\r\n      <td>17:01</td>\r\n      <td>17:05</td>\r\n      <td>17:08</td>\r\n      <td>17:12</td>\r\n      <td>17:16</td>\r\n      <td>17:21</td>\r\n      <td>17:33</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">14</td>\r\n      <td>17:25</td>\r\n      <td>17:34</td>\r\n      <td>17:42</td>\r\n      <td>17:46</td>\r\n      <td>17:51</td>\r\n      <td>17:57</td>\r\n      <td>18:01</td>\r\n      <td>18:05</td>\r\n      <td>18:09</td>\r\n      <td>18:13</td>\r\n      <td>18:18</td>\r\n      <td>18:22</td>\r\n      <td>18:27</td>\r\n      <td>18:29</td>\r\n      <td>18:33</td>\r\n      <td>18:38</td>\r\n      <td>18:50</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">15</td>\r\n      <td>18:20</td>\r\n      <td>18:29</td>\r\n      <td>18:37</td>\r\n      <td>18:41</td>\r\n      <td>18:46</td>\r\n      <td>18:52</td>\r\n      <td>18:56</td>\r\n      <td>19:00</td>\r\n      <td>19:04</td>\r\n      <td>19:08</td>\r\n      <td>19:13</td>\r\n      <td>19:17</td>\r\n      <td>19:20</td>\r\n      <td>19:24</td>\r\n      <td>19:28</td>\r\n      <td>19:33</td>\r\n      <td>19:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">16</td>\r\n      <td>19:35</td>\r\n      <td>19:44</td>\r\n      <td>19:52</td>\r\n      <td>19:56</td>\r\n      <td>20:01</td>\r\n      <td>20:07</td>\r\n      <td>20:11</td>\r\n      <td>20:15</td>\r\n      <td>20:19</td>\r\n      <td>20:23</td>\r\n      <td>20:28</td>\r\n      <td>20:32</td>\r\n      <td>20:35</td>\r\n      <td>20:39</td>\r\n      <td>20:43</td>\r\n      <td>20:48</td>\r\n      <td>21:00</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">17</td>\r\n      <td>20:40</td>\r\n      <td>20:49</td>\r\n      <td>20:57</td>\r\n      <td>21:01</td>\r\n      <td>21:06</td>\r\n      <td>21:11</td>\r\n      <td>21:16</td>\r\n      <td>21:20</td>\r\n      <td>21:24</td>\r\n      <td>21:28</td>\r\n      <td>21:33</td>\r\n      <td>21:37</td>\r\n      <td>21:40</td>\r\n      <td>21:44</td>\r\n      <td>21:48</td>\r\n      <td>21:54</td>\r\n      <td>22:06</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">18</td>\r\n      <td>21:15</td>\r\n      <td>21:24</td>\r\n      <td>21:32</td>\r\n      <td>21:36</td>\r\n      <td>21:41</td>\r\n      <td>21:47</td>\r\n      <td>21:51</td>\r\n      <td>21:55</td>\r\n      <td>21:59</td>\r\n      <td>22:03</td>\r\n      <td>22:08</td>\r\n      <td>22:12</td>\r\n      <td>22:15</td>\r\n      <td>22:19</td>\r\n      <td>22:23</td>\r\n      <td>22:28</td>\r\n      <td>22:40</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">19</td>\r\n      <td>22:05</td>\r\n      <td>22:14</td>\r\n      <td>22:22</td>\r\n      <td>22:26</td>\r\n      <td>22:31</td>\r\n      <td>22:37</td>\r\n      <td>22:41</td>\r\n      <td>22:45</td>\r\n      <td>22:49</td>\r\n      <td>22:53</td>\r\n      <td>22:58</td>\r\n      <td>23:02</td>\r\n      <td>23:05</td>\r\n      <td>23:09</td>\r\n      <td>23:13</td>\r\n      <td>23:18</td>\r\n      <td>23:30</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">20</td>\r\n      <td>22:35</td>\r\n      <td>22:44</td>\r\n      <td>22:52</td>\r\n      <td>22:56</td>\r\n      <td>23:01</td>\r\n      <td>23:07</td>\r\n      <td>23:11</td>\r\n      <td>23:15</td>\r\n      <td>23:19</td>\r\n      <td>23:23</td>\r\n      <td>23:28</td>\r\n      <td>23:32</td>\r\n      <td>23:35</td>\r\n      <td>23:39</td>\r\n      <td>23:43</td>\r\n      <td>23:48</td>\r\n      <td>00:00</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">21</td>\r\n      <td>23:45</td>\r\n      <td>23:54</td>\r\n      <td>00:02</td>\r\n      <td>00:06</td>\r\n      <td>00:11</td>\r\n      <td>00:17</td>\r\n      <td>00:21</td>\r\n      <td>00:25</td>\r\n      <td>00:29</td>\r\n      <td>00:33</td>\r\n      <td>00:38</td>\r\n      <td>00:42</td>\r\n      <td>00:45</td>\r\n      <td>00:49</td>\r\n      <td>00:53</td>\r\n      <td>00:58</td>\r\n      <td>01:10</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<span style=\\\"font-family: monospace;\\\">time difference between stations for locals going from Lonavala to Pune</span>\r\n<table class=\\\"table table-hover table-striped table-condensed table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td></td>\r\n      <td>1</td>\r\n      <td>2</td>\r\n      <td>3</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n      <td>7</td>\r\n      <td>8</td>\r\n      <td>9</td>\r\n      <td>10</td>\r\n      <td>11</td>\r\n      <td>12</td>\r\n      <td>13</td>\r\n      <td>14</td>\r\n      <td>15</td>\r\n      <td>16</td>\r\n      <td>17</td>\r\n      <td>18</td>\r\n      <td>19</td>\r\n      <td>20</td>\r\n      <td>21</td>\r\n      <td>AVG</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">A</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">B</td>\r\n      <td>0</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>0</td>\r\n      <td>9</td>\r\n      <td>0</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>0</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">C</td>\r\n      <td>0</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>0</td>\r\n      <td>8</td>\r\n      <td>0</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>0</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">D</td>\r\n      <td>0</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>0</td>\r\n      <td>4</td>\r\n      <td>0</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>0</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">E</td>\r\n      <td>0</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>0</td>\r\n      <td>5</td>\r\n      <td>0</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n      <td>5</td>\r\n      <td>0</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">F</td>\r\n      <td>0</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>0</td>\r\n      <td>6</td>\r\n      <td>0</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>0</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">G</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">H</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>7</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">I</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">J</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">K</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">L</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>3</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">M</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>5</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">N</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>2</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">O</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">P</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>3</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">Q</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>0</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<p><br></p>\r\n<p><span style=\\\"font-family: monospace;\\\">average time difference for trains going from Lonavala to Pune</span></p>\r\n<table class=\\\"table table-hover table-striped table-condensed table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>FROM</td>\r\n      <td>TO</td>\r\n      <td>AVERAGE TIME</td>\r\n    </tr>\r\n    <tr>\r\n      <td>LONAWALA</td>\r\n      <td>MALWALI</td>\r\n      <td>9</td>\r\n    </tr>\r\n    <tr>\r\n      <td>MALWALI</td>\r\n      <td>KAMSHET</td>\r\n      <td>8</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KAMSHET</td>\r\n      <td>KANHE</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KANHE</td>\r\n      <td>WADGAON</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td>WADGAON</td>\r\n      <td>TALEGAON</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td>TALEGAON</td>\r\n      <td>GORAWADI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>GORAWADI</td>\r\n      <td>BEGDEWADI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>BEGDEWADI</td>\r\n      <td>DEHUROAD</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>DEHUROAD</td>\r\n      <td>AKURDI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>AKURDI</td>\r\n      <td>CHINCHWAD</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>CHINCHWAD</td>\r\n      <td>PIMPRI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>PIMPRI</td>\r\n      <td>KASARWADI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KASARWADI</td>\r\n      <td>DAPODI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>DAPODI</td>\r\n      <td>KHADKI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KHADKI</td>\r\n      <td>SHIVAJINAGAR</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>SHIVAJINAGAR</td>\r\n      <td>PUNE STATION</td>\r\n      <td>12</td>\r\n    </tr>\r\n  </tbody>\r\n</table>", "body": "<p>When creating a system that automatically populates the timings for each station, it is vital to know whether timings remain constant for all the trains that run on that route. What I'm talking about is: does the time taken to get from station A to station B remain the same for all trains going from A to B ?</p>\r\n<p><br></p>\r\n<p>To check this, I scraped the time table from <a href=\\\"http://www.punediary.com/html/upside_local.html\\\" target=\\\"_blank\\\">punediary.com</a> using a tool called <code>scrapy</code>. The following is the output of the script that parses the scraped data, cleans it, and calculates the time difference for every station along with the average.</p>\r\n<p><br></p>\r\n<p>The code and data files are hosted at: <a href=\\\"https://github.com/coolharsh55/scrape-train-timings\\\" target=\\\"_blank\\\">scrape-train-timings (github.com/coolharsh55)</a></p>\r\n<p><br></p>\r\n<h3>---- UP LOCALS ----</h3>\r\n<ul>\r\n  <li>A: PUNE STATION</li>\r\n  <li>B: SHIVAJINAGAR</li>\r\n  <li>C: KHADKI</li>\r\n  <li>D: DAPODI</li>\r\n  <li>E: KASARWADI</li>\r\n  <li>F: PIMPRI</li>\r\n  <li>G: CHINCHWAD</li>\r\n  <li>H: AKURDI</li>\r\n  <li>I: DEHU ROAD</li>\r\n  <li>J: BEGDEWADI</li>\r\n  <li>K: GORAWADI</li>\r\n  <li>L: TALEGAON</li>\r\n  <li>M: WADGAON</li>\r\n  <li>N: KANHE</li>\r\n  <li>O: KAMSHET</li>\r\n  <li>P: MALWALI</li>\r\n  <li>Q: LONAWALA<span></span></li>\r\n</ul>\r\n<p><span style=\\\"font-family: monospace;\\\">train timings for up side locals</span></p>\r\n<table class=\"w-100 mw8 center\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>    </td>\r\n      <td>        A    </td>\r\n      <td>        B    </td>\r\n      <td>        C    </td>\r\n      <td>        D    </td>\r\n      <td>        E    </td>\r\n      <td>        F    </td>\r\n      <td>        G    </td>\r\n      <td>        H    </td>\r\n      <td>        I    </td>\r\n      <td>        J    </td>\r\n      <td>        K    </td>\r\n      <td>        L    </td>\r\n      <td>        M    </td>\r\n      <td>        N    </td>\r\n      <td>        O    </td>\r\n      <td>        P    </td>\r\n      <td>        Q    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">1    </td>\r\n      <td>00:10    </td>\r\n      <td>00:16    </td>\r\n      <td>00:21    </td>\r\n      <td>00:26    </td>\r\n      <td>00:30    </td>\r\n      <td>00:33    </td>\r\n      <td>00:37    </td>\r\n      <td>00:42    </td>\r\n      <td>00:47    </td>\r\n      <td>00:51    </td>\r\n      <td>00:54    </td>\r\n      <td>00:59    </td>\r\n      <td>01:04    </td>\r\n      <td>01:09    </td>\r\n      <td>01:13    </td>\r\n      <td>01:21    </td>\r\n      <td>01:35    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">2    </td>\r\n      <td>04:45    </td>\r\n      <td>04:51    </td>\r\n      <td>04:56    </td>\r\n      <td>05:01    </td>\r\n      <td>05:05    </td>\r\n      <td>05:08    </td>\r\n      <td>05:12    </td>\r\n      <td>05:17    </td>\r\n      <td>05:22    </td>\r\n      <td>05:26    </td>\r\n      <td>05:29    </td>\r\n      <td>05:34    </td>\r\n      <td>05:39    </td>\r\n      <td>05:44    </td>\r\n      <td>05:48    </td>\r\n      <td>05:56    </td>\r\n      <td>06:10    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">3    </td>\r\n      <td>05:45    </td>\r\n      <td>05:51    </td>\r\n      <td>05:56    </td>\r\n      <td>06:01    </td>\r\n      <td>06:05    </td>\r\n      <td>06:08    </td>\r\n      <td>06:12    </td>\r\n      <td>06:17    </td>\r\n      <td>06:22    </td>\r\n      <td>06:26    </td>\r\n      <td>06:29    </td>\r\n      <td>06:34    </td>\r\n      <td>06:39    </td>\r\n      <td>06:44    </td>\r\n      <td>06:48    </td>\r\n      <td>06:56    </td>\r\n      <td>07:10    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">4    </td>\r\n      <td>06:30    </td>\r\n      <td>06:36    </td>\r\n      <td>06:41    </td>\r\n      <td>06:46    </td>\r\n      <td>06:50    </td>\r\n      <td>06:53    </td>\r\n      <td>06:57    </td>\r\n      <td>07:02    </td>\r\n      <td>07:07    </td>\r\n      <td>07:11    </td>\r\n      <td>07:14    </td>\r\n      <td>07:19    </td>\r\n      <td>07:23    </td>\r\n      <td>07:29    </td>\r\n      <td>07:33    </td>\r\n      <td>07:41    </td>\r\n      <td>07:55    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">5    </td>\r\n      <td>06:50    </td>\r\n      <td>06:56    </td>\r\n      <td>07:01    </td>\r\n      <td>07:06    </td>\r\n      <td>07:10    </td>\r\n      <td>07:13    </td>\r\n      <td>07:17    </td>\r\n      <td>07:22    </td>\r\n      <td>07:27    </td>\r\n      <td>07:31    </td>\r\n      <td>07:34    </td>\r\n      <td>07:39    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">6    </td>\r\n      <td>08:05    </td>\r\n      <td>08:11    </td>\r\n      <td>08:16    </td>\r\n      <td>08:21    </td>\r\n      <td>08:25    </td>\r\n      <td>08:28    </td>\r\n      <td>08:32    </td>\r\n      <td>08:37    </td>\r\n      <td>08:42    </td>\r\n      <td>08:47    </td>\r\n      <td>08:49    </td>\r\n      <td>08:54    </td>\r\n      <td>08:59    </td>\r\n      <td>09:04    </td>\r\n      <td>09:08    </td>\r\n      <td>09:16    </td>\r\n      <td>09:30    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">7    </td>\r\n      <td>08:57    </td>\r\n      <td>09:03    </td>\r\n      <td>09:08    </td>\r\n      <td>09:13    </td>\r\n      <td>09:17    </td>\r\n      <td>09:20    </td>\r\n      <td>09:24    </td>\r\n      <td>09:29    </td>\r\n      <td>09:34    </td>\r\n      <td>09:38    </td>\r\n      <td>09:41    </td>\r\n      <td>09:46    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">8    </td>\r\n      <td>09:55    </td>\r\n      <td>10:01    </td>\r\n      <td>10:06    </td>\r\n      <td>10:11    </td>\r\n      <td>10:15    </td>\r\n      <td>10:18    </td>\r\n      <td>10:22    </td>\r\n      <td>10:27    </td>\r\n      <td>10:32    </td>\r\n      <td>10:36    </td>\r\n      <td>10:39    </td>\r\n      <td>10:45    </td>\r\n      <td>10:50    </td>\r\n      <td>10:54    </td>\r\n      <td>10:59    </td>\r\n      <td>11:07    </td>\r\n      <td>11:20    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">9    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td>10:50    </td>\r\n      <td>10:56    </td>\r\n      <td>11:01    </td>\r\n      <td>11:05    </td>\r\n      <td>11:08    </td>\r\n      <td>11:12    </td>\r\n      <td>11:17    </td>\r\n      <td>11:22    </td>\r\n      <td>11:26    </td>\r\n      <td>11:29    </td>\r\n      <td>11:35    </td>\r\n      <td>11:40    </td>\r\n      <td>11:44    </td>\r\n      <td>11:49    </td>\r\n      <td>11:57    </td>\r\n      <td>12:10    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">10    </td>\r\n      <td>11:55    </td>\r\n      <td>12:01    </td>\r\n      <td>12:06    </td>\r\n      <td>12:11    </td>\r\n      <td>12:15    </td>\r\n      <td>12:18    </td>\r\n      <td>12:22    </td>\r\n      <td>12:27    </td>\r\n      <td>12:32    </td>\r\n      <td>12:36    </td>\r\n      <td>12:39    </td>\r\n      <td>12:44    </td>\r\n      <td>12:49    </td>\r\n      <td>12:54    </td>\r\n      <td>12:59    </td>\r\n      <td>13:06    </td>\r\n      <td>13:20    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">11    </td>\r\n      <td>13:00    </td>\r\n      <td>13:06    </td>\r\n      <td>13:11    </td>\r\n      <td>13:16    </td>\r\n      <td>13:20    </td>\r\n      <td>13:23    </td>\r\n      <td>13:27    </td>\r\n      <td>13:32    </td>\r\n      <td>13:37    </td>\r\n      <td>13:41    </td>\r\n      <td>13:44    </td>\r\n      <td>13:49    </td>\r\n      <td>13:54    </td>\r\n      <td>13:59    </td>\r\n      <td>14:03    </td>\r\n      <td>14:12    </td>\r\n      <td>14:26    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">12    </td>\r\n      <td>15:00    </td>\r\n      <td>15:06    </td>\r\n      <td>15:11    </td>\r\n      <td>15:16    </td>\r\n      <td>15:20    </td>\r\n      <td>15:23    </td>\r\n      <td>15:27    </td>\r\n      <td>15:32    </td>\r\n      <td>15:37    </td>\r\n      <td>15:41    </td>\r\n      <td>15:44    </td>\r\n      <td>15:49    </td>\r\n      <td>15:54    </td>\r\n      <td>15:59    </td>\r\n      <td>16:03    </td>\r\n      <td>16:12    </td>\r\n      <td>16:26    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">13    </td>\r\n      <td>15:40    </td>\r\n      <td>15:46    </td>\r\n      <td>15:51    </td>\r\n      <td>15:56    </td>\r\n      <td>16:00    </td>\r\n      <td>16:03    </td>\r\n      <td>16:07    </td>\r\n      <td>16:12    </td>\r\n      <td>16:17    </td>\r\n      <td>16:21    </td>\r\n      <td>16:24    </td>\r\n      <td>16:29    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">14    </td>\r\n      <td>16:25    </td>\r\n      <td>16:31    </td>\r\n      <td>16:36    </td>\r\n      <td>16:41    </td>\r\n      <td>16:45    </td>\r\n      <td>16:47    </td>\r\n      <td>16:52    </td>\r\n      <td>16:57    </td>\r\n      <td>17:02    </td>\r\n      <td>17:06    </td>\r\n      <td>17:09    </td>\r\n      <td>17:14    </td>\r\n      <td>17:19    </td>\r\n      <td>17:24    </td>\r\n      <td>17:28    </td>\r\n      <td>17:36    </td>\r\n      <td>17:50    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">15    </td>\r\n      <td>17:10    </td>\r\n      <td>17:16    </td>\r\n      <td>17:21    </td>\r\n      <td>17:26    </td>\r\n      <td>17:30    </td>\r\n      <td>17:33    </td>\r\n      <td>17:37    </td>\r\n      <td>17:42    </td>\r\n      <td>17:47    </td>\r\n      <td>17:51    </td>\r\n      <td>17:54    </td>\r\n      <td>17:59    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">16    </td>\r\n      <td>18:02    </td>\r\n      <td>18:08    </td>\r\n      <td>18:13    </td>\r\n      <td>18:18    </td>\r\n      <td>18:22    </td>\r\n      <td>18:25    </td>\r\n      <td>18:29    </td>\r\n      <td>18:34    </td>\r\n      <td>18:39    </td>\r\n      <td>18:43    </td>\r\n      <td>18:46    </td>\r\n      <td>18:51    </td>\r\n      <td>18:56    </td>\r\n      <td>19:01    </td>\r\n      <td>19:05    </td>\r\n      <td>19:14    </td>\r\n      <td>19:28    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">17    </td>\r\n      <td>19:05    </td>\r\n      <td>19:11    </td>\r\n      <td>19:16    </td>\r\n      <td>19:21    </td>\r\n      <td>19:25    </td>\r\n      <td>19:28    </td>\r\n      <td>19:32    </td>\r\n      <td>19:37    </td>\r\n      <td>19:42    </td>\r\n      <td>19:46    </td>\r\n      <td>19:49    </td>\r\n      <td>19:54    </td>\r\n      <td>19:59    </td>\r\n      <td>20:04    </td>\r\n      <td>20:08    </td>\r\n      <td>20:16    </td>\r\n      <td>20:30    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">18    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td>19:35    </td>\r\n      <td>19:39    </td>\r\n      <td>19:44    </td>\r\n      <td>19:48    </td>\r\n      <td>19:51    </td>\r\n      <td>19:55    </td>\r\n      <td>20:00    </td>\r\n      <td>20:05    </td>\r\n      <td>20:10    </td>\r\n      <td>20:13    </td>\r\n      <td>20:18    </td>\r\n      <td>20:23    </td>\r\n      <td>20:28    </td>\r\n      <td>20:32    </td>\r\n      <td>20:40    </td>\r\n      <td>20:54    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">19    </td>\r\n      <td>20:00    </td>\r\n      <td>20:06    </td>\r\n      <td>20:11    </td>\r\n      <td>20:16    </td>\r\n      <td>20:20    </td>\r\n      <td>20:23    </td>\r\n      <td>20:27    </td>\r\n      <td>20:32    </td>\r\n      <td>20:37    </td>\r\n      <td>20:41    </td>\r\n      <td>20:44    </td>\r\n      <td>20:49    </td>\r\n      <td>20:54    </td>\r\n      <td>20:59    </td>\r\n      <td>21:03    </td>\r\n      <td>21:11    </td>\r\n      <td>21:25    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">20    </td>\r\n      <td>21:05    </td>\r\n      <td>21:11    </td>\r\n      <td>21:16    </td>\r\n      <td>21:21    </td>\r\n      <td>21:25    </td>\r\n      <td>21:27    </td>\r\n      <td>21:32    </td>\r\n      <td>21:37    </td>\r\n      <td>21:42    </td>\r\n      <td>21:46    </td>\r\n      <td>21:49    </td>\r\n      <td>21:54    </td>\r\n      <td>21:59    </td>\r\n      <td>22:04    </td>\r\n      <td>22:08    </td>\r\n      <td>22:16    </td>\r\n      <td>22:30    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">21    </td>\r\n      <td>22:10    </td>\r\n      <td>22:16    </td>\r\n      <td>22:21    </td>\r\n      <td>22:26    </td>\r\n      <td>22:30    </td>\r\n      <td>22:33    </td>\r\n      <td>22:37    </td>\r\n      <td>22:42    </td>\r\n      <td>22:47    </td>\r\n      <td>22:51    </td>\r\n      <td>22:54    </td>\r\n      <td>22:59    </td>\r\n      <td>23:04    </td>\r\n      <td>23:09    </td>\r\n      <td>23:13    </td>\r\n      <td>23:21    </td>\r\n      <td>23:35    </td>\r\n    </tr>\r\n    <tr>    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">22    </td>\r\n      <td>23:00    </td>\r\n      <td>23:06    </td>\r\n      <td>23:11    </td>\r\n      <td>23:16    </td>\r\n      <td>23:20    </td>\r\n      <td>23:23    </td>\r\n      <td>23:27    </td>\r\n      <td>23:32    </td>\r\n      <td>23:37    </td>\r\n      <td>23:41    </td>\r\n      <td>23:44    </td>\r\n      <td>23:49    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n      <td class=\\\"alert\\\">XX:XX    </td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<p><span style=\\\"font-family: monospace;\\\">time difference between stations for locals going from Pune to Lonavala</span></p>\r\n<table class=\\\"table table-hover table-striped table-condensed table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>    </td>\r\n      <td>1    </td>\r\n      <td>2    </td>\r\n      <td>3    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n      <td>7    </td>\r\n      <td>8    </td>\r\n      <td>9    </td>\r\n      <td>10    </td>\r\n      <td>11    </td>\r\n      <td>12    </td>\r\n      <td>13    </td>\r\n      <td>14    </td>\r\n      <td>15    </td>\r\n      <td>16    </td>\r\n      <td>17    </td>\r\n      <td>18    </td>\r\n      <td>19    </td>\r\n      <td>20    </td>\r\n      <td>21    </td>\r\n      <td>22    </td>\r\n      <td>    AVG    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">A    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n      <td>0    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">B    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>0    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>0    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">C    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">D    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">E    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">F    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>2    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>2    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">G    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">H    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">I    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">J    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">K    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>2    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n      <td>3    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">L    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n      <td>6    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">M    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">N    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>6    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">O    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>5    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>4    </td>\r\n      <td>0    </td>\r\n      <td>5    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">P    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>0    </td>\r\n      <td>8    </td>\r\n      <td>0    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>7    </td>\r\n      <td>9    </td>\r\n      <td>9    </td>\r\n      <td>0    </td>\r\n      <td>8    </td>\r\n      <td>0    </td>\r\n      <td>9    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>8    </td>\r\n      <td>0    </td>\r\n      <td>9    </td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">Q    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>13    </td>\r\n      <td>13    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>14    </td>\r\n      <td>0    </td>\r\n      <td>14    </td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<p><br></p>\r\n<p><span style=\\\"font-family: monospace;\\\">average time difference for trains going from Pune to Lonavala</span></p>\r\n<table class=\\\"table table-hover table-striped table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>FROM</td>\r\n      <td>TO</td>\r\n      <td>AVERAGE TIME</td>\r\n    </tr>\r\n    <tr>\r\n      <td>PUNESTATION</td>\r\n      <td>SHIVAJINAGAR</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td>SHIVAJINAGAR</td>\r\n      <td>KHADKI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KHADKI</td>\r\n      <td>DAPODI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>DAPODI</td>\r\n      <td>KASARWADI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KASARWADI</td>\r\n      <td>PIMPRI</td>\r\n      <td>3</td>\r\n    </tr>\r\n    <tr>\r\n      <td>PIMPRI</td>\r\n      <td>CHINCHWAD</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>CHINCHWAD</td>\r\n      <td>AKURDI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>AKURDI</td>\r\n      <td>DEHU</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>DEHU</td>\r\n      <td>ROAD</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>BEGDEWADI</td>\r\n      <td>GORAWADI</td>\r\n      <td>3</td>\r\n    </tr>\r\n    <tr>\r\n      <td>GORAWADI</td>\r\n      <td>TALEGAON</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td>TALEGAON</td>\r\n      <td>WADGAON</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>WADGAON</td>\r\n      <td>KANHE</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KANHE</td>\r\n      <td>KAMSHET</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KAMSHET</td>\r\n      <td>MALWALI</td>\r\n      <td>9</td>\r\n    </tr>\r\n    <tr>\r\n      <td>MALWALI</td>\r\n      <td>LONAWALA</td>\r\n      <td>4</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<span style=\\\"font-family: monospace;\\\">train timings for down side locals</span>\r\n<table class=\\\"table table-hover table-striped table-condensed table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>A</td>\r\n      <td>B</td>\r\n      <td>C</td>\r\n      <td>D</td>\r\n      <td>E</td>\r\n      <td>F</td>\r\n      <td>G</td>\r\n      <td>H</td>\r\n      <td>I</td>\r\n      <td>J</td>\r\n      <td>K</td>\r\n      <td>L</td>\r\n      <td>M</td>\r\n      <td>N</td>\r\n      <td>O</td>\r\n      <td>P</td>\r\n      <td>Q</td>\r\n      <td>AVG</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">1</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>00:05</td>\r\n      <td>00:09</td>\r\n      <td>00:13</td>\r\n      <td>00:17</td>\r\n      <td>00:21</td>\r\n      <td>00:26</td>\r\n      <td>00:30</td>\r\n      <td>00:33</td>\r\n      <td>00:37</td>\r\n      <td>00:41</td>\r\n      <td>00:46</td>\r\n      <td>00:58</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">2</td>\r\n      <td>05:20</td>\r\n      <td>05:29</td>\r\n      <td>05:37</td>\r\n      <td>05:41</td>\r\n      <td>05:46</td>\r\n      <td>05:52</td>\r\n      <td>05:56</td>\r\n      <td>06:00</td>\r\n      <td>06:04</td>\r\n      <td>06:08</td>\r\n      <td>06:13</td>\r\n      <td>06:17</td>\r\n      <td>06:20</td>\r\n      <td>06:24</td>\r\n      <td>06:28</td>\r\n      <td>06:33</td>\r\n      <td>06:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">3</td>\r\n      <td>06:20</td>\r\n      <td>06:29</td>\r\n      <td>06:37</td>\r\n      <td>06:41</td>\r\n      <td>06:46</td>\r\n      <td>06:52</td>\r\n      <td>06:56</td>\r\n      <td>07:00</td>\r\n      <td>07:04</td>\r\n      <td>07:08</td>\r\n      <td>07:13</td>\r\n      <td>07:17</td>\r\n      <td>07:20</td>\r\n      <td>07:24</td>\r\n      <td>07:28</td>\r\n      <td>07:33</td>\r\n      <td>07:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">4</td>\r\n      <td>07:20</td>\r\n      <td>07:29</td>\r\n      <td>07:37</td>\r\n      <td>07:41</td>\r\n      <td>07:46</td>\r\n      <td>07:52</td>\r\n      <td>07:56</td>\r\n      <td>08:00</td>\r\n      <td>08:04</td>\r\n      <td>08:08</td>\r\n      <td>08:13</td>\r\n      <td>08:17</td>\r\n      <td>08:20</td>\r\n      <td>08:24</td>\r\n      <td>08:28</td>\r\n      <td>08:33</td>\r\n      <td>08:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">5</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>07:50</td>\r\n      <td>07:54</td>\r\n      <td>07:58</td>\r\n      <td>08:02</td>\r\n      <td>08:06</td>\r\n      <td>08:11</td>\r\n      <td>08:15</td>\r\n      <td>08:18</td>\r\n      <td>08:22</td>\r\n      <td>08:26</td>\r\n      <td>08:31</td>\r\n      <td>08:43</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">6</td>\r\n      <td>08:20</td>\r\n      <td>08:29</td>\r\n      <td>08:37</td>\r\n      <td>08:41</td>\r\n      <td>08:46</td>\r\n      <td>08:52</td>\r\n      <td>08:56</td>\r\n      <td>09:00</td>\r\n      <td>09:04</td>\r\n      <td>09:08</td>\r\n      <td>09:13</td>\r\n      <td>09:17</td>\r\n      <td>09:20</td>\r\n      <td>09:24</td>\r\n      <td>09:28</td>\r\n      <td>09:33</td>\r\n      <td>09:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">7</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>09:57</td>\r\n      <td>10:01</td>\r\n      <td>10:08</td>\r\n      <td>10:12</td>\r\n      <td>10:16</td>\r\n      <td>10:21</td>\r\n      <td>10:25</td>\r\n      <td>10:28</td>\r\n      <td>10:32</td>\r\n      <td>10:36</td>\r\n      <td>10:41</td>\r\n      <td>XX:XX</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">8</td>\r\n      <td>09:35</td>\r\n      <td>09:44</td>\r\n      <td>09:52</td>\r\n      <td>09:56</td>\r\n      <td>10:01</td>\r\n      <td>10:07</td>\r\n      <td>10:11</td>\r\n      <td>10:15</td>\r\n      <td>10:19</td>\r\n      <td>10:23</td>\r\n      <td>10:28</td>\r\n      <td>10:32</td>\r\n      <td>10:35</td>\r\n      <td>10:39</td>\r\n      <td>10:43</td>\r\n      <td>10:48</td>\r\n      <td>11:00</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">9</td>\r\n      <td>11:20</td>\r\n      <td>11:29</td>\r\n      <td>11:37</td>\r\n      <td>11:41</td>\r\n      <td>11:46</td>\r\n      <td>11:52</td>\r\n      <td>11:56</td>\r\n      <td>12:00</td>\r\n      <td>12:04</td>\r\n      <td>12:08</td>\r\n      <td>12:13</td>\r\n      <td>12:17</td>\r\n      <td>12:20</td>\r\n      <td>12:24</td>\r\n      <td>12:28</td>\r\n      <td>12:33</td>\r\n      <td>12:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">10</td>\r\n      <td>14:00</td>\r\n      <td>14:09</td>\r\n      <td>14:17</td>\r\n      <td>14:21</td>\r\n      <td>14:26</td>\r\n      <td>14:32</td>\r\n      <td>14:36</td>\r\n      <td>14:40</td>\r\n      <td>14:44</td>\r\n      <td>14:48</td>\r\n      <td>14:53</td>\r\n      <td>14:56</td>\r\n      <td>15:00</td>\r\n      <td>15:04</td>\r\n      <td>15:08</td>\r\n      <td>15:13</td>\r\n      <td>15:25</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">11</td>\r\n      <td>14:55</td>\r\n      <td>15:04</td>\r\n      <td>15:12</td>\r\n      <td>15:16</td>\r\n      <td>15:22</td>\r\n      <td>15:28</td>\r\n      <td>15:32</td>\r\n      <td>15:36</td>\r\n      <td>15:40</td>\r\n      <td>15:44</td>\r\n      <td>15:49</td>\r\n      <td>15:53</td>\r\n      <td>15:57</td>\r\n      <td>16:02</td>\r\n      <td>16:06</td>\r\n      <td>16:09</td>\r\n      <td>16:21</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">12</td>\r\n      <td>15:45</td>\r\n      <td>15:54</td>\r\n      <td>16:02</td>\r\n      <td>16:06</td>\r\n      <td>16:11</td>\r\n      <td>16:17</td>\r\n      <td>16:21</td>\r\n      <td>16:25</td>\r\n      <td>16:29</td>\r\n      <td>16:33</td>\r\n      <td>16:38</td>\r\n      <td>16:42</td>\r\n      <td>16:45</td>\r\n      <td>16:49</td>\r\n      <td>16:53</td>\r\n      <td>16:58</td>\r\n      <td>17:10</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">13</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>XX:XX</td>\r\n      <td>16:40</td>\r\n      <td>16:44</td>\r\n      <td>16:48</td>\r\n      <td>16:52</td>\r\n      <td>16:56</td>\r\n      <td>17:01</td>\r\n      <td>17:05</td>\r\n      <td>17:08</td>\r\n      <td>17:12</td>\r\n      <td>17:16</td>\r\n      <td>17:21</td>\r\n      <td>17:33</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">14</td>\r\n      <td>17:25</td>\r\n      <td>17:34</td>\r\n      <td>17:42</td>\r\n      <td>17:46</td>\r\n      <td>17:51</td>\r\n      <td>17:57</td>\r\n      <td>18:01</td>\r\n      <td>18:05</td>\r\n      <td>18:09</td>\r\n      <td>18:13</td>\r\n      <td>18:18</td>\r\n      <td>18:22</td>\r\n      <td>18:27</td>\r\n      <td>18:29</td>\r\n      <td>18:33</td>\r\n      <td>18:38</td>\r\n      <td>18:50</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">15</td>\r\n      <td>18:20</td>\r\n      <td>18:29</td>\r\n      <td>18:37</td>\r\n      <td>18:41</td>\r\n      <td>18:46</td>\r\n      <td>18:52</td>\r\n      <td>18:56</td>\r\n      <td>19:00</td>\r\n      <td>19:04</td>\r\n      <td>19:08</td>\r\n      <td>19:13</td>\r\n      <td>19:17</td>\r\n      <td>19:20</td>\r\n      <td>19:24</td>\r\n      <td>19:28</td>\r\n      <td>19:33</td>\r\n      <td>19:45</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">16</td>\r\n      <td>19:35</td>\r\n      <td>19:44</td>\r\n      <td>19:52</td>\r\n      <td>19:56</td>\r\n      <td>20:01</td>\r\n      <td>20:07</td>\r\n      <td>20:11</td>\r\n      <td>20:15</td>\r\n      <td>20:19</td>\r\n      <td>20:23</td>\r\n      <td>20:28</td>\r\n      <td>20:32</td>\r\n      <td>20:35</td>\r\n      <td>20:39</td>\r\n      <td>20:43</td>\r\n      <td>20:48</td>\r\n      <td>21:00</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">17</td>\r\n      <td>20:40</td>\r\n      <td>20:49</td>\r\n      <td>20:57</td>\r\n      <td>21:01</td>\r\n      <td>21:06</td>\r\n      <td>21:11</td>\r\n      <td>21:16</td>\r\n      <td>21:20</td>\r\n      <td>21:24</td>\r\n      <td>21:28</td>\r\n      <td>21:33</td>\r\n      <td>21:37</td>\r\n      <td>21:40</td>\r\n      <td>21:44</td>\r\n      <td>21:48</td>\r\n      <td>21:54</td>\r\n      <td>22:06</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">18</td>\r\n      <td>21:15</td>\r\n      <td>21:24</td>\r\n      <td>21:32</td>\r\n      <td>21:36</td>\r\n      <td>21:41</td>\r\n      <td>21:47</td>\r\n      <td>21:51</td>\r\n      <td>21:55</td>\r\n      <td>21:59</td>\r\n      <td>22:03</td>\r\n      <td>22:08</td>\r\n      <td>22:12</td>\r\n      <td>22:15</td>\r\n      <td>22:19</td>\r\n      <td>22:23</td>\r\n      <td>22:28</td>\r\n      <td>22:40</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">19</td>\r\n      <td>22:05</td>\r\n      <td>22:14</td>\r\n      <td>22:22</td>\r\n      <td>22:26</td>\r\n      <td>22:31</td>\r\n      <td>22:37</td>\r\n      <td>22:41</td>\r\n      <td>22:45</td>\r\n      <td>22:49</td>\r\n      <td>22:53</td>\r\n      <td>22:58</td>\r\n      <td>23:02</td>\r\n      <td>23:05</td>\r\n      <td>23:09</td>\r\n      <td>23:13</td>\r\n      <td>23:18</td>\r\n      <td>23:30</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">20</td>\r\n      <td>22:35</td>\r\n      <td>22:44</td>\r\n      <td>22:52</td>\r\n      <td>22:56</td>\r\n      <td>23:01</td>\r\n      <td>23:07</td>\r\n      <td>23:11</td>\r\n      <td>23:15</td>\r\n      <td>23:19</td>\r\n      <td>23:23</td>\r\n      <td>23:28</td>\r\n      <td>23:32</td>\r\n      <td>23:35</td>\r\n      <td>23:39</td>\r\n      <td>23:43</td>\r\n      <td>23:48</td>\r\n      <td>00:00</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">21</td>\r\n      <td>23:45</td>\r\n      <td>23:54</td>\r\n      <td>00:02</td>\r\n      <td>00:06</td>\r\n      <td>00:11</td>\r\n      <td>00:17</td>\r\n      <td>00:21</td>\r\n      <td>00:25</td>\r\n      <td>00:29</td>\r\n      <td>00:33</td>\r\n      <td>00:38</td>\r\n      <td>00:42</td>\r\n      <td>00:45</td>\r\n      <td>00:49</td>\r\n      <td>00:53</td>\r\n      <td>00:58</td>\r\n      <td>01:10</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<span style=\\\"font-family: monospace;\\\">time difference between stations for locals going from Lonavala to Pune</span>\r\n<table class=\\\"table table-hover table-striped table-condensed table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td></td>\r\n      <td>1</td>\r\n      <td>2</td>\r\n      <td>3</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n      <td>7</td>\r\n      <td>8</td>\r\n      <td>9</td>\r\n      <td>10</td>\r\n      <td>11</td>\r\n      <td>12</td>\r\n      <td>13</td>\r\n      <td>14</td>\r\n      <td>15</td>\r\n      <td>16</td>\r\n      <td>17</td>\r\n      <td>18</td>\r\n      <td>19</td>\r\n      <td>20</td>\r\n      <td>21</td>\r\n      <td>AVG</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">A</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n      <td>0</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">B</td>\r\n      <td>0</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>0</td>\r\n      <td>9</td>\r\n      <td>0</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>0</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n      <td>9</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">C</td>\r\n      <td>0</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>0</td>\r\n      <td>8</td>\r\n      <td>0</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>0</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n      <td>8</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">D</td>\r\n      <td>0</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>0</td>\r\n      <td>4</td>\r\n      <td>0</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>0</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">E</td>\r\n      <td>0</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>0</td>\r\n      <td>5</td>\r\n      <td>0</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n      <td>5</td>\r\n      <td>0</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">F</td>\r\n      <td>0</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>0</td>\r\n      <td>6</td>\r\n      <td>0</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>0</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">G</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">H</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>7</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">I</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">J</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">K</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">L</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>3</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">M</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>5</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>3</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">N</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>5</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>2</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">O</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">P</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>3</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>6</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td class=\\\"info\\\">Q</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>0</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n      <td>12</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n<p><br></p>\r\n<p><span style=\\\"font-family: monospace;\\\">average time difference for trains going from Lonavala to Pune</span></p>\r\n<table class=\\\"table table-hover table-striped table-condensed table-bordered\\\">\r\n  <tbody>\r\n    <tr class=\\\"info\\\">\r\n      <td>FROM</td>\r\n      <td>TO</td>\r\n      <td>AVERAGE TIME</td>\r\n    </tr>\r\n    <tr>\r\n      <td>LONAWALA</td>\r\n      <td>MALWALI</td>\r\n      <td>9</td>\r\n    </tr>\r\n    <tr>\r\n      <td>MALWALI</td>\r\n      <td>KAMSHET</td>\r\n      <td>8</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KAMSHET</td>\r\n      <td>KANHE</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KANHE</td>\r\n      <td>WADGAON</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td>WADGAON</td>\r\n      <td>TALEGAON</td>\r\n      <td>6</td>\r\n    </tr>\r\n    <tr>\r\n      <td>TALEGAON</td>\r\n      <td>GORAWADI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>GORAWADI</td>\r\n      <td>BEGDEWADI</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>BEGDEWADI</td>\r\n      <td>DEHUROAD</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>DEHUROAD</td>\r\n      <td>AKURDI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>AKURDI</td>\r\n      <td>CHINCHWAD</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>CHINCHWAD</td>\r\n      <td>PIMPRI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>PIMPRI</td>\r\n      <td>KASARWADI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KASARWADI</td>\r\n      <td>DAPODI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>DAPODI</td>\r\n      <td>KHADKI</td>\r\n      <td>4</td>\r\n    </tr>\r\n    <tr>\r\n      <td>KHADKI</td>\r\n      <td>SHIVAJINAGAR</td>\r\n      <td>5</td>\r\n    </tr>\r\n    <tr>\r\n      <td>SHIVAJINAGAR</td>\r\n      <td>PUNE STATION</td>\r\n      <td>12</td>\r\n    </tr>\r\n  </tbody>\r\n</table>", "headerimage": "", "highlight": "0", "section": 12}, {"id": 27, "title": "handling special cases", "authors": "1", "date_created": "2015-09-16 23:00:00", "date_published": "2015-09-17 14:59:23", "date_updated": "2017-11-12 16:14:35", "is_published": "1", "short_description": "handling some of the special cases  in the train schedule", "tags": "68", "slug": "handling-special-cases", "body_type": "markdown", "body_text": "<p>So far, I've assumed that the data being stored (timings) is static. But\nin reality, trains often run under special circumstances. Where I stay,\nI often see cases like:</p>\n<ul>\n<li>Train A does not stop at X station</li>\n<li>Train B has its timing changed to ### until DD/MM/YY</li>\n<li>Train C will only run until station M until DD/MM/YY</li>\n<li><span></span>and so on...</li>\n</ul>\n<p>There cannot be a generic solution that can solve all such cases in one\nplace. So let me take each one and see how to solve it. If patterns\nemerge, awesome! If not, I have to look into how to solve such cases in\nthe easiest way possible.</p>\n<p>If train A does not stop at a particular station, it's timing value for\nthat station can be set to NULL, which is a special notation meaning <em>no\nvalue</em>. When creating the timings for a train, this particular condition\nshould be checked, and the value set to NULL where applicable.</p>\n<pre class=\"codehilite\"><code>add_train(train):\n  no_stop_stations = \n    select station from special_cases\n  where \n    train=train and condition=no_stop\n  # ... rest of the code\n  for station in station_list:\n    if station in no_stop_stations:\n      timing.time = NULL</code></pre>\n\n\n<p>When the train's timings have changed temporarily, the old timings can\nbe saved and the new ones applied.</p>\n<pre class=\"codehilite\"><code>change_time_temporary(\n  train, new_time, until_date):\n  # save the old timing to a temporary record\n  save_to_temp(train.train_no, train.timing)\n  # delete all timings for this train\n  delete timings where train=train\n  # save new timings\n  train.timings = new_timeadd_train(train)</code></pre>\n\n\n<p>The temporary record can be a file, or a separate table reserved for\nsuch instances. Any form of persistence where the old train timings can\nbe saved and later restored can work. Restoring is pretty much the same\nas saving, where the old values are retrieved, the record is deleted\nfrom the temp table. The timings are deleted for this particular train,\nand then added again with the old values.</p>\n<p>For cases, where a train will run only until a particular station until\nDD/MM/YY (it won't run till its assigned destination), the case is\nsimilar, where instead of saving timing to a temp table, we save the\noriginal destination.</p>\n<pre class=\"codehilite\"><code>change_destination_temporary(\n  train, new_destination, until_date):\n  # save the old destination to a temporary record\n  save_to_temp(train.train_no, train.destination)\n  # delete all timings and add the train again\n  delete all timings where train=train\n  # save train with new destination\n  train.destination = new_destination\n  # get stations based on this new destination\n  add_train(train)</code></pre>\n\n\n<p>The real problem here would be applying these cases to the data model.\nOne way is to hard code such instances into the code itself. Another way\nis to expose a set of API parameters which would allow building queries\nfor such special cases. This requires significant work, but allows a\nlarge variety in the special cases that can be handled.</p>\n<p>A set of choices can be provided to build the queries, such as selecting\na <em>train_no</em> in the first column and one of these in the second:</p>\n<ul>\n<li>DOES NOT STOP -&gt; next field would be a station selector</li>\n<li>TIMING CHANGED -&gt; next field would be a time field</li>\n<li>RUNS UNTIL -&gt; next field would be a station selector</li>\n</ul>\n<p>It is possible to store both the station id and time values in a single\nfield in cases where the time field values are stored as integers\n(seconds since epoch, for e.g.). If not, they can be stored as binary\nobjects, or <em>blobs</em>, and cast to the correct type when required. A\nfourth column, called <em>ORIGINAL_VALUE</em><span\nclass=\"\\\"redactor-invisible-space\\\"\"> can also be added to the table to\nstore the original values. This keeps all associated values in one\nplace, and does not require temporary tables.</span></p>", "body": "So far, I've assumed that the data being stored (timings) is static. But\r\nin reality, trains often run under special circumstances. Where I stay,\r\nI often see cases like:\r\n\r\n-   Train A does not stop at X station\r\n-   Train B has its timing changed to \\#\\#\\# until DD/MM/YY\r\n-   Train C will only run until station M until DD/MM/YY\r\n-   <span></span>and so on...\r\n\r\nThere cannot be a generic solution that can solve all such cases in one\r\nplace. So let me take each one and see how to solve it. If patterns\r\nemerge, awesome! If not, I have to look into how to solve such cases in\r\nthe easiest way possible.\r\n\r\nIf train A does not stop at a particular station, it's timing value for\r\nthat station can be set to NULL, which is a special notation meaning *no\r\nvalue*. When creating the timings for a train, this particular condition\r\nshould be checked, and the value set to NULL where applicable.\r\n```\r\nadd_train(train):\r\n  no_stop_stations = \r\n    select station from special_cases\r\n  where \r\n    train=train and condition=no_stop\r\n  # ... rest of the code\r\n  for station in station_list:\r\n    if station in no_stop_stations:\r\n      timing.time = NULL\r\n```\r\n\r\nWhen the train's timings have changed temporarily, the old timings can\r\nbe saved and the new ones applied.\r\n```\r\nchange_time_temporary(\r\n  train, new_time, until_date):\r\n  # save the old timing to a temporary record\r\n  save_to_temp(train.train_no, train.timing)\r\n  # delete all timings for this train\r\n  delete timings where train=train\r\n  # save new timings\r\n  train.timings = new_timeadd_train(train)\r\n```\r\nThe temporary record can be a file, or a separate table reserved for\r\nsuch instances. Any form of persistence where the old train timings can\r\nbe saved and later restored can work. Restoring is pretty much the same\r\nas saving, where the old values are retrieved, the record is deleted\r\nfrom the temp table. The timings are deleted for this particular train,\r\nand then added again with the old values.\r\n\r\nFor cases, where a train will run only until a particular station until\r\nDD/MM/YY (it won't run till its assigned destination), the case is\r\nsimilar, where instead of saving timing to a temp table, we save the\r\noriginal destination.\r\n```\r\nchange_destination_temporary(\r\n  train, new_destination, until_date):\r\n  # save the old destination to a temporary record\r\n  save_to_temp(train.train_no, train.destination)\r\n  # delete all timings and add the train again\r\n  delete all timings where train=train\r\n  # save train with new destination\r\n  train.destination = new_destination\r\n  # get stations based on this new destination\r\n  add_train(train)\r\n```\r\n\r\nThe real problem here would be applying these cases to the data model.\r\nOne way is to hard code such instances into the code itself. Another way\r\nis to expose a set of API parameters which would allow building queries\r\nfor such special cases. This requires significant work, but allows a\r\nlarge variety in the special cases that can be handled.\r\n\r\nA set of choices can be provided to build the queries, such as selecting\r\na *train\\_no* in the first column and one of these in the second:\r\n\r\n-   DOES NOT STOP -&gt; next field would be a station selector\r\n-   TIMING CHANGED -&gt; next field would be a time field\r\n-   RUNS UNTIL -&gt; next field would be a station selector\r\n\r\nIt is possible to store both the station id and time values in a single\r\nfield in cases where the time field values are stored as integers\r\n(seconds since epoch, for e.g.). If not, they can be stored as binary\r\nobjects, or *blobs*, and cast to the correct type when required. A\r\nfourth column, called *ORIGINAL\\_VALUE*<span\r\nclass=\"\\\"redactor-invisible-space\\\"\"> can also be added to the table to\r\nstore the original values. This keeps all associated values in one\r\nplace, and does not require temporary tables.</span>", "headerimage": "", "highlight": "0", "section": 12}, {"id": 26, "title": "populating train schedule", "authors": "1", "date_created": "2015-09-16 23:00:00", "date_published": "2015-09-17 14:58:12", "date_updated": "2017-11-12 16:08:49", "is_published": "1", "short_description": "Populating the train schedule and directions", "tags": "68", "slug": "populating-train-schedule", "body_type": "markdown", "body_text": "<p>Once the <strong>Station</strong> list has been fed into the database, along with the\nassociated properties (next and previous), the <strong>Trains</strong> can be added to the database as\nwell. The <strong>Timing</strong> objects\nfor each station can be created automatically once a train has been\nadded. The concept of up/down as\ndirections amongst trains is a trivial, but useful one.</p>\n<p>While adding the train, it's\ntiming, train_no, initial station and destination will be the inputs.\nThe ID will be assigned automatically by the system since we have no\nneed to set it explicitly.</p>\n<pre class=\"codehilite\"><code>post_save for Train:\n  if train.initial_station.id &gt; train.destination.id:\n  # direction is UP lonavala to pune\n    add_up_train(train)\n  else:\n  # direction is DOWN: pune to lonavala\n    add_down_train(train)\n\n  add_up_train:\n    station_list = [\n      stations where \n      id&lt;=train.initial_station.id \n      and id&gt;=train.destination ]\n    ( order in descending order )\n    time = train.time \n    # start time\n    for station in station_list:\n      timing.train = train\n      timing.station = station\n      timing.time = time\n     time = time + station.time_to_next\n\n  add_down_train:\n    station_list = [ \n      stations where id&lt;=train.destination\n      and id&gt;=train.initial_station ]\n      ( order in ascending order )\n      time = train.time # start time\n      for station in station_list:\n        timing = Timing()\n        timing.train = train\n        timing.station = station\n        timing.time = time\n        timing.save()\n        time = time + station.time_to_prev</code></pre>\n\n\n<p>This will populate the train\ntimings based on the train (up/down). It can be further refactored using\nabstraction for getting the station list and calculating the time\nbetween stops. For example, lambdas can be used and passed to a generic\nadd_train function:</p>\n<pre class=\"codehilite\"><code>add_train(train):\n  if train.initial_station.id &gt; train.destination.id:\n  # UP\n    top = train.initial_station.id\n    bottom = train.destination.id\n    ordering = descending\n    stop = lambda station: station.time_to_next\n  else:\n  # DOWN\n    top = train.destination.id\n    bottom = train.initial_station.id\n    ordering = ascending\n    stop = lambda station: station.time_to_prev\n    station_list = [\n      stations where id&lt;=top\n      and id&gt;=bottom ]\n      ( order by ordering )\n    time = train.time\n    for station in station_list:\n        timing.train = train\n        timing.station = station\n        timing.time = time\n        timing.save()\n        time = time + stop(station)</code></pre>\n\n\n<p>This results in roughly the same\ncode, but saves duplication where unnecessary. If the train is being\n<strong>modified</strong>, then instead of modifying all the timings, it is easier to\ndelete them all and repopulate the timing table with the new\nentries.</p>\n<pre class=\"codehilite\"><code>add_train(train):\n  if train in [ \n    select train in timings ]\n  ( group by train )\n  # train already has timings, so let's remove them first\n  delete all timings\n    where train=train\n  # ... rest of the code remains the same</code></pre>", "body": "Once the **Station** list has been fed into the database, along with the\r\nassociated properties (next and previous), the **Trains** can be added to the database as\r\nwell. The **Timing** objects\r\nfor each station can be created automatically once a train has been\r\nadded. The concept of up/down as\r\ndirections amongst trains is a trivial, but useful one.\r\n\r\nWhile adding the train, it's\r\ntiming, train_no, initial station and destination will be the inputs.\r\nThe ID will be assigned automatically by the system since we have no\r\nneed to set it explicitly.\r\n```\r\npost_save for Train:\r\n  if train.initial_station.id > train.destination.id:\r\n  # direction is UP lonavala to pune\r\n    add_up_train(train)\r\n  else:\r\n  # direction is DOWN: pune to lonavala\r\n    add_down_train(train)\r\n\r\n  add_up_train:\r\n    station_list = [\r\n      stations where \r\n      id<=train.initial_station.id \r\n      and id>=train.destination ]\r\n    ( order in descending order )\r\n    time = train.time \r\n    # start time\r\n    for station in station_list:\r\n      timing.train = train\r\n      timing.station = station\r\n      timing.time = time\r\n     time = time + station.time_to_next\r\n\r\n  add_down_train:\r\n    station_list = [ \r\n      stations where id<=train.destination\r\n      and id>=train.initial_station ]\r\n      ( order in ascending order )\r\n      time = train.time # start time\r\n      for station in station_list:\r\n        timing = Timing()\r\n        timing.train = train\r\n        timing.station = station\r\n        timing.time = time\r\n        timing.save()\r\n        time = time + station.time_to_prev\r\n```\r\n\r\nThis will populate the train\r\ntimings based on the train (up/down). It can be further refactored using\r\nabstraction for getting the station list and calculating the time\r\nbetween stops. For example, lambdas can be used and passed to a generic\r\nadd\\_train function:\r\n```\r\nadd_train(train):\r\n  if train.initial_station.id > train.destination.id:\r\n  # UP\r\n    top = train.initial_station.id\r\n    bottom = train.destination.id\r\n    ordering = descending\r\n    stop = lambda station: station.time_to_next\r\n  else:\r\n  # DOWN\r\n    top = train.destination.id\r\n    bottom = train.initial_station.id\r\n    ordering = ascending\r\n    stop = lambda station: station.time_to_prev\r\n    station_list = [\r\n      stations where id<=top\r\n      and id>=bottom ]\r\n      ( order by ordering )\r\n    time = train.time\r\n    for station in station_list:\r\n        timing.train = train\r\n        timing.station = station\r\n        timing.time = time\r\n        timing.save()\r\n        time = time + stop(station)\r\n```\r\nThis results in roughly the same\r\ncode, but saves duplication where unnecessary. If the train is being\r\n**modified**, then instead of modifying all the timings, it is easier to\r\ndelete them all and repopulate the timing table with the new\r\nentries.\r\n```\r\nadd_train(train):\r\n  if train in [ \r\n    select train in timings ]\r\n  ( group by train )\r\n  # train already has timings, so let's remove them first\r\n  delete all timings\r\n    where train=train\r\n  # ... rest of the code remains the same\r\n```", "headerimage": "", "highlight": "0", "section": 12}, {"id": 25, "title": "designing data models for trains, timings, and station", "authors": "1", "date_created": "2015-09-16 23:00:00", "date_published": "2015-09-17 14:43:53", "date_updated": "2017-11-12 16:00:09", "is_published": "1", "short_description": "A model for updating train timetables", "tags": "68", "slug": "designing-data-models-for-trains-timings-and-station", "body_type": "markdown", "body_text": "<h3 id=\"train-stations\">Train Stations</h3>\n<p>The train stations are setup as follows:</p>\n<ol>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Pune</span></li>\n<li><span style=\"\\&quot;font-family:\"\n    monospace;\\\"=\"\">Shivajinagar</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Khadki</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Dapodi</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Kasarwadi</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Pimpri</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Chinchwad</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Akurdi</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Dehuroad</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Begdewadi</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Ghorawadi</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Talegaon</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Vadgaon</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Kanhe</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Kamshet</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Malavali</span></li>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Lonavala</span></li>\n</ol>\n<p>Each station has an id number (the same as the index in above list) that\nis used as its unique identifier. Storing and retrieving data in a\ndatabase is easier with <em>primary keys</em>, which are like the unique\nidentifiers for each record. If the user doesn't supply one, the\ndatabase creates one automatically. In this case, explicitly setting the\nID is beneficial as it allows nifty queries such as\n<code>select stations where id&lt;=15;</code> would give all stations from Pune to\nKamshet.</p>\n<p>Each station has three properties:</p>\n<ol>\n<li><span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Name</span></li>\n<li>Time taken to reach this station from the previous one: <span\n    style=\"\\&quot;font-family:\" monospace;\\\"=\"\">time_from_prev</span></li>\n<li>Time taken to reach the next station from this one: <span\n    style=\"\\&quot;font-family:\" monospace;\\\"=\"\">time_to_next</span></li>\n</ol>\n<p>Based on these timings (next and previous) we can calculate the train\ntiming once we know three things:</p>\n<ol>\n<li>Where does it start from? (<strong>initial station</strong>)</li>\n<li>Where does it go to? (<strong>destination</strong>)</li>\n<li>When does it leave the initial station? (<strong>timing</strong>)</li>\n</ol>\n<h2 id=\"train\">Train</h2>\n<p>A train has five properties:</p>\n<ol>\n<li>train_id: a unique <span style=\"\\&quot;font-family:\"\n    monospace;\\\"=\"\">integer</span> used to identify the train, like the\n    <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">station_id</span></li>\n<li>train_no: each train has a number used to identify the train (given\n    by the Railways Department)</li>\n<li>initial station: <span style=\"\\&quot;font-family:\"\n    monospace;\\\"=\"\">station_start </span></li>\n<li>destination: <span style=\"\\&quot;font-family:\"\n    monospace;\\\"=\"\">station_end</span></li>\n<li>timing for leaving the initial station: <span\n    style=\"\\&quot;font-family:\" monospace;\\\"=\"\">timing</span></li>\n</ol>\n<p>The two stations, initial and destination refer to the <strong>Train\nStation</strong>, whereas the <em>timing</em> is a Time object.</p>\n<h2 id=\"storing-train-runs\">Storing train runs</h2>\n<p>Each train run consists of the train leaving the initial station at the\nspecified time towards its destination. We can calculate the scheduled\nstops at all stations based on the station's next and previous timing\nproperties. For eg. A train leaves Pune (station = 1) at exactly 12pm\n(noon) for Lonavala (station = 17). Then we can have:</p>\n<pre class=\"codehilite\"><code>station_list = [\n  stations where id &gt;= Pune.id \n  and id &lt;= Lonavala.id ]\n  ( order by id )\n  train = 12:00\n  time_to_next = 0\n  for station in station_list:\n    time = time + time_to_next\n    train stops at &lt;station&gt; at &lt;time&gt;\n    time_to_next = station.time_to_next</code></pre>\n\n\n<p>This generates the train's time runs from Pune to Lonavala. Using this\napproach to generate the timings <em>every time</em> a request comes in is not\nefficient. Instead, we can use this to calculate the timings for a train\nwhen it is added, and save it to database. So we have a separate table\nfor <strong>Timings</strong> that contains:\n * timings: Time Field\n * train: reference to Train\n * station: reference to Station</p>\n<p>This approach makes sense, since we can get the following frequently\nrequired things very easily:</p>\n<ul>\n<li>get timings for train A: <code>select timings where train = A</code></li>\n<li>get all trains for station B: <code>select timings where station = B</code></li>\n<li>get all trains for station B going towards C:\n    <code>select timings where station = B and train in [ trains going up (pune) or down (lonavala) ]</code></li>\n</ul>", "body": "### Train Stations\r\n\r\nThe train stations are setup as follows:\r\n\r\n1.  <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Pune</span>\r\n2.  <span style=\"\\&quot;font-family:\"\r\n    monospace;\\\"=\"\">Shivajinagar</span>\r\n3.  <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Khadki</span>\r\n4.  <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Dapodi</span>\r\n5.  <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Kasarwadi</span>\r\n6.  <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Pimpri</span>\r\n7.  <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Chinchwad</span>\r\n8.  <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Akurdi</span>\r\n9.  <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Dehuroad</span>\r\n10. <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Begdewadi</span>\r\n11. <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Ghorawadi</span>\r\n12. <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Talegaon</span>\r\n13. <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Vadgaon</span>\r\n14. <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Kanhe</span>\r\n15. <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Kamshet</span>\r\n16. <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Malavali</span>\r\n17. <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Lonavala</span>\r\n\r\nEach station has an id number (the same as the index in above list) that\r\nis used as its unique identifier. Storing and retrieving data in a\r\ndatabase is easier with *primary keys*, which are like the unique\r\nidentifiers for each record. If the user doesn't supply one, the\r\ndatabase creates one automatically. In this case, explicitly setting the\r\nID is beneficial as it allows nifty queries such as\r\n`select stations where id<=15;` would give all stations from Pune to\r\nKamshet.\r\n\r\nEach station has three properties:\r\n\r\n1.  <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">Name</span>\r\n2.  Time taken to reach this station from the previous one: <span\r\n    style=\"\\&quot;font-family:\" monospace;\\\"=\"\">time\\_from\\_prev</span>\r\n3.  Time taken to reach the next station from this one: <span\r\n    style=\"\\&quot;font-family:\" monospace;\\\"=\"\">time\\_to\\_next</span>\r\n\r\nBased on these timings (next and previous) we can calculate the train\r\ntiming once we know three things:\r\n\r\n1.  Where does it start from? (**initial station**)\r\n2.  Where does it go to? (**destination**)\r\n3.  When does it leave the initial station? (**timing**)\r\n\r\nTrain\r\n-----\r\n\r\nA train has five properties:\r\n\r\n1.  train\\_id: a unique <span style=\"\\&quot;font-family:\"\r\n    monospace;\\\"=\"\">integer</span> used to identify the train, like the\r\n    <span style=\"\\&quot;font-family:\" monospace;\\\"=\"\">station\\_id</span>\r\n2.  train\\_no: each train has a number used to identify the train (given\r\n    by the Railways Department)\r\n3.  initial station: <span style=\"\\&quot;font-family:\"\r\n    monospace;\\\"=\"\">station\\_start </span>\r\n4.  destination: <span style=\"\\&quot;font-family:\"\r\n    monospace;\\\"=\"\">station\\_end</span>\r\n5.  timing for leaving the initial station: <span\r\n    style=\"\\&quot;font-family:\" monospace;\\\"=\"\">timing</span>\r\n\r\nThe two stations, initial and destination refer to the **Train\r\nStation**, whereas the *timing* is a Time object.\r\n\r\n\r\nStoring train runs\r\n------------------\r\n\r\nEach train run consists of the train leaving the initial station at the\r\nspecified time towards its destination. We can calculate the scheduled\r\nstops at all stations based on the station's next and previous timing\r\nproperties. For eg. A train leaves Pune (station = 1) at exactly 12pm\r\n(noon) for Lonavala (station = 17). Then we can have:\r\n```\r\nstation_list = [\r\n  stations where id >= Pune.id \r\n  and id <= Lonavala.id ]\r\n  ( order by id )\r\n  train = 12:00\r\n  time_to_next = 0\r\n  for station in station_list:\r\n    time = time + time_to_next\r\n    train stops at <station> at <time>\r\n    time_to_next = station.time_to_next\r\n```\r\n\r\nThis generates the train's time runs from Pune to Lonavala. Using this\r\napproach to generate the timings *every time* a request comes in is not\r\nefficient. Instead, we can use this to calculate the timings for a train\r\nwhen it is added, and save it to database. So we have a separate table\r\nfor **Timings** that contains:\r\n * timings: Time Field\r\n * train: reference to Train\r\n * station: reference to Station\r\n\r\nThis approach makes sense, since we can get the following frequently\r\nrequired things very easily:\r\n\r\n-   get timings for train A: `select timings where train = A`\r\n-   get all trains for station B: `select timings where station = B`\r\n-   get all trains for station B going towards C:\r\n    `select timings where station = B and train in [ trains going up (pune) or down (lonavala) ]`", "headerimage": "", "highlight": "0", "section": 12}, {"id": 24, "title": "Implementation approaches", "authors": "1", "date_created": "2014-06-07 23:00:00", "date_published": "2014-06-08 14:34:50", "date_updated": "2017-11-12 15:53:33", "is_published": "1", "short_description": "Different approaches to implement a topic-based news aggregator", "tags": "203", "slug": "implementation-approaches", "body_type": "markdown", "body_text": "<p>In\u00a0<strong>computer science</strong>, an\u00a0<strong>Implementation</strong>\u00a0is a realization of\na\u00a0technical specification\u00a0or algorithm as a\u00a0program,\u00a0software component,\nor other\u00a0computer system\u00a0through\u00a0computer\nprogramming\u00a0anddeployment.<a href=\"\\%22http://en.wikipedia.org/wiki/Implementation#Computer_science\\%22\">Wikipedia</a></p>\n<p>The main aspect of the aggregator is its content curation. Which\nessentially means we have to sort all the feeds into topics/categories\nbased on what they are about. We have two fields of interest in an RSS\nField \u2013 one is\u00a0<strong>title</strong>\u00a0and the other is\u00a0<strong>description.\u00a0</strong>Using these,\nit is possible to analyze the feed and take a good guess what it is\nabout.</p>\n<p>Another approach we can take is to analyze the consecutive words in a\nfeed title, and compare them with other feed titles. Common words such\nas verbs, adjectives etc. will need to be filtered out. An example of\nthis could be the recent\u00a0<strong>Google I/O</strong>\u00a0where many feed titles contained\nthe word thereby allowing the algorithm to correctly categorize them.\nThis would be the work of a tokenizer and an iterative loop comparing\nthe result with every other title.</p>\n<p>Another way would to be create a\u00a0<em>Set\u00a0</em>of recognized categories as we go\nalong, and search the title for these words. If the title contains words\noccurring in existing categories, the the feed is categorized under it.\nIf not, then the feed words are used to identify a new category. The\nrank of a category can be formed as we move through the feed. After the\nfeeds have been categorized, a single loop to re-evaluate the category\nis necessary to weed out any errors in categorizing. If a feed has been\ncategorized as something else inspire of it containing words belonging\nto a better ranked category, this will correct it.</p>\n<p>A further scalable approach is to store the feeds in a database on the\nserver, and then to query the tables to identify the relationships. The\nwords extracted from the feeds would be stored in tables, and each row\nwould be joined with the corresponding rows containing those words. The\nresults of which would then be entered into a new category. Doing such\noperations at the server scale allows providing these curated feeds to\nall subscribing users. However, the preference and storage for each user\nwill still have to be on device. Only the performance intensive\noperations will be performed at the server.</p>", "body": "In\u00a0**computer science**, an\u00a0**Implementation**\u00a0is a realization of\r\na\u00a0technical specification\u00a0or algorithm as a\u00a0program,\u00a0software component,\r\nor other\u00a0computer system\u00a0through\u00a0computer\r\nprogramming\u00a0anddeployment.[Wikipedia](\\%22http://en.wikipedia.org/wiki/Implementation#Computer_science\\%22)\r\n\r\nThe main aspect of the aggregator is its content curation. Which\r\nessentially means we have to sort all the feeds into topics/categories\r\nbased on what they are about. We have two fields of interest in an RSS\r\nField \u2013 one is\u00a0**title**\u00a0and the other is\u00a0**description.\u00a0**Using these,\r\nit is possible to analyze the feed and take a good guess what it is\r\nabout.\r\n\r\nAnother approach we can take is to analyze the consecutive words in a\r\nfeed title, and compare them with other feed titles. Common words such\r\nas verbs, adjectives etc. will need to be filtered out. An example of\r\nthis could be the recent\u00a0**Google I/O**\u00a0where many feed titles contained\r\nthe word thereby allowing the algorithm to correctly categorize them.\r\nThis would be the work of a tokenizer and an iterative loop comparing\r\nthe result with every other title.\r\n\r\nAnother way would to be create a\u00a0*Set\u00a0*of recognized categories as we go\r\nalong, and search the title for these words. If the title contains words\r\noccurring in existing categories, the the feed is categorized under it.\r\nIf not, then the feed words are used to identify a new category. The\r\nrank of a category can be formed as we move through the feed. After the\r\nfeeds have been categorized, a single loop to re-evaluate the category\r\nis necessary to weed out any errors in categorizing. If a feed has been\r\ncategorized as something else inspire of it containing words belonging\r\nto a better ranked category, this will correct it.\r\n\r\nA further scalable approach is to store the feeds in a database on the\r\nserver, and then to query the tables to identify the relationships. The\r\nwords extracted from the feeds would be stored in tables, and each row\r\nwould be joined with the corresponding rows containing those words. The\r\nresults of which would then be entered into a new category. Doing such\r\noperations at the server scale allows providing these curated feeds to\r\nall subscribing users. However, the preference and storage for each user\r\nwill still have to be on device. Only the performance intensive\r\noperations will be performed at the server.", "headerimage": "", "highlight": "0", "section": 11}, {"id": 23, "title": "Repetition in News Aggregator", "authors": "1", "date_created": "2014-06-06 23:00:00", "date_published": "2014-06-07 14:30:49", "date_updated": "2017-11-12 15:44:18", "is_published": "1", "short_description": "service that organises news from different sources by topic", "tags": "203", "slug": "repetition-in-news-aggregator", "body_type": "markdown", "body_text": "<p><strong>Content curation</strong>\u00a0can be carried out either manually or\nautomatically. In the first case, it\u2019s done by specially designated\ncurators. In the second case, it\u2019s done using one or more of the\nfollowing:\u00a0<strong>Collaborative filtering, Semantic analysis, and Social\nrating.</strong>[Wikipedia](http://en.wikipedia.org/wiki/Content_curation)</p>\n<p>What does a user want to do when he opens a news aggregator app? He\nwants to read the news. That\u2019s exactly the approach that has been taken\nby every news aggregator app until now. The user wants to read the news\nfrom various sources, and he may be searching for a particular news\nsource, so the app will contain filters, folders, tags etc. to let\nthe\u00a0<em>user</em>\u00a0do the hard work. Apps like Flipboard rely on\u00a0<em>users</em>\u00a0to\ncurate the articles into Magazines, which mostly will not satisfy other\nusers looking for their own version of news reading. In such a case, a\nmachine algorithm that curates the news based on the topic at hand is\nideal for the user, as it allows the freedom to then specify which of\nthe current popular topics the user is interested in.</p>\n<p>Let\u2019s take the example of Amazon\u2019s Fire Phone launch a few days back\n(Jun 18th). Virtually every news source was filling in post after post\nabout the many wonders brought on by Jeff Bezos. Translate this to what\nthe user saw : A list full of Fire phone articles repeated again and\nagain. Compare with what the user\u00a0<em>wanted</em>\u00a0: To simply know about the\nFire phone. There are two things to\u00a0<em>learn\u00a0</em>and\u00a0<em>understand\u00a0</em>here \u2013</p>\n<ul>\n<li>The user wanted to know more about the Fire phone without reading\n    about it again and again</li>\n<li>The repeated Fire phone articles overshadowed other news articles</li>\n</ul>\n<p>Which brings us to our solution \u2013 which for now, simply addresses the\ntwo things we\u2019ve just learnt \u2013\u00a0<em>repetition\u00a0</em>and\u00a0<em>preference</em>. Repeated\narticles can simply be clubbed together under a label called\u00a0<em>Amazon\nlaunches Fire Phone\u00a0</em>and since there were more articles about it than\nany other topic, it floats right to the top. Other similar articles are\nclubbed together and shown based on how frequent the different sources\nwrote about it.</p>\n<p>To tell the user how many articles are currently available about the\nAmazon Fire Phone, a small number could be shown in the corner of the\nlabel. Selecting the label would open the feeds page for the articles\nrelated to the topic. The user can then browse the topics by time, or\nsource. Once the user has finished reading, marking the entire label as\nread allows the user to clean up the feed and to focus on other topics.\nThis allows the reading of more news, and makes a better use of time.</p>\n<p>Other existing (if any) features can still be built on top of this\nstructure as the feeds are merely categorized under a label, but are\nstill available. The UI would not be drastically any different from\nother apps that have folders, filters, streams etc. already in use. This\nmeans that existing apps can easily adapt this approach as an added\nfunctionality without much UI change.</p>", "body": "**Content curation**\u00a0can be carried out either manually or\r\nautomatically. In the first case, it\u2019s done by specially designated\r\ncurators. In the second case, it\u2019s done using one or more of the\r\nfollowing:\u00a0**Collaborative filtering, Semantic analysis, and Social\r\nrating.**[Wikipedia](http://en.wikipedia.org/wiki/Content_curation\\)\r\n\r\nWhat does a user want to do when he opens a news aggregator app? He\r\nwants to read the news. That\u2019s exactly the approach that has been taken\r\nby every news aggregator app until now. The user wants to read the news\r\nfrom various sources, and he may be searching for a particular news\r\nsource, so the app will contain filters, folders, tags etc. to let\r\nthe\u00a0*user*\u00a0do the hard work. Apps like Flipboard rely on\u00a0*users*\u00a0to\r\ncurate the articles into Magazines, which mostly will not satisfy other\r\nusers looking for their own version of news reading. In such a case, a\r\nmachine algorithm that curates the news based on the topic at hand is\r\nideal for the user, as it allows the freedom to then specify which of\r\nthe current popular topics the user is interested in.\r\n\r\nLet\u2019s take the example of Amazon\u2019s Fire Phone launch a few days back\r\n(Jun 18th). Virtually every news source was filling in post after post\r\nabout the many wonders brought on by Jeff Bezos. Translate this to what\r\nthe user saw : A list full of Fire phone articles repeated again and\r\nagain. Compare with what the user\u00a0*wanted*\u00a0: To simply know about the\r\nFire phone. There are two things to\u00a0*learn\u00a0*and\u00a0*understand\u00a0*here \u2013\r\n\r\n-   The user wanted to know more about the Fire phone without reading\r\n    about it again and again\r\n-   The repeated Fire phone articles overshadowed other news articles\r\n\r\nWhich brings us to our solution \u2013 which for now, simply addresses the\r\ntwo things we\u2019ve just learnt \u2013\u00a0*repetition\u00a0*and\u00a0*preference*. Repeated\r\narticles can simply be clubbed together under a label called\u00a0*Amazon\r\nlaunches Fire Phone\u00a0*and since there were more articles about it than\r\nany other topic, it floats right to the top. Other similar articles are\r\nclubbed together and shown based on how frequent the different sources\r\nwrote about it.\r\n\r\nTo tell the user how many articles are currently available about the\r\nAmazon Fire Phone, a small number could be shown in the corner of the\r\nlabel. Selecting the label would open the feeds page for the articles\r\nrelated to the topic. The user can then browse the topics by time, or\r\nsource. Once the user has finished reading, marking the entire label as\r\nread allows the user to clean up the feed and to focus on other topics.\r\nThis allows the reading of more news, and makes a better use of time.\r\n\r\nOther existing (if any) features can still be built on top of this\r\nstructure as the feeds are merely categorized under a label, but are\r\nstill available. The UI would not be drastically any different from\r\nother apps that have folders, filters, streams etc. already in use. This\r\nmeans that existing apps can easily adapt this approach as an added\r\nfunctionality without much UI change.", "headerimage": "", "highlight": "0", "section": 11}, {"id": 18, "title": "RDF/OWL content-negotiation using NGINX", "authors": "1", "date_created": "2017-08-19 17:59:53", "date_published": "2017-08-19 17:59:56", "date_updated": "2017-09-29 15:26:39", "is_published": "1", "short_description": "Performing content-negotiation for RDF and OWL types with Nginx", "tags": "133,173,176,174", "slug": "rdfowl-content-negotiation-using-nginx", "body_type": "markdown", "body_text": "<blockquote>\n<p>A similar particulars for Apache are available from \n<a href=\"https://www.w3.org/TR/swbp-vocab-pub/\">W3C Best practices for publishing RDF Vocabularies</a></p>\n</blockquote>\n<h2 id=\"what-is-content-negotiation\">What is <em>content-negotiation</em>?</h2>\n<p><a href=\"https://en.wikipedia.org/wiki/Content_negotiation\">Wikipedia</a> defines <em>content-negotiation</em> as different (mime types) documents are served at the same URI.\nIn simpler terms, this allows requesting different <em>formats</em> on the same URI.\nHence the <em>negotiation</em> of <em>content</em>, as in, the requester asks for a specific\ncontent and the server sends it, or replies back with what is available.\nThis allows a single URI for the resource, where the requester can get the data in whatever format they want, provided the server already has that format.\nSo a request for an image at http://example.com/image will <em>image.jpeg</em> if\nthe requester wants a JPEG, or it may send <em>image.png</em> as default.</p>\n<h2 id=\"content-negotiation-for-rdf-and-owl\">Content negotiation for RDF and OWL</h2>\n<p>For serving ontologies and datasets, content negotiation is a valuable mechanism because of the varying formats, while ontologies need to be served with a single, unchanging url.\nTherefore, say, when an ontology or dataset is served at http://example.com/ontology, the requester can ask for <code>RDF/XML</code> or <code>Turtle</code> or <code>N3</code>, and the server will respond correctly if the file exists on the server.\nThis also puts the onus on the maintainer of the ontology to keep <em>a lot of versions</em> of the ontology to fulfil content negotiation.</p>\n<h2 id=\"setting-up-content-negotiation-on-nginx\">Setting up content negotiation on Nginx</h2>\n<p>Nginx is a simple, versatile web server, which is great for quick handling of web apps, but the content-negotiation aspect is not as fleshed out as with Apache. Therefore, the following will only enable content-negotiation <em>partially</em> and has several <em>holes</em> in it. But it gets the job done.</p>\n<p>First, set the following snippet in the <code>http</code> section of <code>/etc/nginx/nginx.conf</code> to map the request type to file types.</p>\n<pre class=\"codehilite\"><code>map $http_accept $ld_suffix{\n    &quot;~*owl&quot; &quot;.owl&quot;;\n    &quot;~*rdf&quot; &quot;.rdf&quot;;\n    &quot;~*xml&quot; &quot;.xml&quot;;\n} </code></pre>\n\n\n<p>Then, to make Nginx aware of mime types we want to serve, add the following in <code>/etc/nginx/mime.types</code> to the existing <code>types</code> dictionary.</p>\n<pre class=\"codehilite\"><code>    text/turtle                           ttl;\n    application/rdf+xml                   rdf;\n    application/n-triples                 nt;\n    application/ld+json                   jsonld;\n    application/owl+xml                   owl;\n    text/trig                             trig;\n    application/n-quads                   nq;</code></pre>\n\n\n<p>Once this is done, open the webapp configuration (usually in <code>sites-*</code>) and set it up so-</p>\n<pre class=\"codehilite\"><code>location /ontologies {\n    alias /apps/ontologies;\n    autoindex on;\n    try_files $uri $uri$ld_suffix =404;\n}</code></pre>\n\n\n<p>which will serve the files out of <code>/apps/ontologies/</code> by basically trying out different combinations of files, and if not available, will sent a <code>HTTP-404</code>.\nAn easy way to test this is using <code>curl</code> as -</p>\n<pre class=\"codehilite\"><code>curl -I -L -H &quot;Accept: &lt;MIME-TYPE&gt;&quot;  URI\n# response should contain\n# Content-Type: &lt;MIME-TYPE&gt;</code></pre>", "body": "> A similar particulars for Apache are available from \r\n> [W3C Best practices for publishing RDF Vocabularies](https://www.w3.org/TR/swbp-vocab-pub/)\r\n\r\n## What is *content-negotiation*?\r\n\r\n[Wikipedia](https://en.wikipedia.org/wiki/Content_negotiation) defines *content-negotiation* as different (mime types) documents are served at the same URI.\r\nIn simpler terms, this allows requesting different *formats* on the same URI.\r\nHence the *negotiation* of *content*, as in, the requester asks for a specific\r\ncontent and the server sends it, or replies back with what is available.\r\nThis allows a single URI for the resource, where the requester can get the data in whatever format they want, provided the server already has that format.\r\nSo a request for an image at http://example.com/image will *image.jpeg* if\r\nthe requester wants a JPEG, or it may send *image.png* as default.\r\n\r\n## Content negotiation for RDF and OWL\r\nFor serving ontologies and datasets, content negotiation is a valuable mechanism because of the varying formats, while ontologies need to be served with a single, unchanging url.\r\nTherefore, say, when an ontology or dataset is served at http://example.com/ontology, the requester can ask for `RDF/XML` or `Turtle` or `N3`, and the server will respond correctly if the file exists on the server.\r\nThis also puts the onus on the maintainer of the ontology to keep *a lot of versions* of the ontology to fulfil content negotiation.\r\n\r\n## Setting up content negotiation on Nginx\r\n\r\nNginx is a simple, versatile web server, which is great for quick handling of web apps, but the content-negotiation aspect is not as fleshed out as with Apache. Therefore, the following will only enable content-negotiation *partially* and has several *holes* in it. But it gets the job done.\r\n\r\nFirst, set the following snippet in the `http` section of `/etc/nginx/nginx.conf` to map the request type to file types.\r\n```\r\nmap $http_accept $ld_suffix{\r\n\t\"~*owl\" \".owl\";\r\n\t\"~*rdf\" \".rdf\";\r\n\t\"~*xml\" \".xml\";\r\n} \r\n```\r\n\r\nThen, to make Nginx aware of mime types we want to serve, add the following in `/etc/nginx/mime.types` to the existing `types` dictionary.\r\n```\r\n    text/turtle                           ttl;\r\n    application/rdf+xml                   rdf;\r\n    application/n-triples                 nt;\r\n    application/ld+json                   jsonld;\r\n    application/owl+xml                   owl;\r\n    text/trig                             trig;\r\n    application/n-quads                   nq;\r\n```\r\n\r\nOnce this is done, open the webapp configuration (usually in `sites-*`) and set it up so-\r\n```\r\nlocation /ontologies {\r\n\talias /apps/ontologies;\r\n\tautoindex on;\r\n\ttry_files $uri $uri$ld_suffix =404;\r\n}\r\n```\r\nwhich will serve the files out of `/apps/ontologies/` by basically trying out different combinations of files, and if not available, will sent a `HTTP-404`.\r\nAn easy way to test this is using `curl` as -\r\n\r\n```\r\ncurl -I -L -H \"Accept: <MIME-TYPE>\"  URI\r\n# response should contain\r\n# Content-Type: <MIME-TYPE>\r\n```", "headerimage": "", "highlight": "0", "section": 9}, {"id": 17, "title": "Tools for Semantic Web", "authors": "1", "date_created": "2017-08-18 19:20:01", "date_published": "2017-08-18 19:20:03", "date_updated": "2017-09-02 19:13:11", "is_published": "1", "short_description": "A (WIP) list of tools and utilities for working with Semantic Web", "tags": "180,184,179,178,173,176,181,85,174,182,183,172,177", "slug": "tools-for-semantic-web", "body_type": "markdown", "body_text": "<h2 id=\"validators\">Validators</h2>\n<h3 id=\"rdf\">RDF</h3>\n<ul>\n<li><a href=\"https://www.w3.org/RDF/Validator/\">W3 RDF Validator</a> - requires RDF to be in XML form</li>\n<li><a href=\"https://www.w3.org/2012/pyRdfa/Validator.html\">W3 RDFa Validator</a></li>\n<li><a href=\"http://rdf.greggkellogg.net/distiller\">RDF Distiller</a> - a ruby gem that validates RDF from URI. Also available as a <code>GET</code> interface.</li>\n<li><a href=\"http://ttl.summerofcode.be/\">TTL Validator</a> - A turtle RDF validator project</li>\n<li><a href=\"http://iot.ee.surrey.ac.uk/SSNValidation/\">SSN Ontology Validation Service</a> - allows uploading file or loading from URI. Also shows a tag cloud based on the graph.</li>\n<li><a href=\"https://www.w3.org/2015/03/ShExValidata/\">W3 ShEx Validata</a> - validates RDF expressed against <a href=\"http://www.w3.org/2013/ShEx/Primer\">ShEx (Shape Expressions)</a>\nconstraints</li>\n</ul>\n<h3 id=\"owl\">OWL</h3>\n<ul>\n<li><a href=\"http://mowl-power.cs.man.ac.uk:8080/validator/\">Machester OWL Validator</a></li>\n</ul>\n<h2 id=\"converters\">Converters</h2>\n<ul>\n<li><a href=\"https://www.w3.org/wiki/ConverterToRdf\">W3 - Converters to RDF</a> - a (comprehensive) list of tools that convert <em>to</em> RDF.</li>\n<li><a href=\"http://www.easyrdf.org/converter\">EasyRDF</a> - allows conversion from and to a lot of formats. Can load from URI or text entered on page.</li>\n<li><a href=\"https://rdf-translator.appspot.com/\">RDF Translator</a></li>\n<li><a href=\"http://any23.org/\">Apache Any23</a> - convert anything to triples.</li>\n<li><a href=\"http://www.ebusiness-unibw.org/tools/rdf2rdfa/\">RDF2RDFa</a> - converts rdf/xml and turtle/n3 to RDFa snippets.</li>\n</ul>\n<h2 id=\"triple-stores\">Triple Stores</h2>\n<p><a href=\"https://en.wikipedia.org/wiki/List_of_subject-predicate-object_databases\">Wikipedia page</a>\nlisting and comparing different triple-stores.</p>\n<ul>\n<li><a href=\"https://jena.apache.org/index.html\">Jena Fuseki + TDB</a> - The <em>easiest</em> option to get it up and running if the work is local, doesn't need to <em>scale</em> and is mostly <em>development</em> oriented. The page also has some good tutorials on SPARQL and use of triple store.</li>\n<li><a href=\"https://virtuoso.openlinksw.com/\">Openlink Virtuoso</a> - A fast and responsive triple-store that comes ready-for-production out of the box. Setup is easy on servers, though documentation is sparse. Is the triple-store used for serving DBPedia's SPARQL queries.</li>\n<li><a href=\"https://franz.com/agraph/allegrograph/\">AllegroGraph</a></li>\n<li><a href=\"http://www.marklogic.com/\">MarkLogic</a></li>\n<li><a href=\"https://ontotext.com/products/graphdb/\">ontotext GraphDB</a></li>\n<li><a href=\"http://parliament.semwebcentral.org/\">Parliament</a></li>\n<li><a href=\"https://github.com/RDFLib/rdflib\">RDFLib</a> - python library with (separate) backends for persistence of graphs</li>\n<li><a href=\"http://librdf.org/\">Redland librdf</a> - a series of libraries that also contain RDF storage module</li>\n<li><a href=\"http://www.stardog.com/\">Stardog</a></li>\n</ul>\n<h2 id=\"tools-utils-and-libraries\">Tools, Utils, and Libraries</h2>\n<p>There are times when RDF and their ontological ilk may need to be in code,\nfor various purposes, and need interoperability with a programming language.</p>\n<h3 id=\"protege\"><a href=\"http://protege.stanford.edu/\">Protege</a></h3>\n<p>This is the tool <em>everyone and their supervisors</em> use to model OWL ontologies. It is quite adept at handling, though the UI/UX may feel clunkk, the features are <em>large</em> and will be quite ample to get by with. The documentation and introduction videos/pages help with getting it up and running, but don't do a good job of explaining how to progress from there. But it's hand down the best tool for the job. There's even a (light) web version.</p>\n<h3 id=\"exposing-triples-over-the-web\">Exposing triples over the web</h3>\n<ul>\n<li><a href=\"http://wifo5-03.informatik.uni-mannheim.de/pubby/\">Pubby</a> - a simple tool that exposes data using SPARQL <code>DESCRIBE</code> queries. Very simple to setup. Good way to expose datasets, but limited in scope as only one graph as such can be exposed with one instance.</li>\n</ul>\n<h3 id=\"python\">Python</h3>\n<ul>\n<li><a href=\"https://github.com/RDFLib/rdflib\">rdflib</a> is a python library for working with RDF graphs and offers a <em>pythonic</em> way to do things. It supports a large array of formats, though \n<a href=\"https://github.com/RDFLib/rdflib-jsonld\">serialising to json-ld requires another library</a>.\nPersistence is possible through various backends (and additional ones available as plugins).\nSPARQL queries are also possible with the library, and small utilities like\nnavigating the graph, namespaces, or datatypes are handles well.</li>\n<li><a href=\"http://librdf.org/\">Redland librdf</a> - has a few RDF libraries, though haven't encountered anyone using them</li>\n<li><a href=\"https://github.com/cosminbasca/surfrdf\">SuRF</a> - An ORM for expressing RDF nodes using Python classes</li>\n<li><a href=\"https://github.com/josd/eye\">EYE - Euler Yet another proof Engine</a> - performs\nsemi-backward reasnoning and is interoperable with Cwm</li>\n<li><a href=\"https://www.w3.org/2000/10/swap/doc/cwm\">Cwm</a> - forward-chaining reasoner for querying, checking, transforming, and filtering</li>\n<li><a href=\"https://github.com/rob-metalinkage/django-rdf-io\">django-rdf-io</a> - syncing Django models with a triple-store</li>\n</ul>\n<h3 id=\"java\">Java</h3>\n<ul>\n<li><a href=\"https://jena.apache.org/\">Apache Jena</a> - I <em>presume</em> that this is the most <em>popular</em> library for dealing with RDF\nsince most academics tend to use this (or Protege) for most of their needs.<ul>\n<li><em>ARQ</em> - a SPARQL engine</li>\n<li><em>Fuseki</em> - a SPARQL endpoint</li>\n<li><em>TDB</em> - persistence through triple-store</li>\n<li>Models for OWL</li>\n<li>Inference through reasoners</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"dokieli\"><a href=\"https://dokie.li/\">dokie.li</a></h3>\n<p>A tool to help create 'RDFa pages', often required in (good) publications,\nwhich uses <a href=\"https://www.w3.org/TR/annotation-model/\">Web Annotations Data Model</a>\nto provide comments, reviews, and feedbacks over articles in self-hosted format.</p>\n<h2 id=\"documentation\">Documentation</h2>\n<h3 id=\"lode\">LODE</h3>\n<p><a href=\"http://www.essepuntato.it/lode\">LODE</a> is a service that creates a HTML documentation page for an OWL ontology. It has reasoners, import+closure, and the ability to select a particular language from the ontology.</p>\n<h3 id=\"widoco\">Widoco</h3>\n<p><a href=\"https://dgarijo.github.io/Widoco/\">Widoco</a> is a Java tool, available as a JAR file, that uses LODE internally to generate the documentation of the ontology. It provides several additional features such as embedding a WebVOWL representation in the page, providing ToC and Introduction/Overview sections, etc.</p>", "body": "## Validators\r\n\r\n### RDF\r\n\r\n* [W3 RDF Validator](https://www.w3.org/RDF/Validator/) - requires RDF to be in XML form\r\n* [W3 RDFa Validator](https://www.w3.org/2012/pyRdfa/Validator.html)\r\n* [RDF Distiller](http://rdf.greggkellogg.net/distiller) - a ruby gem that validates RDF from URI. Also available as a `GET` interface.\r\n* [TTL Validator](http://ttl.summerofcode.be/) - A turtle RDF validator project\r\n* [SSN Ontology Validation Service](http://iot.ee.surrey.ac.uk/SSNValidation/) - allows uploading file or loading from URI. Also shows a tag cloud based on the graph.\r\n* [W3 ShEx Validata](https://www.w3.org/2015/03/ShExValidata/) - validates RDF expressed against [ShEx (Shape Expressions)](http://www.w3.org/2013/ShEx/Primer)\r\nconstraints\r\n\r\n### OWL\r\n\r\n* [Machester OWL Validator](http://mowl-power.cs.man.ac.uk:8080/validator/)\r\n\r\n## Converters\r\n\r\n* [W3 - Converters to RDF](https://www.w3.org/wiki/ConverterToRdf) - a (comprehensive) list of tools that convert _to_ RDF.\r\n* [EasyRDF](http://www.easyrdf.org/converter) - allows conversion from and to a lot of formats. Can load from URI or text entered on page.\r\n* [RDF Translator](https://rdf-translator.appspot.com/)\r\n* [Apache Any23](http://any23.org/) - convert anything to triples.\r\n* [RDF2RDFa](http://www.ebusiness-unibw.org/tools/rdf2rdfa/) - converts rdf/xml and turtle/n3 to RDFa snippets.\r\n\r\n## Triple Stores\r\n[Wikipedia page](https://en.wikipedia.org/wiki/List_of_subject-predicate-object_databases)\r\nlisting and comparing different triple-stores.\r\n\r\n* [Jena Fuseki + TDB](https://jena.apache.org/index.html) - The _easiest_ option to get it up and running if the work is local, doesn't need to _scale_ and is mostly _development_ oriented. The page also has some good tutorials on SPARQL and use of triple store.\r\n* [Openlink Virtuoso](https://virtuoso.openlinksw.com/) - A fast and responsive triple-store that comes ready-for-production out of the box. Setup is easy on servers, though documentation is sparse. Is the triple-store used for serving DBPedia's SPARQL queries.\r\n* [AllegroGraph](https://franz.com/agraph/allegrograph/)\r\n* [MarkLogic](http://www.marklogic.com/)\r\n* [ontotext GraphDB](https://ontotext.com/products/graphdb/)\r\n* [Parliament](http://parliament.semwebcentral.org/)\r\n* [RDFLib](https://github.com/RDFLib/rdflib) - python library with (separate) backends for persistence of graphs\r\n* [Redland librdf](http://librdf.org/) - a series of libraries that also contain RDF storage module\r\n* [Stardog](http://www.stardog.com/)\r\n\r\n## Tools, Utils, and Libraries\r\nThere are times when RDF and their ontological ilk may need to be in code,\r\nfor various purposes, and need interoperability with a programming language.\r\n\r\n### [Protege](http://protege.stanford.edu/)\r\nThis is the tool *everyone and their supervisors* use to model OWL ontologies. It is quite adept at handling, though the UI/UX may feel clunkk, the features are _large_ and will be quite ample to get by with. The documentation and introduction videos/pages help with getting it up and running, but don't do a good job of explaining how to progress from there. But it's hand down the best tool for the job. There's even a (light) web version.\r\n\r\n### Exposing triples over the web\r\n* [Pubby](http://wifo5-03.informatik.uni-mannheim.de/pubby/) - a simple tool that exposes data using SPARQL `DESCRIBE` queries. Very simple to setup. Good way to expose datasets, but limited in scope as only one graph as such can be exposed with one instance.\r\n\r\n### Python\r\n* [rdflib](https://github.com/RDFLib/rdflib) is a python library for working with RDF graphs and offers a _pythonic_ way to do things. It supports a large array of formats, though \r\n[serialising to json-ld requires another library](https://github.com/RDFLib/rdflib-jsonld).\r\nPersistence is possible through various backends (and additional ones available as plugins).\r\nSPARQL queries are also possible with the library, and small utilities like\r\nnavigating the graph, namespaces, or datatypes are handles well.\r\n* [Redland librdf](http://librdf.org/) - has a few RDF libraries, though haven't encountered anyone using them\r\n* [SuRF](https://github.com/cosminbasca/surfrdf) - An ORM for expressing RDF nodes using Python classes\r\n* [EYE - Euler Yet another proof Engine](https://github.com/josd/eye) - performs\r\nsemi-backward reasnoning and is interoperable with Cwm\r\n* [Cwm](https://www.w3.org/2000/10/swap/doc/cwm) - forward-chaining reasoner for querying, checking, transforming, and filtering\r\n* [django-rdf-io](https://github.com/rob-metalinkage/django-rdf-io) - syncing Django models with a triple-store\r\n\r\n### Java\r\n* [Apache Jena](https://jena.apache.org/) - I _presume_ that this is the most _popular_ library for dealing with RDF\r\nsince most academics tend to use this (or Protege) for most of their needs.\r\n    * *ARQ* - a SPARQL engine\r\n    * *Fuseki* - a SPARQL endpoint\r\n    * *TDB* - persistence through triple-store\r\n    * Models for OWL\r\n    * Inference through reasoners\r\n    \r\n### [dokie.li](https://dokie.li/)\r\nA tool to help create 'RDFa pages', often required in (good) publications,\r\nwhich uses [Web Annotations Data Model](https://www.w3.org/TR/annotation-model/)\r\nto provide comments, reviews, and feedbacks over articles in self-hosted format.\r\n\r\n## Documentation\r\n\r\n### LODE\r\n[LODE](http://www.essepuntato.it/lode) is a service that creates a HTML documentation page for an OWL ontology. It has reasoners, import+closure, and the ability to select a particular language from the ontology.\r\n\r\n### Widoco\r\n[Widoco](https://dgarijo.github.io/Widoco/) is a Java tool, available as a JAR file, that uses LODE internally to generate the documentation of the ontology. It provides several additional features such as embedding a WebVOWL representation in the page, providing ToC and Introduction/Overview sections, etc.", "headerimage": "", "highlight": "0", "section": 10}, {"id": 22, "title": "Documenting ontologies using Widoco", "authors": "1", "date_created": "2017-09-02 17:32:13", "date_published": "2017-09-02 17:32:14", "date_updated": "2017-09-02 17:43:28", "is_published": "1", "short_description": "Widoco is a nifty utility to document OWL2 vocabularies", "tags": "190,173,176,177", "slug": "documenting-ontologies-using-widoco", "body_type": "markdown", "body_text": "<p><a href=\"https://dgarijo.github.io/Widoco/\">Widoco</a> is a Java tool that generates HTML documentation for OWL2 vocabularies. It uses <a href=\"http://www.essepuntato.it/lode\">LODE</a> as its backend for generating the documentation and provides several additional features on top of it such as an embedding of the WebVOWL visualization as well as sections pertaining to description of the ontology such as Introduction, Overview, and Description.</p>\n<h2 id=\"installation\">Installation</h2>\n<p>Widoco is available as a JAR file, which contains all the dependencies, and requires Java 1.8. Running the JAR file is simple as -</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">java -jar widoco.jar</code></pre>\n\n\n<p>though there are several configuration options on the documentation page. Widoco can run an interactive wizard for selecting the various ontology properties, or be run as a shell utility.</p>\n<h2 id=\"annotations\">Annotations</h2>\n<p>Widoco uses annotations found in the OWL2 ontology based on a series of <a href=\"https://dgarijo.github.io/Widoco/doc/bestPractices/index-en.html\">guidelines</a> published on its site. Once the ontology has been annotated with these properties, Widoco reflects them in the generated documentation. These are -</p>\n<pre class=\"codehilite\"><code>dct:title\ndct:creator\ndct:created\ndct:modified\nvann:preferredNamespaceURI\nvann:preferredNamespacePrefix\nowl:versionInfo\nowl:versionIRI\ndct:description\nrdfs:comment\ndct:license\ndct:abstract</code></pre>\n\n\n<p>A description of these terms and how they are used within the ontology, and by Widoco, is available on the Widoco site (see above guidelines link).</p>\n<h2 id=\"configurations\">Configurations</h2>\n<p>Widoco can be run via the command line using the configuration options, or as a wizard. The options can be saved in a config file with the default path of <code>config/config.properties</code>.\nSee an example <a href=\"https://github.com/dgarijo/Widoco/blob/master/src/main/resources/config/config.properties\">config file</a> from the Widoco test folder.</p>\n<h3 id=\"first-run\">First run</h3>\n<p>When running Widoco for the first time, the following configuration options should be set (not necessary, but as an example) -</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">java -jar widoco.jar \\\n    -ontFile $PATH_TO_ONTOLOGY_FILE\n    # OR \n    -ontURI $URL_TO_ONTOLOGY\n    -outFolder $PATH_TO_DOCUMENTATION\n    -confFile $PATH_CONFIG\n    -includeImportedOntologies\n    -webVowl\n    -licensius\n    -rewriteAll</code></pre>\n\n\n<p>Also consider adding the <code>-oops</code> flag to get an evaluation of the ontology using the <a href=\"http://oops.linkeddata.es/index.jsp\">OOPS</a> service that checks for common errors and malpractices in ontologies.</p>\n<p>In the first run, Widoco will generate the documentation in the specified output folder. This documentation contains distinct HTML files with the following folder structure -</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">.\n\u251c\u2500\u2500 docs # -- THIS WAS THE OUTPUT DOCUMENTATION FOLDER --\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 406.html  # for HTTP406 errors\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index-en.html  # ontology documentation index page\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ontology.nt  # NT representation of the ontology\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ontology.ttl  # Turtle representation of the ontology\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ontology.xml  # RDF/XML representation of the ontology\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provenance  # provenance of the ontology\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 readme.md  # auto-generated Widoco file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 resources\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sections  # These are the various sections in the documentation page\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 abstract-en.html\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 crossref-en.html\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 description-en.html\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 introduction-en.html\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 overview-en.html\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 references-en.html\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 webvowl\n7 directories, 30 files</code></pre>\n\n\n<p>You can see what the default documentation page looks like by opening the <code>index-en.html</code> document in a browser. Be aware of the browser issues and limitations as discussed on the Widoco page. As of the date this page was written, there is an issue with Chrome regarding opening files without a special flag that allows such files to be directly loaded by the page itself.</p>\n<p>The <em>sections</em> folder contains placeholder documents for the various sections. Now you can go ahead and edit the <em>sections</em> such as Abstract, Introduction, etc. (which are HTML, btw), and Widoco will persist these sections (which means not overwrite them) the next time the ontology is updated (and Widoco is run over it again).</p>\n<h2 id=\"subsequent-runs\">Subsequent runs</h2>\n<p>On subsequent runs, Widoco (the smart tool that it is), can be configured to update only the changes in the ontology, preserving the underlying documentations (in essence, the <em>sections</em> folder). This is done via the <code>-crossRef</code> option, used in place of the <code>-rewriteAll</code> option which replaces all sections with placeholders.</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">java -jar widoco.jar \\\n    -ontFile $PATH_TO_ONTOLOGY_FILE\n    # OR \n    -ontURI $URL_TO_ONTOLOGY\n    -outFolder $PATH_TO_DOCUMENTATION\n    -confFile $PATH_CONFIG\n    -includeImportedOntologies\n    -webVowl\n    -licensius\n    -crossRef</code></pre>\n\n\n<p>As the documentation says,</p>\n<blockquote>\n<p>The <code>-crossRef</code> option will ONLY generate the overview and cross reference sections. The index document will NOT be generated. The htaccess, provenance page, etc., will not be generated unless requested by other flags. This flag in intended to be used only after a first version of the documentation exists.</p>\n</blockquote>\n<p>Along with updating the ontology documentation, Widoco can also be set to use the provenance trace to automatically link subsequent versions to the previous ones via the config file.</p>\n<h2 id=\"my-config-file\">My config file</h2>\n<p>My config file, shared here for example , goes something like -</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">abstract=The abstract for my ontology\nontologyTitle=The title for my ontology\nontologyPrefix=myonto\nontologyNamespaceURI=http://example.com\nontologyName=The GDPR Provenance ontology\nthisVersionURI=http://example.com/version1.1\nlatestVersionURI=http://example.com\npreviousVersionURI=http://example.com/version1.0\ndateOfRelease=2017-08-01\nontologyRevisionNumber=1.1\nlicenseURI=http://purl.org/NET/rdflicense/cc-by4.0\nlicenseName=Creative Commons CC-BY\nlicenseIconURL=null\nciteAs=\nDOI=\nstatus=Ontology Specification Draft\nbackwardsCompatibleWith=\npublisher=\npublisherURI=\npublisherInstitution=\npublisherInstitutionURI=\nauthors=Harshvardhan J. Pandit\nauthorsURI=http://purl.org/adaptcentre/people/harshvardhan_pandit\nauthorsInstitution=ADAPT Centre, Trinity College Dublin\nauthorsInstitutionURI=https://adaptcentre.ie\ncontributors=\ncontributorsURI=\ncontributorsInstitution=\ncontributorsInstitutionURI=\nimportedOntologyNames=\nimportedOntologyURIs=\nextendedOntologyNames=\nextendedOntologyURIs=\nRDFXMLSerialization=ontology.xml\nTurtleSerialization=ontology.ttl\nN3Serialization=ontology.nt</code></pre>", "body": "[Widoco](https://dgarijo.github.io/Widoco/) is a Java tool that generates HTML documentation for OWL2 vocabularies. It uses [LODE](http://www.essepuntato.it/lode) as its backend for generating the documentation and provides several additional features on top of it such as an embedding of the WebVOWL visualization as well as sections pertaining to description of the ontology such as Introduction, Overview, and Description.\r\n\r\n## Installation\r\nWidoco is available as a JAR file, which contains all the dependencies, and requires Java 1.8. Running the JAR file is simple as -\r\n```bash\r\njava -jar widoco.jar\r\n```\r\nthough there are several configuration options on the documentation page. Widoco can run an interactive wizard for selecting the various ontology properties, or be run as a shell utility.\r\n\r\n## Annotations\r\nWidoco uses annotations found in the OWL2 ontology based on a series of [guidelines](https://dgarijo.github.io/Widoco/doc/bestPractices/index-en.html) published on its site. Once the ontology has been annotated with these properties, Widoco reflects them in the generated documentation. These are -\r\n```\r\ndct:title\r\ndct:creator\r\ndct:created\r\ndct:modified\r\nvann:preferredNamespaceURI\r\nvann:preferredNamespacePrefix\r\nowl:versionInfo\r\nowl:versionIRI\r\ndct:description\r\nrdfs:comment\r\ndct:license\r\ndct:abstract\r\n```\r\nA description of these terms and how they are used within the ontology, and by Widoco, is available on the Widoco site (see above guidelines link).\r\n\r\n## Configurations\r\nWidoco can be run via the command line using the configuration options, or as a wizard. The options can be saved in a config file with the default path of `config/config.properties`.\r\nSee an example [config file](https://github.com/dgarijo/Widoco/blob/master/src/main/resources/config/config.properties) from the Widoco test folder.\r\n\r\n### First run\r\nWhen running Widoco for the first time, the following configuration options should be set (not necessary, but as an example) -\r\n```bash\r\njava -jar widoco.jar \\\r\n\t-ontFile $PATH_TO_ONTOLOGY_FILE\r\n\t# OR \r\n\t-ontURI $URL_TO_ONTOLOGY\r\n\t-outFolder $PATH_TO_DOCUMENTATION\r\n\t-confFile $PATH_CONFIG\r\n\t-includeImportedOntologies\r\n\t-webVowl\r\n\t-licensius\r\n\t-rewriteAll\r\n```\r\n\r\nAlso consider adding the `-oops` flag to get an evaluation of the ontology using the [OOPS](http://oops.linkeddata.es/index.jsp) service that checks for common errors and malpractices in ontologies.\r\n\r\nIn the first run, Widoco will generate the documentation in the specified output folder. This documentation contains distinct HTML files with the following folder structure -\r\n```bash\r\n.\r\n\u251c\u2500\u2500 docs # -- THIS WAS THE OUTPUT DOCUMENTATION FOLDER --\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 406.html  # for HTTP406 errors\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index-en.html  # ontology documentation index page\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ontology.nt  # NT representation of the ontology\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ontology.ttl  # Turtle representation of the ontology\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ontology.xml  # RDF/XML representation of the ontology\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provenance  # provenance of the ontology\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 readme.md  # auto-generated Widoco file\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 resources\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sections  # These are the various sections in the documentation page\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 abstract-en.html\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 crossref-en.html\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 description-en.html\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 introduction-en.html\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 overview-en.html\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 references-en.html\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 webvowl\r\n7 directories, 30 files\r\n```\r\n\r\nYou can see what the default documentation page looks like by opening the `index-en.html` document in a browser. Be aware of the browser issues and limitations as discussed on the Widoco page. As of the date this page was written, there is an issue with Chrome regarding opening files without a special flag that allows such files to be directly loaded by the page itself.\r\n\r\nThe *sections* folder contains placeholder documents for the various sections. Now you can go ahead and edit the *sections* such as Abstract, Introduction, etc. (which are HTML, btw), and Widoco will persist these sections (which means not overwrite them) the next time the ontology is updated (and Widoco is run over it again).\r\n\r\n## Subsequent runs\r\nOn subsequent runs, Widoco (the smart tool that it is), can be configured to update only the changes in the ontology, preserving the underlying documentations (in essence, the *sections* folder). This is done via the `-crossRef` option, used in place of the `-rewriteAll` option which replaces all sections with placeholders.\r\n```bash\r\njava -jar widoco.jar \\\r\n\t-ontFile $PATH_TO_ONTOLOGY_FILE\r\n\t# OR \r\n\t-ontURI $URL_TO_ONTOLOGY\r\n\t-outFolder $PATH_TO_DOCUMENTATION\r\n\t-confFile $PATH_CONFIG\r\n\t-includeImportedOntologies\r\n\t-webVowl\r\n\t-licensius\r\n\t-crossRef\r\n```\r\nAs the documentation says,\r\n> The `-crossRef` option will ONLY generate the overview and cross reference sections. The index document will NOT be generated. The htaccess, provenance page, etc., will not be generated unless requested by other flags. This flag in intended to be used only after a first version of the documentation exists.\r\n\r\nAlong with updating the ontology documentation, Widoco can also be set to use the provenance trace to automatically link subsequent versions to the previous ones via the config file.\r\n\r\n## My config file\r\nMy config file, shared here for example , goes something like -\r\n```bash\r\nabstract=The abstract for my ontology\r\nontologyTitle=The title for my ontology\r\nontologyPrefix=myonto\r\nontologyNamespaceURI=http://example.com\r\nontologyName=The GDPR Provenance ontology\r\nthisVersionURI=http://example.com/version1.1\r\nlatestVersionURI=http://example.com\r\npreviousVersionURI=http://example.com/version1.0\r\ndateOfRelease=2017-08-01\r\nontologyRevisionNumber=1.1\r\nlicenseURI=http://purl.org/NET/rdflicense/cc-by4.0\r\nlicenseName=Creative Commons CC-BY\r\nlicenseIconURL=null\r\nciteAs=\r\nDOI=\r\nstatus=Ontology Specification Draft\r\nbackwardsCompatibleWith=\r\npublisher=\r\npublisherURI=\r\npublisherInstitution=\r\npublisherInstitutionURI=\r\nauthors=Harshvardhan J. Pandit\r\nauthorsURI=http://purl.org/adaptcentre/people/harshvardhan_pandit\r\nauthorsInstitution=ADAPT Centre, Trinity College Dublin\r\nauthorsInstitutionURI=https://adaptcentre.ie\r\ncontributors=\r\ncontributorsURI=\r\ncontributorsInstitution=\r\ncontributorsInstitutionURI=\r\nimportedOntologyNames=\r\nimportedOntologyURIs=\r\nextendedOntologyNames=\r\nextendedOntologyURIs=\r\nRDFXMLSerialization=ontology.xml\r\nTurtleSerialization=ontology.ttl\r\nN3Serialization=ontology.nt\r\n```", "headerimage": "", "highlight": "0", "section": 9}, {"id": 21, "title": "Setting up Pubby", "authors": "1", "date_created": "2017-08-23 16:57:37", "date_published": "2017-08-23 16:57:39", "date_updated": "2017-08-23 17:07:14", "is_published": "1", "short_description": "Using pubby to expose resources in a dataset", "tags": "173,189,172,177", "slug": "setting-up-pubby", "body_type": "markdown", "body_text": "<p><a href=\"http://wifo5-03.informatik.uni-mannheim.de/pubby/\">Pubby</a> is a nifty little tool\nthat is great for exposing RDF datasets accessed through SPARQL endpoints \nas browsable HTML pages. What this allows is to create a populated web-page for\nresources available in SPARQL endpoints. Pubby uses <code>DESCRIBE</code> queries to populate\nthe HTML page. To see it in action, visit [OPMW](http://opmw.org] example pages with\nthe <a href=\"http://www.opmw.org/export/page/resource/WorkflowTemplate/SIMILARWORDS\">Similar Words</a>\nexample showing all RDF links in HTML.</p>\n<h2 id=\"installation\">Installation</h2>\n<p>Pubby can be downloaded from the <a href=\"http://wifo5-03.informatik.uni-mannheim.de/pubby/download/\">download page</a>\nor the source can be accessed through the <a href=\"https://github.com/cygri/pubby\">Github project</a>.\nUsually, the latest version is advocated to be used, but in this case, I found an\nunresolved issue with showing RDF prefixes in the generated documents. There was\na proposed solution on <a href=\"https://stackoverflow.com/questions/25869241/pubby-displays-question-marks-instead-of-the-actual-namespaces\">StackOverflow</a> with two answers that\npropose <a href=\"https://stackoverflow.com/a/25869703\">adding prefixes to the config file</a> and\n<a href=\"https://stackoverflow.com/a/27406059\">setting the prefixes as URI</a> both of which\n<em>did not work</em> in my case. Therefore, downgraded from version 0.3.3 to version\n0.3.2. And important change in these two versions is that pubby changed the configuration\nfile format from N3 to Turtle. However, they both still look fairly similar, so there\nis not much of a change in terms of reading and configuration.</p>\n<p>To get pubby, use <code>curl</code> and unzip the contents like -</p>\n<pre class=\"codehilite\"><code class=\"language-bash\"># wget -O pubby.zip http://wifo5-03.informatik.uni-mannheim.de/pubby/download/pubby-0.3.3.zip\ncurl -o pubby.zip http://wifo5-03.informatik.uni-mannheim.de/pubby/download/pubby-0.3.3.zip\n# use unzip or jar -xf\njar -xf pubby.zip</code></pre>\n\n\n<h2 id=\"serving-with-jetty\">Serving with Jetty</h2>\n<p>Pubby can be served using Tomcat or Jetty, or any other mechanism of serving web containers.\nIt does not come with a <code>WAR</code> file, but contains a <code>WEB-INF</code> folder which is\nready to served. If pubby is to be served as the <em>root</em> which means it is\ndirectly accessible from wherever jetty is running, such as <code>localhost:8080</code>,\nthen the <code>webapps</code> folder must contain the pubby contents as <code>root</code> (folder name).\nOtherwise, jetty can be configured to run pubby as a servelet at the desired url.</p>\n<p>Jetty is available for download as a package, in which case, it is installed\nas a service, or one can download the portable application and set it up.\nIn this case, jetty can be setup as a service using the file\n<code>/etc/systemd/system/pubby.service</code> as -</p>\n<pre class=\"codehilite\"><code>[Unit]\nDescription=Pubby server using Jetty\nAfter=network.target\n\n[Service]\nUser=&lt; user &gt;\nGroup=&lt; dev &gt;\nWorkingDirectory=&lt; folder containing jetty &gt;\nExecStart=/usr/bin/java -jar start.jar\n\n[Install]\nWantedBy=multi-user.target</code></pre>\n\n\n<h3 id=\"serving-using-nginx\">Serving using Nginx</h3>\n<p>Once jetty is running the pubby servelet, Nginx can be configured to serve this using\na proxy service as -</p>\n<pre class=\"codehilite\"><code class=\"language-nginx\">location /&lt;DESIRED URL/ {\n    proxy_set_header X-Real-IP $remote_addr ;\n    proxy_set_header X-Forwarded-For $remote_addr ;\n    proxy_set_header Host $host ;\n    proxy_set_header X-NginX-Proxy true;\n    rewrite ^/&lt;DESIRED NAMESPACE SET IN PUBBY CONFIG&gt;/?(.*) /$1 break;\n    proxy_pass http://&lt;JETTY ADDRESS&gt;/;\n    proxy_redirect off;\n}</code></pre>\n\n\n<h2 id=\"configuration\">Configuration</h2>\n<p>The pubby config file is located in the <code>WEB-INF</code> folder and is named either\n<code>config.n3</code> for N3 or <code>config.ttl</code> for Turtle, depending on the version of\npubby being used.</p>\n<h3 id=\"prefixes\">Prefixes</h3>\n<p>The starting prefixes define the prefix seen in the HTML page output,\nalong with those used on the page.</p>\n<h3 id=\"server-configuration-section\">Server configuration section</h3>\n<p>This is the section marked as an instance of <code>conf:Configuration</code>.</p>\n<ul>\n<li><em>projectName</em> - this is the name of project displayed on the page</li>\n<li><em>projectHomepage</em> - this is the URI for the project homepage</li>\n<li><em>usePrefixesFrom</em> - this defines the location where the prefixes are loaded\nfrom, a value of <code>&lt;&gt;</code> indicates the config file, or this can contain a URI from\nwhich the prefixes will be loaded</li>\n<li><em>indexResource</em> - this is the URI of the resource that will be displayed\nwhen the 'homepage' of pubby is displayed; or to put it in another way, this is\nthe resource that will be displayed on the landing page</li>\n</ul>\n<h3 id=\"dataset-configuration-section\">Dataset configuration section</h3>\n<p>This is an section in the <em>Server configuration</em> section, defined as annotations\nof <code>conf:dataset</code> property.</p>\n<ul>\n<li><em>sparqlEndpoint</em> - this is the <em>SPARQL</em> endpoint URL from which resources will\nbe loaded</li>\n<li><em>datasetBase</em> - this is the common URI prefix, similar to the <code>@prefix</code> used in\nSPARQL queries</li>\n</ul>", "body": "[Pubby](http://wifo5-03.informatik.uni-mannheim.de/pubby/) is a nifty little tool\r\nthat is great for exposing RDF datasets accessed through SPARQL endpoints \r\nas browsable HTML pages. What this allows is to create a populated web-page for\r\nresources available in SPARQL endpoints. Pubby uses `DESCRIBE` queries to populate\r\nthe HTML page. To see it in action, visit [OPMW](http://opmw.org] example pages with\r\nthe [Similar Words](http://www.opmw.org/export/page/resource/WorkflowTemplate/SIMILARWORDS)\r\nexample showing all RDF links in HTML.\r\n\r\n## Installation\r\n\r\nPubby can be downloaded from the [download page](http://wifo5-03.informatik.uni-mannheim.de/pubby/download/)\r\nor the source can be accessed through the [Github project](https://github.com/cygri/pubby).\r\nUsually, the latest version is advocated to be used, but in this case, I found an\r\nunresolved issue with showing RDF prefixes in the generated documents. There was\r\na proposed solution on [StackOverflow](https://stackoverflow.com/questions/25869241/pubby-displays-question-marks-instead-of-the-actual-namespaces) with two answers that\r\npropose [adding prefixes to the config file](https://stackoverflow.com/a/25869703) and\r\n[setting the prefixes as URI](https://stackoverflow.com/a/27406059) both of which\r\n*did not work* in my case. Therefore, downgraded from version 0.3.3 to version\r\n0.3.2. And important change in these two versions is that pubby changed the configuration\r\nfile format from N3 to Turtle. However, they both still look fairly similar, so there\r\nis not much of a change in terms of reading and configuration.\r\n\r\nTo get pubby, use `curl` and unzip the contents like -\r\n```bash\r\n# wget -O pubby.zip http://wifo5-03.informatik.uni-mannheim.de/pubby/download/pubby-0.3.3.zip\r\ncurl -o pubby.zip http://wifo5-03.informatik.uni-mannheim.de/pubby/download/pubby-0.3.3.zip\r\n# use unzip or jar -xf\r\njar -xf pubby.zip\r\n```\r\n\r\n## Serving with Jetty\r\n\r\nPubby can be served using Tomcat or Jetty, or any other mechanism of serving web containers.\r\nIt does not come with a `WAR` file, but contains a `WEB-INF` folder which is\r\nready to served. If pubby is to be served as the *root* which means it is\r\ndirectly accessible from wherever jetty is running, such as `localhost:8080`,\r\nthen the `webapps` folder must contain the pubby contents as `root` (folder name).\r\nOtherwise, jetty can be configured to run pubby as a servelet at the desired url.\r\n\r\nJetty is available for download as a package, in which case, it is installed\r\nas a service, or one can download the portable application and set it up.\r\nIn this case, jetty can be setup as a service using the file\r\n`/etc/systemd/system/pubby.service` as -\r\n```\r\n[Unit]\r\nDescription=Pubby server using Jetty\r\nAfter=network.target\r\n                                                                                \r\n[Service]\r\nUser=< user >\r\nGroup=< dev >\r\nWorkingDirectory=< folder containing jetty >\r\nExecStart=/usr/bin/java -jar start.jar\r\n                                                                                \r\n[Install]\r\nWantedBy=multi-user.target\r\n```\r\n\r\n### Serving using Nginx\r\nOnce jetty is running the pubby servelet, Nginx can be configured to serve this using\r\na proxy service as -\r\n```nginx\r\nlocation /<DESIRED URL/ {\r\n\tproxy_set_header X-Real-IP $remote_addr ;\r\n\tproxy_set_header X-Forwarded-For $remote_addr ;\r\n\tproxy_set_header Host $host ;\r\n\tproxy_set_header X-NginX-Proxy true;\r\n\trewrite ^/<DESIRED NAMESPACE SET IN PUBBY CONFIG>/?(.*) /$1 break;\r\n\tproxy_pass http://<JETTY ADDRESS>/;\r\n\tproxy_redirect off;\r\n}\r\n```\r\n\r\n## Configuration\r\nThe pubby config file is located in the `WEB-INF` folder and is named either\r\n`config.n3` for N3 or `config.ttl` for Turtle, depending on the version of\r\npubby being used.\r\n\r\n### Prefixes\r\nThe starting prefixes define the prefix seen in the HTML page output,\r\nalong with those used on the page.\r\n\r\n### Server configuration section\r\nThis is the section marked as an instance of `conf:Configuration`.\r\n\r\n* *projectName* - this is the name of project displayed on the page\r\n* *projectHomepage* - this is the URI for the project homepage\r\n* *usePrefixesFrom* - this defines the location where the prefixes are loaded\r\nfrom, a value of `<>` indicates the config file, or this can contain a URI from\r\nwhich the prefixes will be loaded\r\n* *indexResource* - this is the URI of the resource that will be displayed\r\nwhen the 'homepage' of pubby is displayed; or to put it in another way, this is\r\nthe resource that will be displayed on the landing page\r\n\r\n### Dataset configuration section\r\nThis is an section in the *Server configuration* section, defined as annotations\r\nof `conf:dataset` property.\r\n\r\n* *sparqlEndpoint* - this is the *SPARQL* endpoint URL from which resources will\r\nbe loaded\r\n* *datasetBase* - this is the common URI prefix, similar to the `@prefix` used in\r\nSPARQL queries", "headerimage": "", "highlight": "0", "section": 9}, {"id": 19, "title": "Setting up Fuseki", "authors": "1", "date_created": "2017-08-23 15:27:44", "date_published": "2017-08-23 15:27:46", "date_updated": "2017-08-23 16:21:24", "is_published": "1", "short_description": "Getting Apache Fuseki up and running with minimal configuration", "tags": "186,185,172,177", "slug": "setting-up-fuseki", "body_type": "markdown", "body_text": "<p><a href=\"https://jena.apache.org/\">Apache Jena</a> is an amazing (Java) framework for working\nwith semantic web ontologies. <a href=\"https://jena.apache.org/documentation/fuseki2/index.html\">Fuseki</a> is a SPARQL end-point which is super-easy to set up and use, and <a href=\"https://jena.apache.org/documentation/tdb/index.html\">TDB</a> is the native triple-store that is already configured into Fuseki and just needs to be enabled.\nIf the purpose of setting up a SPARQL end-point or triple-store is mostly <em>dev</em> and doesn't need to be <em>production</em> grade, Fuseki+TDB is the best way to experiment.</p>\n<h2 id=\"installing-fuseki\">Installing Fuseki</h2>\n<p>The <a href=\"https://jena.apache.org/download/\">download</a> for the Jena framework lists the Fuseki\ndownloads under <em>Apache Jena Fuseki</em>, and has two downloads, one of which is a <code>.tar.gz</code> while the other is a <code>.zip</code>. Fuseki needs <code>Java-8</code> to be installed, so if you don't have that,\nyou can install it with-</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">sudo apt-get install openjdk-8-jre openjdk-8-jdk</code></pre>\n\n\n<p>-and you might need to add some repositories (or PPA) to get the OpenJDK into <em>apt</em>.\nMost online resources detail installing the <em>official</em> version of Java, provided by Oracle, though I would prefer to use OpenJDK rather than get it from Oracle.</p>\n<p>To download fuseki-files directly to the server, you can use curl like so-</p>\n<pre class=\"codehilite\"><code class=\"language-bash\"># this will download the file into the current directory\n# link copied from fuseki download page\ncurl -o fuseki.zip http://www-us.apache.org/dist/jena/binaries/apache-jena-3.4.0.zip</code></pre>\n\n\n<p>Then unzip the file with</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">unzip fuseki.zip</code></pre>\n\n\n<p>Or if you don't have <code>unzip</code> installed, you can use <code>java</code>'s packaging tool like-</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">jar -xf fuseki.zip</code></pre>\n\n\n<p>If you downloaded the <code>.tar.gz</code> version, use</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">tar -xvf fuseki.tar.gz</code></pre>\n\n\n<h2 id=\"configurations\">Configurations</h2>\n<p>The fuseki configurations are in the file <code>run/config.ttl</code> which is in the Turtle format.\nThat's some nice dogfooding right there, a RDF triple-store and SPARQL endpoint configured\nusing RDF itself. The other bits of configurations are in the folder <code>run/configurations/</code> and are populated by Fuseki if you add in a service, or can have manually added services as well.</p>\n<p>The <a href=\"https://jena.apache.org/documentation/fuseki2/fuseki-configuration.html\">configuration file documentation</a>\nspecifies the various parameters and options that can be entered into the file.\nThere are two types of entries - <em>services</em> and <em>datasets</em>, with services providing\na common endpoint for various datasets and configurations.</p>\n<h3 id=\"service\">Service</h3>\n<p>A service can be declared as (example from official docs) -</p>\n<pre class=\"codehilite\"><code class=\"language-turtle\">&lt;#service1&gt; rdf:type fuseki:Service ;\n    fuseki:name                       &quot;ds&quot; ;       # http://host:port/ds\n    fuseki:serviceQuery               &quot;sparql&quot; ;   # SPARQL query service\n    fuseki:serviceQuery               &quot;query&quot; ;    # SPARQL query service (alt name)\n    fuseki:serviceUpdate              &quot;update&quot; ;   # SPARQL update service\n    fuseki:serviceUpload              &quot;upload&quot; ;   # Non-SPARQL upload service\n    fuseki:serviceReadWriteGraphStore &quot;data&quot; ;     # SPARQL Graph store protocol (read and write)\n    # A separate read-only graph store endpoint:\n    fuseki:serviceReadGraphStore      &quot;get&quot; ;      # SPARQL Graph store protocol (read only)\n    fuseki:dataset                   &lt;#dataset&gt; ;\n    .</code></pre>\n\n\n<p>which declares that the service has a SPARQL endpoint, with update and upload features,\nand serves the dataset defined by <code>#dataset</code>. As there is no special configuration,\nthe dataset is 'stored' in memory.</p>\n<h3 id=\"dataset\">Dataset</h3>\n<p>For storing the database using <code>TDB</code>, define the dataset config as (from official docs) -</p>\n<pre class=\"codehilite\"><code class=\"language-turtle\">&lt;#dataset&gt; rdf:type      tdb:DatasetTDB ;\n    tdb:location &quot;DB&quot; ; # &lt;----- THIS LINE --&gt;\n    # Query timeout on this dataset (1s, 1000 milliseconds)\n    ja:context [ ja:cxtName &quot;arq:queryTimeout&quot; ;  ja:cxtValue &quot;1000&quot; ] ;\n    # Make the default graph be the union of all named graphs.\n    ## tdb:unionDefaultGraph true ;\n     .</code></pre>\n\n\n<p>this creates (if not present) a folder called <code>DB</code> and stores all the data files in it.\nThis folder is portable, so you can move the folder around, take backups, etc.</p>\n<h2 id=\"exposing-sparql-end-point-using-nginx\">Exposing SPARQL end-point using Nginx</h2>\n<p>There are several more options on the official documentation which are highly encouraged\nto be read. I'll detail a use-case for setting up the server on localhost, exposing it\nusing Nginx and serving an RDF dataset persisted by TDB.</p>\n<h3 id=\"setting-up-fuseki-as-a-system-service\">Setting up Fuseki as a system service</h3>\n<p>Setting up fuseki as a system service allows the service to be managed using\nthe system utils (<em>service</em> or <em>systemd</em>). \nThere are <a href=\"https://jena.apache.org/documentation/fuseki2/fuseki-run.html\">official docs</a>\ndetailing this, or alternatively, this can be done by creating a file in\n<code>/etc/systemd/system</code> with the name <code>fuseki.service</code> with the contents -</p>\n<pre class=\"codehilite\"><code>[Unit]\nDescription=Fuseki server for SPARQL endpoint\nAfter=network.target\n\n[Service]\nUser=&lt;user&gt;\nGroup=&lt;usergroup&gt;\nWorkingDirectory=&lt;location of fuseki jar&gt;\nExecStart=/usr/bin/java -jar fuseki-server.jar &lt;options&gt;\n\n[Install]\nWantedBy=multi-user.target</code></pre>\n\n\n<h2 id=\"security\">Security</h2>\n<p>The one thing about fuseki is that it offers no security or access control by itself.\nInstead, <a href=\"http://shiro.apache.org/\">Apache Shiro</a> is used to provide a limited amount\nof security. Shiro allows for setting username/password for access to the fuseki server\ninstance running, so that without the credentials, one cannot access the datasets.</p>", "body": "[Apache Jena](https://jena.apache.org/) is an amazing (Java) framework for working\r\nwith semantic web ontologies. [Fuseki](https://jena.apache.org/documentation/fuseki2/index.html) is a SPARQL end-point which is super-easy to set up and use, and [TDB](https://jena.apache.org/documentation/tdb/index.html) is the native triple-store that is already configured into Fuseki and just needs to be enabled.\r\nIf the purpose of setting up a SPARQL end-point or triple-store is mostly *dev* and doesn't need to be *production* grade, Fuseki+TDB is the best way to experiment.\r\n\r\n## Installing Fuseki\r\n\r\nThe [download](https://jena.apache.org/download/) for the Jena framework lists the Fuseki\r\ndownloads under *Apache Jena Fuseki*, and has two downloads, one of which is a `.tar.gz` while the other is a `.zip`. Fuseki needs `Java-8` to be installed, so if you don't have that,\r\nyou can install it with-\r\n```bash\r\nsudo apt-get install openjdk-8-jre openjdk-8-jdk\r\n```\r\n-and you might need to add some repositories (or PPA) to get the OpenJDK into *apt*.\r\nMost online resources detail installing the *official* version of Java, provided by Oracle, though I would prefer to use OpenJDK rather than get it from Oracle.\r\n\r\nTo download fuseki-files directly to the server, you can use curl like so-\r\n```bash\r\n# this will download the file into the current directory\r\n# link copied from fuseki download page\r\ncurl -o fuseki.zip http://www-us.apache.org/dist/jena/binaries/apache-jena-3.4.0.zip\r\n```\r\n\r\nThen unzip the file with\r\n```bash\r\nunzip fuseki.zip\r\n```\r\nOr if you don't have `unzip` installed, you can use `java`'s packaging tool like-\r\n```bash\r\njar -xf fuseki.zip\r\n```\r\nIf you downloaded the `.tar.gz` version, use\r\n```bash\r\ntar -xvf fuseki.tar.gz\r\n```\r\n\r\n## Configurations\r\n\r\nThe fuseki configurations are in the file `run/config.ttl` which is in the Turtle format.\r\nThat's some nice dogfooding right there, a RDF triple-store and SPARQL endpoint configured\r\nusing RDF itself. The other bits of configurations are in the folder `run/configurations/` and are populated by Fuseki if you add in a service, or can have manually added services as well.\r\n\r\nThe [configuration file documentation](https://jena.apache.org/documentation/fuseki2/fuseki-configuration.html)\r\nspecifies the various parameters and options that can be entered into the file.\r\nThere are two types of entries - *services* and *datasets*, with services providing\r\na common endpoint for various datasets and configurations.\r\n### Service\r\nA service can be declared as (example from official docs) -\r\n```turtle\r\n<#service1> rdf:type fuseki:Service ;\r\n    fuseki:name                       \"ds\" ;       # http://host:port/ds\r\n    fuseki:serviceQuery               \"sparql\" ;   # SPARQL query service\r\n    fuseki:serviceQuery               \"query\" ;    # SPARQL query service (alt name)\r\n    fuseki:serviceUpdate              \"update\" ;   # SPARQL update service\r\n    fuseki:serviceUpload              \"upload\" ;   # Non-SPARQL upload service\r\n    fuseki:serviceReadWriteGraphStore \"data\" ;     # SPARQL Graph store protocol (read and write)\r\n    # A separate read-only graph store endpoint:\r\n    fuseki:serviceReadGraphStore      \"get\" ;      # SPARQL Graph store protocol (read only)\r\n    fuseki:dataset                   <#dataset> ;\r\n    .\r\n```\r\nwhich declares that the service has a SPARQL endpoint, with update and upload features,\r\nand serves the dataset defined by `#dataset`. As there is no special configuration,\r\nthe dataset is 'stored' in memory.\r\n### Dataset\r\nFor storing the database using `TDB`, define the dataset config as (from official docs) -\r\n```turtle\r\n<#dataset> rdf:type      tdb:DatasetTDB ;\r\n    tdb:location \"DB\" ; # <----- THIS LINE -->\r\n    # Query timeout on this dataset (1s, 1000 milliseconds)\r\n    ja:context [ ja:cxtName \"arq:queryTimeout\" ;  ja:cxtValue \"1000\" ] ;\r\n    # Make the default graph be the union of all named graphs.\r\n    ## tdb:unionDefaultGraph true ;\r\n     .\r\n```\r\nthis creates (if not present) a folder called `DB` and stores all the data files in it.\r\nThis folder is portable, so you can move the folder around, take backups, etc.\r\n\r\n## Exposing SPARQL end-point using Nginx\r\nThere are several more options on the official documentation which are highly encouraged\r\nto be read. I'll detail a use-case for setting up the server on localhost, exposing it\r\nusing Nginx and serving an RDF dataset persisted by TDB.\r\n\r\n### Setting up Fuseki as a system service\r\nSetting up fuseki as a system service allows the service to be managed using\r\nthe system utils (*service* or *systemd*). \r\nThere are [official docs](https://jena.apache.org/documentation/fuseki2/fuseki-run.html)\r\ndetailing this, or alternatively, this can be done by creating a file in\r\n`/etc/systemd/system` with the name `fuseki.service` with the contents -\r\n```\r\n[Unit]\r\nDescription=Fuseki server for SPARQL endpoint\r\nAfter=network.target\r\n                                                                                \r\n[Service]\r\nUser=<user>\r\nGroup=<usergroup>\r\nWorkingDirectory=<location of fuseki jar>\r\nExecStart=/usr/bin/java -jar fuseki-server.jar <options>\r\n                                                                                \r\n[Install]\r\nWantedBy=multi-user.target\r\n```\r\n\r\n## Security\r\nThe one thing about fuseki is that it offers no security or access control by itself.\r\nInstead, [Apache Shiro](http://shiro.apache.org/) is used to provide a limited amount\r\nof security. Shiro allows for setting username/password for access to the fuseki server\r\ninstance running, so that without the credentials, one cannot access the datasets.", "headerimage": "", "highlight": "0", "section": 9}, {"id": 20, "title": "Setting up Openlink Virtuoso", "authors": "1", "date_created": "2017-08-23 16:11:28", "date_published": "2017-08-23 16:11:30", "date_updated": "2017-08-23 16:21:02", "is_published": "1", "short_description": "Setting up Virtuoso as a triple-store and serving with Nginx", "tags": "133,173,187,177,188", "slug": "setting-up-openlink-virtuoso", "body_type": "markdown", "body_text": "<p><a href=\"https://virtuoso.openlinksw.com/linked-data/\">Openlink Virtuoso</a> is a powerful\ntriple-store (and also a traditional RDBMS) with many different features. Setting\nup virtuoso is easy as packages are available in most distributions. Virtuoso has\na bizzare collection of documentation which is scattered, unorganised, and sometimes\nmissing. Depite this, it is a solid tool which is easy to set up and use, and comes\nconfigured ready for production use.</p>\n<h2 id=\"installation\">Installation</h2>\n<p>The package <code>virtuoso-opensource</code> is available on Debian based systems, and can be\ninstalled with -</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">sudo apt-get install virtuoso-opensource</code></pre>\n\n\n<p>which will install virtuoso and set it up as a system service with the name</p>\n<pre class=\"codehilite\"><code>virtuoso-opensource-X.x</code></pre>\n\n\n<p>with <code>X.x</code> being version numbers, which for me were <code>6.1</code>.\nThe service can be managed as:</p>\n<pre class=\"codehilite\"><code class=\"language-bash\"># start, stop, restart, status\nsudo service virtuoso-opensource-X.x start\nsudo service virtuoso-opensource-X.x stop\nsudo service virtuoso-opensource-X.x restart\nsudo service virtuoso-opensource-X.x status</code></pre>\n\n\n<p>During installation, virtuoso will ask to set a password for two users -\n<code>DBA</code> and <code>DAV</code> which are like <code>admins</code> for the web interface and management actions.\nIt is essential to remember the password as this is required to make changes to\nvirtuoso and also to add other users.</p>\n<h2 id=\"configuration\">Configuration</h2>\n<p>The config file is located at -</p>\n<pre class=\"codehilite\"><code>/etc/virtuoso-opensource-X.x/virtuoso.ini</code></pre>\n\n\n<p>and contains settings for storage location and server settings. Virtuoso has the option\nof serving the management interface over a SSL certificate (located in the <em>Parameters</em>\nsection) which is commented out by default. The configuration for the Web interface is\nin the <code>HTTPServer</code> section.</p>\n<p><code>ServerPort</code> refers to the port the virtuoso interface runs at, which is <code>8890</code> by default,\nwhich can be changed through this option. A description of the various options is\navailable at <a href=\"http://docs.openlinksw.com/virtuoso/dbadm/\">link</a>.</p>\n<h2 id=\"conductor\">Conductor</h2>\n<p>The virtuoso web interface is called conductor, and offers management capabilities\nfor all its features. It is served by default at <code>/conductor</code> URL prefixed\nby wherever virtuoso is being served.</p>\n<h3 id=\"linked-data\">Linked Data</h3>\n<p>The <em>linked data</em> section in <em>Conductor</em> offers a <em>SPARQL</em> endpoint, query interface,\nand management capabilities for graphs and datasets. The default tab for <em>SPARQL</em>\nis a query interface which queries the (<em>default</em>) graph specified and displays\nthe results in the page itself. <em>Graphs</em> shows all available graphs in the triple store,\nand virtuoso comes with a lot of RDF data and some graphs by default, which one can\nassume are required for its configurations and data settings. The <em>Namespaces</em> tab\nshows the stored namespaces for RDF graphs, and one can add custom namespaces here.\n<em>Quad Store Upload</em> provides a simple way to upload a RDF file as a dataset or import\nit from a URL. It requires the <em>named graph IRI</em> under which the dataset is stored in the\ntriple store. There is no <em>default</em> graph, therefore the namespace <em>has</em> to be provided.</p>\n<h2 id=\"isql\">iSQL</h2>\n<p>Virtuoso provides a utility called <em>Interactive SQL</em> or <em>iSQL</em> which is accessed\nusing <code>isql-vt</code> or can be symlinked from <code>/usr/bin/isql-vt</code>. This utility provides\nSQL-like access to the datasets which can be used to perform SPARQL queries or\nupload data into the triple store.</p>\n<h2 id=\"sparql-endpoint\">SPARQL Endpoint</h2>\n<p>By default, <code>/sparql</code> is the <em>SPARQL</em> endpoint provided by virtuoso, and requires no\naccess control to set up or access. So once you have used <em>Conductor</em> or <em>iSQL</em> to upload\nthe dataset, the <em>SPARQL</em> endpoint is ready to serve the data for the given graph IRI.\nThe only thing to configure is to serve datasets under a given IRI.</p>\n<h2 id=\"exposing-virtuoso-interfaces-using-nginx\">Exposing Virtuoso interfaces using Nginx</h2>\n<p>By default, Virtuoso runs at <code>localhost:8890</code>, which Nginx can be configured with a\nproxy to pass traffic to the server. However, for some reason, Nginx cannot pass\nin a reverse proxy, or map URL to the localhost as required. A <em>hack</em> around this\nis to configure all the locations virtuoso requires as URL accesses, and proxy pass\nthem to the Virtuoso server. A list of them is-</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">/virtuoso\n/conductor\n/about\n/category\n/class\n/data\n/describe\n/delta.vsp\n/fct\n/issparql\n/ontology\n/page\n/property\n/rdfdesc\n/resource\n/services\n/snorql\n/sparql-auth\n/sparql\n/statics\n/void\n/wikicompany</code></pre>\n\n\n<p>If a particular service is to be restricted or not provided, then simply remove\nits URL from the Nginx configurations. An example of a proxy configuration for\na URL is -</p>\n<pre class=\"codehilite\"><code class=\"language-nginx\">location /sparql {\n    proxy_set_header X-Real-IP $remote_addr ;\n    proxy_set_header X-Forwarded-For $remote_addr ;\n    proxy_set_header Host $host ;\n    proxy_set_header X-NginX-Proxy true;\n    rewrite ^/virtuoso/?(.*) /$1 break;\n    proxy_pass http://localhost:8890/;\n    proxy_redirect off;</code></pre>", "body": "[Openlink Virtuoso](https://virtuoso.openlinksw.com/linked-data/) is a powerful\r\ntriple-store (and also a traditional RDBMS) with many different features. Setting\r\nup virtuoso is easy as packages are available in most distributions. Virtuoso has\r\na bizzare collection of documentation which is scattered, unorganised, and sometimes\r\nmissing. Depite this, it is a solid tool which is easy to set up and use, and comes\r\nconfigured ready for production use.\r\n\r\n## Installation\r\nThe package `virtuoso-opensource` is available on Debian based systems, and can be\r\ninstalled with -\r\n```bash\r\nsudo apt-get install virtuoso-opensource\r\n```\r\nwhich will install virtuoso and set it up as a system service with the name\r\n```\r\nvirtuoso-opensource-X.x\r\n```\r\nwith `X.x` being version numbers, which for me were `6.1`.\r\nThe service can be managed as:\r\n```bash\r\n# start, stop, restart, status\r\nsudo service virtuoso-opensource-X.x start\r\nsudo service virtuoso-opensource-X.x stop\r\nsudo service virtuoso-opensource-X.x restart\r\nsudo service virtuoso-opensource-X.x status\r\n```\r\n\r\nDuring installation, virtuoso will ask to set a password for two users -\r\n`DBA` and `DAV` which are like `admins` for the web interface and management actions.\r\nIt is essential to remember the password as this is required to make changes to\r\nvirtuoso and also to add other users.\r\n\r\n## Configuration\r\nThe config file is located at -\r\n```\r\n/etc/virtuoso-opensource-X.x/virtuoso.ini\r\n```\r\nand contains settings for storage location and server settings. Virtuoso has the option\r\nof serving the management interface over a SSL certificate (located in the *Parameters*\r\nsection) which is commented out by default. The configuration for the Web interface is\r\nin the `HTTPServer` section.\r\n\r\n`ServerPort` refers to the port the virtuoso interface runs at, which is `8890` by default,\r\nwhich can be changed through this option. A description of the various options is\r\navailable at [link](http://docs.openlinksw.com/virtuoso/dbadm/).\r\n\r\n## Conductor\r\nThe virtuoso web interface is called conductor, and offers management capabilities\r\nfor all its features. It is served by default at `/conductor` URL prefixed\r\nby wherever virtuoso is being served.\r\n\r\n### Linked Data\r\nThe *linked data* section in *Conductor* offers a *SPARQL* endpoint, query interface,\r\nand management capabilities for graphs and datasets. The default tab for *SPARQL*\r\nis a query interface which queries the (*default*) graph specified and displays\r\nthe results in the page itself. *Graphs* shows all available graphs in the triple store,\r\nand virtuoso comes with a lot of RDF data and some graphs by default, which one can\r\nassume are required for its configurations and data settings. The *Namespaces* tab\r\nshows the stored namespaces for RDF graphs, and one can add custom namespaces here.\r\n*Quad Store Upload* provides a simple way to upload a RDF file as a dataset or import\r\nit from a URL. It requires the *named graph IRI* under which the dataset is stored in the\r\ntriple store. There is no *default* graph, therefore the namespace *has* to be provided.\r\n\r\n## iSQL\r\nVirtuoso provides a utility called *Interactive SQL* or *iSQL* which is accessed\r\nusing `isql-vt` or can be symlinked from `/usr/bin/isql-vt`. This utility provides\r\nSQL-like access to the datasets which can be used to perform SPARQL queries or\r\nupload data into the triple store.\r\n\r\n## SPARQL Endpoint\r\nBy default, `/sparql` is the *SPARQL* endpoint provided by virtuoso, and requires no\r\naccess control to set up or access. So once you have used *Conductor* or *iSQL* to upload\r\nthe dataset, the *SPARQL* endpoint is ready to serve the data for the given graph IRI.\r\nThe only thing to configure is to serve datasets under a given IRI.\r\n\r\n## Exposing Virtuoso interfaces using Nginx\r\nBy default, Virtuoso runs at `localhost:8890`, which Nginx can be configured with a\r\nproxy to pass traffic to the server. However, for some reason, Nginx cannot pass\r\nin a reverse proxy, or map URL to the localhost as required. A *hack* around this\r\nis to configure all the locations virtuoso requires as URL accesses, and proxy pass\r\nthem to the Virtuoso server. A list of them is-\r\n```bash\r\n/virtuoso\r\n/conductor\r\n/about\r\n/category\r\n/class\r\n/data\r\n/describe\r\n/delta.vsp\r\n/fct\r\n/issparql\r\n/ontology\r\n/page\r\n/property\r\n/rdfdesc\r\n/resource\r\n/services\r\n/snorql\r\n/sparql-auth\r\n/sparql\r\n/statics\r\n/void\r\n/wikicompany\r\n```\r\nIf a particular service is to be restricted or not provided, then simply remove\r\nits URL from the Nginx configurations. An example of a proxy configuration for\r\na URL is -\r\n```nginx\r\nlocation /sparql {\r\n\tproxy_set_header X-Real-IP $remote_addr ;\r\n\tproxy_set_header X-Forwarded-For $remote_addr ;\r\n\tproxy_set_header Host $host ;\r\n\tproxy_set_header X-NginX-Proxy true;\r\n\trewrite ^/virtuoso/?(.*) /$1 break;\r\n\tproxy_pass http://localhost:8890/;\r\n\tproxy_redirect off;\r\n```", "headerimage": "", "highlight": "0", "section": 9}, {"id": 16, "title": "Primer on Semantic Web Ontologies", "authors": "1", "date_created": "2017-08-18 17:51:38", "date_published": "2017-08-18 17:51:39", "date_updated": "2017-08-18 19:25:32", "is_published": "1", "short_description": "A short description for RDF,RDFS, and OWL and their serialisation formats", "tags": "173,176,174,175,172", "slug": "primer-on-semantic-web-ontologies", "body_type": "markdown", "body_text": "<h2 id=\"introduction\">Introduction</h2>\n<p>The basic ontology for recording anything (and everything)\nis <a href=\"https://www.w3.org/RDF/\">RDF</a>, which is just a way of expressing\nknowledge in the form of <em>triples</em> or <em>(subject, object, predicate)</em> form.\n<a href=\"https://www.w3.org/TR/rdf-schema/\">RDFS</a> and <a href=\"https://www.w3.org/OWL/\">OWL</a>\nwhich build a relationship-constraint model on top of RDF.\nTo summarise, <code>RDF</code> allows expressing knowledge, pure and simple; <code>RDFS</code>\nand <code>OWL</code> add additional relationships such as hierarchy, inheritance,\nand restrictions (or in the case of <code>OWL</code>, axioms) that enrich the knowledge\nthrough the use of 'structure'. This is not a guide to learn about\nsemantic web ontologies, but if you're looking for that sort of a resource,\nyou may as well start with a <a href=\"https://stackoverflow.com/questions/1740341/what-is-the-difference-between-rdf-and-owl\">StackOverflow answer</a>\nand then move on to <a href=\"https://en.wikipedia.org/wiki/Web_Ontology_Language\">Wikipedia - OWL</a>.</p>\n<h2 id=\"usage\">Usage</h2>\n<p>RDF is used to represent knowledge, but not its validity or <em>truth</em>. So,\nstatements like <code>&lt;harsh&gt; &lt;is&gt; &lt;awesome&gt;</code> are perfectly valid RDF, but there\nis no way to check their 'validity' or 'correctness'. To express some\nconstraints over the use of resources, one uses RDFS and OWL to structure\nhow the knowledge may be represented. The ontologies also additionally\nallow inheritance and axioms, which can help create new knowledge through\ninference and reasoning. For example,</p>\n<pre class=\"codehilite\"><code>&lt;class:Man&gt; &lt;rdf:type&gt; &lt;subclass:Human&gt;\n&lt;class:Woman&gt; &lt;rdf:type&gt; &lt;subclass:Human&gt;\n&lt;property:isMother&gt;\n    &lt;rdf:type&gt; &lt;owl:Property&gt;\n    &lt;rdf:domain&gt; &lt;class:Woman&gt;\n    &lt;rdf:range&gt; &lt;class:Human&gt;\n&lt;property:isSon&gt;\n    &lt;rdf:type&gt; &lt;owl:Property&gt;\n    &lt;rdf:domain&gt; &lt;class:Man&gt;\n    &lt;rdf:range&gt; &lt;class:Human&gt;\n\n&lt;node:Harsh&gt; &lt;rdf:type&gt; &lt;class:Man&gt;\n    &lt;property:isSon&gt; &lt;node:HarshMom&gt;\n&lt;node:HarshMom&gt; &lt;property:isMother&gt; &lt;node:Harsh&gt;</code></pre>\n\n\n<p>defines classes <code>Man</code> and <code>Woman</code> as subclasses of <code>Human</code>, with the\nproperties <code>isMother</code> and <code>isSon</code> with domains and ranges. It then\ndefines that <code>Harsh</code> is a <code>Man</code>, and is the son of <code>HarshMom</code>. At this point,\nthe only inference possible is that <code>HarshMom</code> is of type <code>Human</code>. The next\nsentence specifies that <code>HarshMom</code> is the mother of <code>Harsh</code>, and therefore,\nit is possible to infer that <code>HarshMom</code> is a <code>Woman</code>. Although this is a simple\nexample, it amply shows how the structuring of knowledge can be shaped\nusing relationships. Simpler constraints help represent complex knowledge.</p>\n<h2 id=\"formats\">Formats</h2>\n<p>RDFS and OWL are serialised down into RDF, so they are essentially represented as\nRDF in most cases. There are possibilities of having OWL only formats, which do\nnot cater to RDF, but OWL, as an ontology is based on RDF.</p>\n<ul>\n<li><strong>RDF/XML:</strong> This is (sort-of) the default format for serialising RDF. Files usually end in <code>.rdf</code> though <code>.xml</code> extension would also be valid, but is not recommended. The content-type is <code>text/rdf+xml</code>.</li>\n<li><strong>Turtle:</strong> This is the easiest format to read and write in, and is quite concise. It's extenion is <code>.ttl</code> and the content-type is <code>text/turtle</code>.</li>\n<li><strong>N-Triples:</strong> This is a similar format to Turtle, though is arguably less concise. The extension is <code>.nt</code> and the content-type is <code>application/n_triples</code>.</li>\n<li><strong>Notation 3:</strong> Another concise format, it's extension is <code>.n3</code> and the content-type is <code>text/n3</code>.</li>\n<li><strong>JSON-LD:</strong> To make the propogation of RDF easier on the web, the JSON-LD format was developed. It uses a notation based on the JSON format with annotations catering to RDF keywords. It's extension is <code>.jsonld</code> and content-type is <code>application/ld+json</code>.</li>\n<li><strong>RDFa:</strong> This is a way to embed RDF information in HTML tags, while still allowing traditional page markup to be used and displayed. This is, in theory, the best of both worlds as the resulting document is presentable for human as well as machine consumption.</li>\n</ul>", "body": "## Introduction\r\nThe basic ontology for recording anything (and everything)\r\nis [RDF](https://www.w3.org/RDF/), which is just a way of expressing\r\nknowledge in the form of *triples* or *(subject, object, predicate)* form.\r\n[RDFS](https://www.w3.org/TR/rdf-schema/) and [OWL](https://www.w3.org/OWL/)\r\nwhich build a relationship-constraint model on top of RDF.\r\nTo summarise, `RDF` allows expressing knowledge, pure and simple; `RDFS`\r\nand `OWL` add additional relationships such as hierarchy, inheritance,\r\nand restrictions (or in the case of `OWL`, axioms) that enrich the knowledge\r\nthrough the use of 'structure'. This is not a guide to learn about\r\nsemantic web ontologies, but if you're looking for that sort of a resource,\r\nyou may as well start with a [StackOverflow answer](https://stackoverflow.com/questions/1740341/what-is-the-difference-between-rdf-and-owl)\r\nand then move on to [Wikipedia - OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language).\r\n\r\n## Usage\r\nRDF is used to represent knowledge, but not its validity or *truth*. So,\r\nstatements like `<harsh> <is> <awesome>` are perfectly valid RDF, but there\r\nis no way to check their 'validity' or 'correctness'. To express some\r\nconstraints over the use of resources, one uses RDFS and OWL to structure\r\nhow the knowledge may be represented. The ontologies also additionally\r\nallow inheritance and axioms, which can help create new knowledge through\r\ninference and reasoning. For example,\r\n\r\n```\r\n<class:Man> <rdf:type> <subclass:Human>\r\n<class:Woman> <rdf:type> <subclass:Human>\r\n<property:isMother>\r\n\t<rdf:type> <owl:Property>\r\n\t<rdf:domain> <class:Woman>\r\n\t<rdf:range> <class:Human>\r\n<property:isSon>\r\n\t<rdf:type> <owl:Property>\r\n\t<rdf:domain> <class:Man>\r\n\t<rdf:range> <class:Human>\r\n\r\n<node:Harsh> <rdf:type> <class:Man>\r\n\t<property:isSon> <node:HarshMom>\r\n<node:HarshMom> <property:isMother> <node:Harsh>\r\n```\r\n\r\ndefines classes `Man` and `Woman` as subclasses of `Human`, with the\r\nproperties `isMother` and `isSon` with domains and ranges. It then\r\ndefines that `Harsh` is a `Man`, and is the son of `HarshMom`. At this point,\r\nthe only inference possible is that `HarshMom` is of type `Human`. The next\r\nsentence specifies that `HarshMom` is the mother of `Harsh`, and therefore,\r\nit is possible to infer that `HarshMom` is a `Woman`. Although this is a simple\r\nexample, it amply shows how the structuring of knowledge can be shaped\r\nusing relationships. Simpler constraints help represent complex knowledge.\r\n\r\n## Formats\r\n\r\nRDFS and OWL are serialised down into RDF, so they are essentially represented as\r\nRDF in most cases. There are possibilities of having OWL only formats, which do\r\nnot cater to RDF, but OWL, as an ontology is based on RDF.\r\n\r\n* **RDF/XML:** This is (sort-of) the default format for serialising RDF. Files usually end in `.rdf` though `.xml` extension would also be valid, but is not recommended. The content-type is `text/rdf+xml`.\r\n* **Turtle:** This is the easiest format to read and write in, and is quite concise. It's extenion is `.ttl` and the content-type is `text/turtle`.\r\n* **N-Triples:** This is a similar format to Turtle, though is arguably less concise. The extension is `.nt` and the content-type is `application/n_triples`.\r\n* **Notation 3:** Another concise format, it's extension is `.n3` and the content-type is `text/n3`.\r\n* **JSON-LD:** To make the propogation of RDF easier on the web, the JSON-LD format was developed. It uses a notation based on the JSON format with annotations catering to RDF keywords. It's extension is `.jsonld` and content-type is `application/ld+json`.\r\n* **RDFa:** This is a way to embed RDF information in HTML tags, while still allowing traditional page markup to be used and displayed. This is, in theory, the best of both worlds as the resulting document is presentable for human as well as machine consumption.", "headerimage": "", "highlight": "0", "section": 9}, {"id": 14, "title": "nested for loops", "authors": "1", "date_created": "2017-06-23 14:25:00", "date_published": "2017-06-23 14:25:00", "date_updated": "2017-07-08 07:36:58", "is_published": "1", "short_description": "resolve nested for loops by preventing repeated iterations", "tags": "159", "slug": "nested-for-loops", "body_type": "markdown", "body_text": "<p>I saw a post on Facebook by a colleague from my lab about the runtime of her program\nbeing quite high.. I thought this can\u2019t be right, imagining a large\namount of data being crunched copiously over and over. Thinking that this would offer\nan interesting problem (and a way to procrastinate from work), I offered to help her.\nThe next day we sat looking at the lines of code trying to identify what exactly was\ntaking that much time, and where could we do things better. Scrolling through the\nvarious files and classes and methods all made up of Java, my eyes fell on a nesting\nof for-loops that went on and on and on. Years of stereotyped-advice rang out in\nmy head as I recoiled looking at the indentation of each for-loop going on and over.\nI resisted the urge to point out that this is bad practice , thinking it would be\ncondescending on my part, and more importantly - not necessarily good advice.</p>\n<h2 id=\"understanding-for-loops\">Understanding <code>for</code> loops</h2>\n<p>For loops have a purpose - to iterate a block of code over and over until the given\ncondition no longer holds true. To that end, they are a fancy way of writing a while\nloop. Logically, they are equivalent -</p>\n<pre class=\"codehilite\"><code class=\"language-java\">for (int i=0 ; i&lt;100 ; i++) {\n    // do something\n}\n\n{ // if you are pedantic about block scope\nint i = 0;\nwhile(i &lt; 100) {\n    // do something\n}\n} // variable i will never leave this block</code></pre>\n\n\n<p>The for loop is meant to process things over and over, so a nested for loop can be\nused to iterate all possible permutations. For example,</p>\n<pre class=\"codehilite\"><code class=\"language-java\">for (int i=0 ; i&lt;m ; i++)\n    for (int j=0; j&lt;n ; j++)</code></pre>\n\n\n<p>is guaranteed to run <code>m x n</code> times, creating all possible pairs (combinations)\nof values of <code>i</code> and <code>j</code> . This is the easiest way to create such pairs.\nHowever, careful observations can be made for the sake of optimisation whether\nall such pairs are indeed required to be calculated.</p>\n<p>The nesting of for loops we had on the screen was <code>13</code> levels deep, with variable\nloop limits. That takes it to the order of <code>n^13</code> , which is pretty gigantic in theory.\nSo how do we optimise this? The approach lies in understanding the problem, rather\nthan speculating approaches based on abstract mathematical applications. </p>\n<h2 id=\"understanding-the-problem-background\">Understanding the problem background</h2>\n<p>The colleague who wrote the original piece of code was researching into the spread\nof diseases. To that end, she wanted to test the various combinations of environmental\nfactors and calculate a score for how infectious it is.\nThe problem the for-loops meant to solve were to combine all factors in every possible\nway - hence the nesting.\nThe key factor here is combination , and not permutation . The latter is where the\norder does not matter, therefore, it is fewer in number, and more efficient to calculate.</p>\n<p>Each environmental factor comes from a group which is distinct from the other groups,\nwhich are things like season and geographical location.\nIf we have <code>n</code> distinct groups each with <code>m</code> distinct items, then the total possible\nways of combining them becomes <code>n x n x n x ... (m times)</code> or <code>n ^ m</code>.\nThis is how we get a nesting of 13 for-loops. </p>\n<h2 id=\"memoization\">Memoization</h2>\n<p><a href=\"https://en.wikipedia.org/wiki/Memoization\">Memoization</a> is a technique where previous\ncomputations are stored to speed up processing for repetitive calculations.\nA good way to describe it would be to write this function which stores the results\nof previously computed calculation in a cache . When a pre-computed result is asked\nfor, it returns the value from the cache in constant time - O(1) . Pretty efficient.\nThe only downside is the <a href=\"https://en.wikipedia.org/wiki/Lookup_table\"><em>lookup cost</em></a>\nand the storage for the cache. Even this can be further optimized using\n<a href=\"https://en.wikipedia.org/wiki/Hash_function\"><em>hashing</em></a> .</p>\n<pre class=\"codehilite\"><code class=\"language-java\">int[] cache = new int[100];\n\n// calculate something for 0 &lt;= n &lt; 100\nint calculate_something(int n) {\n    if (cache[n] != 0) {\n        return cache[n];\n    }\n    int result = perform_calculations();\n    cache[n] = result;\n    return result;\n}</code></pre>\n\n\n<p>For our use case, we can use memoization to save pre-computed combinations.\nThis can be applied as saving the results of the inner loops so that they are\nnot iterated over and over again. </p>\n<h2 id=\"java-string-ops\">Java String-ops</h2>\n<p>In Java, <code>Strings</code> are <em>immutable</em> , which means that once assigned, a string is\nessentially a constant, and cannot be changed. Any operation that acts on the string\ncreates a new String object. In the given scenario, each environmental condition\nwas expressed as a String, and all loops were combining them through concatenation .\nWhich meant that there were new String objects being created over and over 13 levels down.</p>\n<p>Perhaps a better design would be to represent the environmental conditions as\n<em>enumerations</em> or some other constant factor which speeds up their combination\noperation. But given the scenario, let us assume that they <em>have</em> to be Strings.\nIn this case, the objective is to prevent the repetitive creation of new Strings.\nThe goal of each nested loop to add an item from a group to the final String.\nInstead, we make use of <code>List</code> to hold the results for us, as they are much better\nin terms of performance and efficiency. If desired, they can be turned into a String\nat the end.</p>\n<p>We require one loop to go over all of the environmental groups.\nThis will be the outer loop.\nWe then require another loop to go over the items in each group and add them to\na list of items. We store the results in a list of lists.\nEach list is a combination of items.</p>\n<p>The approach is pretty simple -</p>\n<ol>\n<li>For every list, add the current item to it</li>\n<li>Collect all lists and replace the result with this new list</li>\n</ol>\n<pre class=\"codehilite\"><code class=\"language-java\">// create a list of lists\nList&lt;List&lt;String&gt;&gt; results = new ArrayList&lt;ArrayList&lt;String&gt;&gt;();\n\n// put the items in the first group to populate the list\nfor (i=0 ; i&lt;group[0].length ; i++) {\n    // create a new list for each item\n    List&lt;String&gt; newlist = new ArrayList&lt;String&gt;();\n    // add the item to the list; it will be the sole member\n    newlist.add(group[0][i]);\n    // add this new list to the results grouplist\n    results.add(newlist);\n}\n\n// iterate over the rest of groups\nfor (i=1 ; i&lt;no_groups ; i++) {\n    // create an empty grouplist (list of lists)\n    // to preserve the original during processing\n    List&lt;List&lt;String&gt;&gt; newresults = new ArrayList&lt;ArrayList&lt;String&gt;&gt;();\n    // iterate over the items in group\n    for (j=0 ; j&lt;group[i].length ; j++) {\n        // iterate over the items in results\n        for (k=0 ; k&lt;results.length ; k++) {\n            // create a copy of the list, and add the current item to it\n            List&lt;String&gt; newlist = new ArrayList&lt;String&gt;(results[k]);\n            newlist.add(group[i][j]);\n            // add this list to the new group of results\n            newresults.add(newlist);\n    }\n    // replace the results with the new ones\n    results = newresults;\n}</code></pre>\n\n\n<p>The way this works is, after copying over items from the first group, the contents\nof results will be -</p>\n<pre class=\"codehilite\"><code>[[A1],[A2],[A3]...[An]]</code></pre>\n\n\n<p>Then, we iterate <code>m</code> times, once for each of the remaining groups (outer loop);\nand then for each item in the group, we create a new list by adding the current\nitem to every list in the results. Which gives -</p>\n<pre class=\"codehilite\"><code>[[A1,B1],...[AnB1],[A1,B2]...[An,Bn]]</code></pre>\n\n\n<p>The <em>cache</em> here is the stored result of every combination of the previous\niterations, and we only add the current items to each of them without repeating the\ncalculations. Additionally, there are no new String operations, therefore, no\nnew Strings are being created. However, we create a lot of lists . Turns out\nthat this is quite efficient because lists are better in terms of memory management\nthan Strings as they can be expanded with any free space on the heap whereas a String\nrequires continuous allocations.</p>\n<p>Though the number of combinations remain the same - <code>m x n</code>, the number of loops\nrequired to process them has reduced them from 13 to 3. The optimization trick\nhere is to avoid repeating the same computation over and over again by using\nmemoization . This reduced the runtime of this part from several minutes to\na few seconds . </p>\n<h2 id=\"more-optimizations\">More optimizations</h2>\n<p>The original program also removed combinations which were impossible - such as\na certain region having a certain season (which never happens). The way these\nwere done was by taking the impossible condition in a list, generating all possible\ncombinations of them and concatenating them as Strings. Then it checked whether\nthese occurred as substring in the combinations of results. Since we deal with lists\nhere, expressing these as lists of items that are impossible together makes sense.\nSo we store the impossible conditions together as a list, and check whether the item is\na sub-list of every list. If it is, then we remove that list as a violated condition.\nThis operation is not as efficient as calculating substrings because lists deal in\nobject references whereas Strings are allocated as continuous arrays and have byte\nchecking. But the offset achieved from not creating Strings in the original loop\nproblem is enough to make this a trivial delay in comparison. </p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Through the optimisations, the total runtime went down a bit,\nwhich I\u2019m sure can be reduced further down. What I learnt through this\nlittle exercise was the application of what concepts I learned through\ncollege, and then later while doing little projects. I tailored the solution\nbased on the problem, which is expected of an engineer - to solve real-world\nproblems through practical solutions. It is through various such problems\nthat collective experience is gained and one levels up in knowledge.</p>", "body": "I saw a post on Facebook by a colleague from my lab about the runtime of her program\r\nbeing quite high.. I thought this can\u2019t be right, imagining a large\r\namount of data being crunched copiously over and over. Thinking that this would offer\r\nan interesting problem (and a way to procrastinate from work), I offered to help her.\r\nThe next day we sat looking at the lines of code trying to identify what exactly was\r\ntaking that much time, and where could we do things better. Scrolling through the\r\nvarious files and classes and methods all made up of Java, my eyes fell on a nesting\r\nof for-loops that went on and on and on. Years of stereotyped-advice rang out in\r\nmy head as I recoiled looking at the indentation of each for-loop going on and over.\r\nI resisted the urge to point out that this is bad practice , thinking it would be\r\ncondescending on my part, and more importantly - not necessarily good advice.\r\n\r\n## Understanding `for` loops\r\n\r\nFor loops have a purpose - to iterate a block of code over and over until the given\r\ncondition no longer holds true. To that end, they are a fancy way of writing a while\r\nloop. Logically, they are equivalent -\r\n```java\r\nfor (int i=0 ; i<100 ; i++) {\r\n    // do something\r\n}\r\n\r\n{ // if you are pedantic about block scope\r\nint i = 0;\r\nwhile(i < 100) {\r\n    // do something\r\n}\r\n} // variable i will never leave this block\r\n```\r\nThe for loop is meant to process things over and over, so a nested for loop can be\r\nused to iterate all possible permutations. For example,\r\n```java\r\nfor (int i=0 ; i<m ; i++)\r\n    for (int j=0; j<n ; j++)\r\n```\r\nis guaranteed to run `m x n` times, creating all possible pairs (combinations)\r\nof values of `i` and `j` . This is the easiest way to create such pairs.\r\nHowever, careful observations can be made for the sake of optimisation whether\r\nall such pairs are indeed required to be calculated.\r\n\r\nThe nesting of for loops we had on the screen was `13` levels deep, with variable\r\nloop limits. That takes it to the order of `n^13` , which is pretty gigantic in theory.\r\nSo how do we optimise this? The approach lies in understanding the problem, rather\r\nthan speculating approaches based on abstract mathematical applications. \r\n\r\n## Understanding the problem background\r\n\r\nThe colleague who wrote the original piece of code was researching into the spread\r\nof diseases. To that end, she wanted to test the various combinations of environmental\r\nfactors and calculate a score for how infectious it is.\r\nThe problem the for-loops meant to solve were to combine all factors in every possible\r\nway - hence the nesting.\r\nThe key factor here is combination , and not permutation . The latter is where the\r\norder does not matter, therefore, it is fewer in number, and more efficient to calculate.\r\n\r\nEach environmental factor comes from a group which is distinct from the other groups,\r\nwhich are things like season and geographical location.\r\nIf we have `n` distinct groups each with `m` distinct items, then the total possible\r\nways of combining them becomes `n x n x n x ... (m times)` or `n ^ m`.\r\nThis is how we get a nesting of 13 for-loops. \r\n\r\n## Memoization\r\n\r\n[Memoization](https://en.wikipedia.org/wiki/Memoization) is a technique where previous\r\ncomputations are stored to speed up processing for repetitive calculations.\r\nA good way to describe it would be to write this function which stores the results\r\nof previously computed calculation in a cache . When a pre-computed result is asked\r\nfor, it returns the value from the cache in constant time - O(1) . Pretty efficient.\r\nThe only downside is the [_lookup cost_](https://en.wikipedia.org/wiki/Lookup_table)\r\nand the storage for the cache. Even this can be further optimized using\r\n[_hashing_](https://en.wikipedia.org/wiki/Hash_function) .\r\n```java\r\nint[] cache = new int[100];\r\n\r\n// calculate something for 0 <= n < 100\r\nint calculate_something(int n) {\r\n    if (cache[n] != 0) {\r\n        return cache[n];\r\n    }\r\n    int result = perform_calculations();\r\n    cache[n] = result;\r\n    return result;\r\n}\r\n```\r\nFor our use case, we can use memoization to save pre-computed combinations.\r\nThis can be applied as saving the results of the inner loops so that they are\r\nnot iterated over and over again. \r\n\r\n## Java String-ops\r\n\r\nIn Java, `Strings` are _immutable_ , which means that once assigned, a string is\r\nessentially a constant, and cannot be changed. Any operation that acts on the string\r\ncreates a new String object. In the given scenario, each environmental condition\r\nwas expressed as a String, and all loops were combining them through concatenation .\r\nWhich meant that there were new String objects being created over and over 13 levels down.\r\n\r\nPerhaps a better design would be to represent the environmental conditions as\r\n_enumerations_ or some other constant factor which speeds up their combination\r\noperation. But given the scenario, let us assume that they _have_ to be Strings.\r\nIn this case, the objective is to prevent the repetitive creation of new Strings.\r\nThe goal of each nested loop to add an item from a group to the final String.\r\nInstead, we make use of `List` to hold the results for us, as they are much better\r\nin terms of performance and efficiency. If desired, they can be turned into a String\r\nat the end.\r\n\r\nWe require one loop to go over all of the environmental groups.\r\nThis will be the outer loop.\r\nWe then require another loop to go over the items in each group and add them to\r\na list of items. We store the results in a list of lists.\r\nEach list is a combination of items.\r\n\r\nThe approach is pretty simple -\r\n\r\n1. For every list, add the current item to it\r\n2. Collect all lists and replace the result with this new list\r\n\r\n```java\r\n// create a list of lists\r\nList<List<String>> results = new ArrayList<ArrayList<String>>();\r\n\r\n// put the items in the first group to populate the list\r\nfor (i=0 ; i<group[0].length ; i++) {\r\n    // create a new list for each item\r\n    List<String> newlist = new ArrayList<String>();\r\n    // add the item to the list; it will be the sole member\r\n    newlist.add(group[0][i]);\r\n    // add this new list to the results grouplist\r\n    results.add(newlist);\r\n}\r\n\r\n// iterate over the rest of groups\r\nfor (i=1 ; i<no_groups ; i++) {\r\n    // create an empty grouplist (list of lists)\r\n    // to preserve the original during processing\r\n    List<List<String>> newresults = new ArrayList<ArrayList<String>>();\r\n    // iterate over the items in group\r\n    for (j=0 ; j<group[i].length ; j++) {\r\n        // iterate over the items in results\r\n        for (k=0 ; k<results.length ; k++) {\r\n            // create a copy of the list, and add the current item to it\r\n            List<String> newlist = new ArrayList<String>(results[k]);\r\n            newlist.add(group[i][j]);\r\n            // add this list to the new group of results\r\n            newresults.add(newlist);\r\n    }\r\n    // replace the results with the new ones\r\n    results = newresults;\r\n}\r\n```\r\n\r\nThe way this works is, after copying over items from the first group, the contents\r\nof results will be -\r\n```\r\n[[A1],[A2],[A3]...[An]]\r\n```\r\nThen, we iterate `m` times, once for each of the remaining groups (outer loop);\r\nand then for each item in the group, we create a new list by adding the current\r\nitem to every list in the results. Which gives -\r\n```\r\n[[A1,B1],...[AnB1],[A1,B2]...[An,Bn]]\r\n```\r\nThe _cache_ here is the stored result of every combination of the previous\r\niterations, and we only add the current items to each of them without repeating the\r\ncalculations. Additionally, there are no new String operations, therefore, no\r\nnew Strings are being created. However, we create a lot of lists . Turns out\r\nthat this is quite efficient because lists are better in terms of memory management\r\nthan Strings as they can be expanded with any free space on the heap whereas a String\r\nrequires continuous allocations.\r\n\r\nThough the number of combinations remain the same - `m x n`, the number of loops\r\nrequired to process them has reduced them from 13 to 3. The optimization trick\r\nhere is to avoid repeating the same computation over and over again by using\r\nmemoization . This reduced the runtime of this part from several minutes to\r\na few seconds . \r\n\r\n## More optimizations\r\n\r\nThe original program also removed combinations which were impossible - such as\r\na certain region having a certain season (which never happens). The way these\r\nwere done was by taking the impossible condition in a list, generating all possible\r\ncombinations of them and concatenating them as Strings. Then it checked whether\r\nthese occurred as substring in the combinations of results. Since we deal with lists\r\nhere, expressing these as lists of items that are impossible together makes sense.\r\nSo we store the impossible conditions together as a list, and check whether the item is\r\na sub-list of every list. If it is, then we remove that list as a violated condition.\r\nThis operation is not as efficient as calculating substrings because lists deal in\r\nobject references whereas Strings are allocated as continuous arrays and have byte\r\nchecking. But the offset achieved from not creating Strings in the original loop\r\nproblem is enough to make this a trivial delay in comparison. \r\n\r\n## Conclusion\r\n\r\nThrough the optimisations, the total runtime went down a bit,\r\nwhich I\u2019m sure can be reduced further down. What I learnt through this\r\nlittle exercise was the application of what concepts I learned through\r\ncollege, and then later while doing little projects. I tailored the solution\r\nbased on the problem, which is expected of an engineer - to solve real-world\r\nproblems through practical solutions. It is through various such problems\r\nthat collective experience is gained and one levels up in knowledge.", "headerimage": "", "highlight": "0", "section": 7}, {"id": 15, "title": "Notification when printing *actually* completes", "authors": "1", "date_created": "2017-07-07 22:12:34", "date_published": "2017-07-07 22:12:38", "date_updated": "2017-07-07 22:22:52", "is_published": "1", "short_description": "a script to notify when the printer actually finishes the printing job", "tags": "11,135,136", "slug": "notification-when-printing-actually-completes", "body_type": "markdown", "body_text": "<p>My desk in the lab (or office) is at one end, near the door. The nearest printer is at the end other end of the room, about 10 meters away. Whenever I print something, the printer, one of those big laser ones, emits some typical sounds of printing. I have to wait until those sounds stop to determine my print job has completed. Sometimes, there are several people printing at once, and it becomes impossible to tell when my job in particular has completed printing. The status on my machine is not entirely accurate, because it only tells when a print job has been <em>accepted</em> by the printer, not when it actually finishes printing. Most people give their print order and simply walk to the printer after 5-10 minutes to pick up their printouts. But I wanted a more <em>elegant</em> solution, something befitting my training as a researcher and an engineer. So I set out to write a script that can notify me when the printing finishes on the printer.</p>\n<p>The inspiration for this came from one of those stories on the internet. This one went something like this - guy in office wrote a lot of scripts to automate things like messaging his manager when he was late based on whether he was logged into the office computer, setting up the coffee machine to make the brew in the exact time he took to walk over, and several others. </p>\n<p>One of the first challenges I faced was to determine what job was printing on the printer. Since the printer is always connected to the network, I looked for a way to query it. Laser printers have a network interface accessible at their IP address. This is an online webpage hosted <em>by</em> the printer (yes, the printer is acting as a server) that details status information, admin configurations, and also shows the current printing status. The printer in our office is a Lexmark T650, whose status webpage does not show <em>which</em> job is printing, nor is there a log of all print jobs. </p>\n<p>There is a protocol called <a href=\"https://en.wikipedia.org/wiki/Simple_Network_Management_Protocol\">Simple Network Management Protocol (SNMP)</a> for managed devices on the network such as routers, modems, servers, and importantly - printers. The details of what mechanisms are available for querying over the protocol are detailed in a <a href=\"https://en.wikipedia.org/wiki/Management_information_base\">Management Information Base (MIB)</a>. Often times, printer manufacturers use private MIBs to detail query string that their printers interact with. I got to know about this protocol thanks to <a href=\"https://www.reddit.com/user/cocoabean\">/u/cocoabean</a>.</p>\n<p>The Lexmark MIB has a section called <code>opsys</code> that provides various operational status information on the printer. In particular, the MIB of interest is <code>opsysCurrentJob</code> with OID <code>.1.3.6.1.4.1.641.1.1.3</code> which is defined as - </p>\n<blockquote>\n<p><em>\"A textual description of the currently printing job containing the Source NOS, Source server, Source user, Job number, and Job size, separated by CR LF.  A NULL string indicates no active job.\"</em></p>\n</blockquote>\n<p>The format of the response is of the form -</p>\n<pre class=\"codehilite\"><code>STRING: \n&quot;TCP/IP 134.226.63.214,42524\nPort 9100\n357\nUnknown&quot;</code></pre>\n\n\n<p>The IP address part of the response is the address of the machine that <em>sent</em> that job. So it will be my IP address if my job is being printed. The way to detect when my job has finished printing is to detect the context change of the current job no longer having my IP address.</p>\n<p>To query the printer, I used the tool <code>snmpwalk</code>  which allows querying printers using the SNMP protocol. It can be used as -</p>\n<pre class=\"codehilite\"><code>snmpwalk -v 2c -c public &lt;PRINTER-IP&gt; &lt;MIB-OID&gt;</code></pre>\n\n\n<p>The script needs a way to determine what the last job status was - if it was printing my job or not. One alternative is to keep the script running in the background at all times, thereby keeping the variables in memory, but this solution is not elegant as it leaves a process running. Another option is to store the status of the previous job somewhere. I chose <code>/tmp</code> as it is a temporary storage that gets cleaned up automatically, but never in the middle of usage.</p>\n<p>Getting the previous status, whether the printer is currently printing, and whether the print job has my IP, the following states are possible:</p>\n<pre>\n| Previous state | Currently printing | Printing my job |   Action   |\n| :------------: | :----------------: | :-------------: | :--------: |\n|       Y        |         Y          |        Y        |     --     |\n|       N        |         Y          |        Y        |     --     |\n|     **Y**      |       **Y**        |      **N**      | **notify** |\n|       N        |         Y          |        N        |     --     |\n|     **Y**      |       **N**        |      **Y**      | **notify** |\n|       N        |         N          |        Y        |     --     |\n|     **Y**      |       **N**        |      **N**      | **notify** |\n|       N        |         N          |        N        |     --     |\n</pre>\n\n<p>Summarising all conditions in which the script should notify -</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">if  [ $previous_state == true ] &amp;&amp; \\\n    [ $currently_printing == false || $printing_myjob == false ];\nthen\n    notify &quot;print complete&quot;\nfi</code></pre>\n\n\n<p>The overall algorithm goes something like this -</p>\n<ol>\n<li>Retrieve my IP address`</li>\n<li>Retrieve current printer job string</li>\n<li>If it is empty, then set <code>currently_printing</code> and <code>printing_myjob</code> to <code>false</code></li>\n<li>Else<ol>\n<li>set <code>currently_printing</code> to <code>true</code></li>\n<li>If  job IP is same as my IP, set <code>printing_myjob</code> to <code>true</code></li>\n<li>Else set <code>printing_myjob</code> to <code>false</code></li>\n</ol>\n</li>\n<li>Read previous state from file <code>/tmp/printerpreviousstate</code> into <code>previous_state</code></li>\n<li>If notification condition is satisfied, generate notification</li>\n</ol>\n<p>To notify, I used <code>zenity</code>, although several alternatives exist. Another option that uses the system notification panel (if it exists) is <code>notify-send</code>. Both work fine and which one to choose is a matter of preference.</p>\n<p>I put the script up as a <code>cron</code> job running every 5 seconds. While cron runs jobs every minute, I created multiple entries with delays using <code>sleep</code> -</p>\n<pre class=\"codehilite\"><code>* * * * * ~/bin/printjob_status.sh                                              \n* * * * * sleep 5; ~/bin/printjob_status.sh                                     \n* * * * * sleep 10; ~/bin/printjob_status.sh                                    \n* * * * * sleep 15; ~/bin/printjob_status.sh                                    \n* * * * * sleep 20; ~/bin/printjob_status.sh                                    \n* * * * * sleep 25; ~/bin/printjob_status.sh</code></pre>\n\n\n<p>I ran into some issues with displaying notifications from inside a cron script. To get around that, I had to <code>export</code> a variable called <code>DBUS_SESSION_BUS_ADDRESS</code> and target the notification explicitly to a display using <code>DISPLAY=:0</code>. The reasons for this elude me.</p>\n<p>In the end, I managed to get the script running, and to how a notification informing me that my printing job was complete. For now, I'm happy, though I'm sure I will probably tweak it some more in the future.</p>", "body": "My desk in the lab (or office) is at one end, near the door. The nearest printer is at the end other end of the room, about 10 meters away. Whenever I print something, the printer, one of those big laser ones, emits some typical sounds of printing. I have to wait until those sounds stop to determine my print job has completed. Sometimes, there are several people printing at once, and it becomes impossible to tell when my job in particular has completed printing. The status on my machine is not entirely accurate, because it only tells when a print job has been *accepted* by the printer, not when it actually finishes printing. Most people give their print order and simply walk to the printer after 5-10 minutes to pick up their printouts. But I wanted a more *elegant* solution, something befitting my training as a researcher and an engineer. So I set out to write a script that can notify me when the printing finishes on the printer.\r\n\r\nThe inspiration for this came from one of those stories on the internet. This one went something like this - guy in office wrote a lot of scripts to automate things like messaging his manager when he was late based on whether he was logged into the office computer, setting up the coffee machine to make the brew in the exact time he took to walk over, and several others. \r\n\r\nOne of the first challenges I faced was to determine what job was printing on the printer. Since the printer is always connected to the network, I looked for a way to query it. Laser printers have a network interface accessible at their IP address. This is an online webpage hosted *by* the printer (yes, the printer is acting as a server) that details status information, admin configurations, and also shows the current printing status. The printer in our office is a Lexmark T650, whose status webpage does not show *which* job is printing, nor is there a log of all print jobs. \r\n\r\nThere is a protocol called [Simple Network Management Protocol (SNMP)](https://en.wikipedia.org/wiki/Simple_Network_Management_Protocol) for managed devices on the network such as routers, modems, servers, and importantly - printers. The details of what mechanisms are available for querying over the protocol are detailed in a [Management Information Base (MIB)](https://en.wikipedia.org/wiki/Management_information_base). Often times, printer manufacturers use private MIBs to detail query string that their printers interact with. I got to know about this protocol thanks to [/u/cocoabean](https://www.reddit.com/user/cocoabean).\r\n\r\nThe Lexmark MIB has a section called `opsys` that provides various operational status information on the printer. In particular, the MIB of interest is `opsysCurrentJob` with OID `.1.3.6.1.4.1.641.1.1.3` which is defined as - \r\n\r\n> *\"A textual description of the currently printing job containing the Source NOS, Source server, Source user, Job number, and Job size, separated by CR LF.  A NULL string indicates no active job.\"*\r\n\r\nThe format of the response is of the form -\r\n\r\n```\r\nSTRING: \r\n\"TCP/IP 134.226.63.214,42524\r\nPort 9100\r\n357\r\nUnknown\"\r\n```\r\n\r\nThe IP address part of the response is the address of the machine that *sent* that job. So it will be my IP address if my job is being printed. The way to detect when my job has finished printing is to detect the context change of the current job no longer having my IP address.\r\n\r\nTo query the printer, I used the tool `snmpwalk`  which allows querying printers using the SNMP protocol. It can be used as -\r\n\r\n```\r\nsnmpwalk -v 2c -c public <PRINTER-IP> <MIB-OID>\r\n```\r\n\r\nThe script needs a way to determine what the last job status was - if it was printing my job or not. One alternative is to keep the script running in the background at all times, thereby keeping the variables in memory, but this solution is not elegant as it leaves a process running. Another option is to store the status of the previous job somewhere. I chose `/tmp` as it is a temporary storage that gets cleaned up automatically, but never in the middle of usage.\r\n\r\nGetting the previous status, whether the printer is currently printing, and whether the print job has my IP, the following states are possible:\r\n\r\n<pre>\r\n| Previous state | Currently printing | Printing my job |   Action   |\r\n| :------------: | :----------------: | :-------------: | :--------: |\r\n|       Y        |         Y          |        Y        |     --     |\r\n|       N        |         Y          |        Y        |     --     |\r\n|     **Y**      |       **Y**        |      **N**      | **notify** |\r\n|       N        |         Y          |        N        |     --     |\r\n|     **Y**      |       **N**        |      **Y**      | **notify** |\r\n|       N        |         N          |        Y        |     --     |\r\n|     **Y**      |       **N**        |      **N**      | **notify** |\r\n|       N        |         N          |        N        |     --     |\r\n</pre>\r\n\r\nSummarising all conditions in which the script should notify -\r\n\r\n```bash\r\nif\t[ $previous_state == true ] && \\\r\n\t[ $currently_printing == false || $printing_myjob == false ];\r\nthen\r\n\tnotify \"print complete\"\r\nfi\r\n```\r\n\r\nThe overall algorithm goes something like this -\r\n\r\n1.  Retrieve my IP address`\r\n2.  Retrieve current printer job string\r\n   1. If it is empty, then set `currently_printing` and `printing_myjob` to `false`\r\n   2. Else\r\n      1. set `currently_printing` to `true`\r\n      2. If  job IP is same as my IP, set `printing_myjob` to `true`\r\n      3. Else set `printing_myjob` to `false`\r\n3.  Read previous state from file `/tmp/printerpreviousstate` into `previous_state`\r\n4.  If notification condition is satisfied, generate notification\r\n\r\nTo notify, I used `zenity`, although several alternatives exist. Another option that uses the system notification panel (if it exists) is `notify-send`. Both work fine and which one to choose is a matter of preference.\r\n\r\nI put the script up as a `cron` job running every 5 seconds. While cron runs jobs every minute, I created multiple entries with delays using `sleep` -\r\n\r\n```\r\n* * * * * ~/bin/printjob_status.sh                                              \r\n* * * * * sleep 5; ~/bin/printjob_status.sh                                     \r\n* * * * * sleep 10; ~/bin/printjob_status.sh                                    \r\n* * * * * sleep 15; ~/bin/printjob_status.sh                                    \r\n* * * * * sleep 20; ~/bin/printjob_status.sh                                    \r\n* * * * * sleep 25; ~/bin/printjob_status.sh\r\n```\r\n\r\nI ran into some issues with displaying notifications from inside a cron script. To get around that, I had to `export` a variable called `DBUS_SESSION_BUS_ADDRESS` and target the notification explicitly to a display using `DISPLAY=:0`. The reasons for this elude me.\r\n\r\nIn the end, I managed to get the script running, and to how a notification informing me that my printing job was complete. For now, I'm happy, though I'm sure I will probably tweak it some more in the future.", "headerimage": "https://s3-eu-west-1.amazonaws.com/harshp-media/dev/printjob_status.png", "highlight": "0", "section": 8}, {"id": 13, "title": "snapshot-20170610", "authors": "1", "date_created": "2017-06-10 19:48:00", "date_published": "2017-06-10 19:48:00", "date_updated": "2017-06-10 20:11:23", "is_published": "1", "short_description": "Details of all my devices", "tags": "155,157,145,146,147,148,144,156,150,152,153,149,151,154", "slug": "snapshot-20170610", "body_type": "markdown", "body_text": "<p>As this is the first post, I must explain what exactly this is all about.\nToolset snapshots is a series where I record all the tools I work with\nand try to document the reasons why I chose them. It is intended more\nto be a developer journal rather than a discussion of why or what I chose.\nIf in the future, I start working on a tool, I intend to write more about\nit in another section. This one here, is more of a listing of what I work\nwith.</p>\n<p>I use the following three machines - </p>\n<ul>\n<li>Dell XPS 13 (2017) - <em>XNMPRO</em></li>\n<li>Macbook Air 13 (2013) - <em>XNMAIR</em></li>\n<li>Custom PC (2017) - XNMRZN</li>\n</ul>\n<p>Apart from these, I own -</p>\n<ul>\n<li>OnePlus 3T smartphone - <em>XNMOP3</em></li>\n<li>iPad Air 2 - <em>XNMIPA</em></li>\n<li>Pebble smartwatch - <em>XNMPBL</em></li>\n</ul>\n<p>I have a tendency to name all of my devices with the prefix <code>XNM</code>.\nThe reasons go back to a considerable number of years, quite possibly a\ndecade, and revolve around my fascination with the number <code>86</code> and the\nelement <a href=\"https://en.wikipedia.org/wiki/Xenon\">Xenon</a>. I'll have to \nwrite another post with the reasons I went with it, if I can remember them.</p>\n<h2 id=\"xnmpro-xnmair\">XNMPRO / XNMAIR</h2>\n<blockquote>\n<p><a href=\"https://github.com/coolharsh55/dotfiles\">dotfiles</a></p>\n</blockquote>\n<ul>\n<li>operating system: Xubuntu (Ubuntu --&gt; Debian)</li>\n<li>window manager: i3</li>\n<li>compositor: compton</li>\n<li>file-manager: Thunar</li>\n<li>terminal: xfce4-term</li>\n<li>browser: firefox (primary), google chrome</li>\n<li>screen-lock: xautolock</li>\n<li>applets: xfce - network manager, power manager</li>\n<li>editor: vim 8</li>\n<li>shell: bash</li>\n<li>backup software: borg</li>\n<li>notes: zim</li>\n<li>tmux (terminal multiplexer)</li>\n<li>clipboard: clipit</li>\n<li>launcher: dmenu</li>\n<li>text-expander: texpander (custom script)</li>\n<li>office suite: libre office</li>\n<li>file sync: dropbox, google drive (insync)</li>\n<li>music playback: spotify</li>\n<li>games: steam</li>\n</ul>\n<h2 id=\"xnmrzn\">XNMRZN</h2>\n<ul>\n<li>operating system: Windows 10 Pro</li>\n<li>browser: firefox (primary), google chrome</li>\n<li>music: spotify</li>\n<li>file sync: dropbox, google drive</li>\n</ul>\n<h2 id=\"xnmop3\">XNMOP3</h2>\n<ul>\n<li>operating system: OxygenOS 4 (Android 7)</li>\n<li>launcher: stock</li>\n<li>camera: stock</li>\n<li>music: spotify</li>\n<li>messenger:<ul>\n<li>stock messages</li>\n<li>whatsapp</li>\n<li>facebook messenger</li>\n<li>hangouts</li>\n</ul>\n</li>\n<li>dialer: stock</li>\n<li>gallery: stock, google photos</li>\n<li>email: inbox + gmail</li>\n<li>notes: google keep</li>\n<li>calendar: google calendar</li>\n<li>browser: firefox (primary), google chrome</li>\n</ul>\n<h2 id=\"xnmipa\">XNMIPA</h2>\n<ul>\n<li>operating system: iOS 10</li>\n<li>music: spotify</li>\n<li>messenger:<ul>\n<li>facebook messenger</li>\n<li>hangouts</li>\n</ul>\n</li>\n<li>photos: google photos</li>\n<li>email: inbox + gmail</li>\n<li>notes: google keep</li>\n<li>calendar: stock</li>\n<li>browser: google chrome</li>\n</ul>", "body": "As this is the first post, I must explain what exactly this is all about.\r\nToolset snapshots is a series where I record all the tools I work with\r\nand try to document the reasons why I chose them. It is intended more\r\nto be a developer journal rather than a discussion of why or what I chose.\r\nIf in the future, I start working on a tool, I intend to write more about\r\nit in another section. This one here, is more of a listing of what I work\r\nwith.\r\n\r\nI use the following three machines - \r\n\r\n* Dell XPS 13 (2017) - *XNMPRO*\r\n* Macbook Air 13 (2013) - *XNMAIR*\r\n* Custom PC (2017) - XNMRZN\r\n\r\nApart from these, I own -\r\n\r\n* OnePlus 3T smartphone - *XNMOP3*\r\n* iPad Air 2 - *XNMIPA*\r\n* Pebble smartwatch - *XNMPBL*\r\n\r\nI have a tendency to name all of my devices with the prefix `XNM`.\r\nThe reasons go back to a considerable number of years, quite possibly a\r\ndecade, and revolve around my fascination with the number `86` and the\r\nelement [Xenon](https://en.wikipedia.org/wiki/Xenon). I'll have to \r\nwrite another post with the reasons I went with it, if I can remember them.\r\n\r\n\r\n\r\n## XNMPRO / XNMAIR\r\n\r\n> [dotfiles](https://github.com/coolharsh55/dotfiles)\r\n\r\n* operating system: Xubuntu (Ubuntu --> Debian)\r\n* window manager: i3\r\n* compositor: compton\r\n* file-manager: Thunar\r\n* terminal: xfce4-term\r\n* browser: firefox (primary), google chrome\r\n* screen-lock: xautolock\r\n* applets: xfce - network manager, power manager\r\n* editor: vim 8\r\n* shell: bash\r\n* backup software: borg\r\n* notes: zim\r\n* tmux (terminal multiplexer)\r\n* clipboard: clipit\r\n* launcher: dmenu\r\n* text-expander: texpander (custom script)\r\n* office suite: libre office\r\n* file sync: dropbox, google drive (insync)\r\n* music playback: spotify\r\n* games: steam\r\n\r\n\r\n\r\n## XNMRZN\r\n\r\n* operating system: Windows 10 Pro\r\n* browser: firefox (primary), google chrome\r\n* music: spotify\r\n* file sync: dropbox, google drive\r\n\r\n\r\n\r\n## XNMOP3\r\n\r\n* operating system: OxygenOS 4 (Android 7)\r\n* launcher: stock\r\n* camera: stock\r\n* music: spotify\r\n* messenger:\r\n    * stock messages\r\n    * whatsapp\r\n    * facebook messenger\r\n    * hangouts\r\n* dialer: stock\r\n* gallery: stock, google photos\r\n* email: inbox + gmail\r\n* notes: google keep\r\n* calendar: google calendar\r\n* browser: firefox (primary), google chrome\r\n\r\n\r\n\r\n## XNMIPA\r\n\r\n* operating system: iOS 10\r\n* music: spotify\r\n* messenger:\r\n    * facebook messenger\r\n    * hangouts\r\n* photos: google photos\r\n* email: inbox + gmail\r\n* notes: google keep\r\n* calendar: stock\r\n* browser: google chrome", "headerimage": "", "highlight": "0", "section": 6}, {"id": 12, "title": "Vim config", "authors": "1", "date_created": "2017-06-04 05:23:00", "date_published": "2017-06-04 05:23:00", "date_updated": "2017-06-04 05:30:37", "is_published": "1", "short_description": "A description of vim config options and plugins", "tags": "143", "slug": "vim-config", "body_type": "markdown", "body_text": "<blockquote>\n<p><strong>dotfiles</strong> available <a href=\"https://github.com/coolharsh55/dotfiles\">here</a></p>\n</blockquote>\n<p><a href=\"http://www.vim.org/\">Vim</a> is important to know because of the position\nit enjoys of being pre-installed on (nearly all) servers. For quick text\nediting, there's nothing better than it. Plus, it doesn't take up a lot\nof memory or CPU time, works well and consistently out of the box, and is\nvery <em>very</em> powerful. </p>\n<p>In this post, I'll detail my vim configuration that has been gathered up over\nthe years.</p>\n<h2 id=\"colorscheme\">Colorscheme</h2>\n<p>Vim contains several pre-installed colorschemes. However, I like to use\n<a href=\"https://github.com/altercation/vim-colors-solarized\">Solarized</a> with\nthe background set to dark. This matches my terminal colors, and allows\nme to work well into the dark night. Sometimes during the day, I use\nthe light variant as it has a better contrast against the hard sunlight.</p>\n<pre class=\"codehilite\"><code>set background=dark\ncolorscheme solarized</code></pre>\n\n\n<h2 id=\"formatting\">Formatting</h2>\n<h3 id=\"whitespace\">Whitespace</h3>\n<p>Whitespace formatting is important because literring a file with a mix\nof tabs and spaces is a cardinal sin. I like to use a consisting tab\nof 4 spaces across all editors I use. Vim can automatically expand tabs\nto spaces, which is great.</p>\n<pre class=\"codehilite\"><code>set tabstop=4\nset softtabstop=4\nset expandtab\nset shiftwidth=4</code></pre>\n\n\n<h3 id=\"rulers-and-margins\">rulers and margins</h3>\n<p>Vim can show a visual line at a certain column to indicate text spilling\nover. It is also possible to automatically (hard or soft) wrap lines\nat that particular column. Usually, this is at 80 or 120 characters wide.\nI usually like to keep it to 80 columns.</p>\n<pre class=\"codehilite\"><code>set colorcolumn=80\nset ruler</code></pre>\n\n\n<h3 id=\"folding\">folding</h3>\n<p>Folding means hiding certain text - say a function or a class and then\ndisplaying the minimum required information such as class name or \nfunction definition. In vim, it is possible to restrict folds at certain\nlevels or to set how the folds are calculated. Since I mostly work in\npython, I've set up the folding based on indent. I've also remapped\nfolding to the space key to make it super-easy to open and close folds.</p>\n<pre class=\"codehilite\"><code>set foldmethod=indent\nset foldlevel=99\n&quot; enable folding with spacebar\nnoremap &lt;space&gt; za</code></pre>\n\n\n<h3 id=\"filetype\">Filetype</h3>\n<p>Vim can configure its settings according to the file type. Since I mostly\nwork with Python, which has indent-based syntax, vim knows to work with\nthe languge blocks in terms of <em>indents</em>.</p>\n<pre class=\"codehilite\"><code>set autoindent\nset fileformat=unix\nsyntax enable\nlet python_highlight_all=1\nfiletype indent on\nset encoding=utf-8\nset autochdir   &quot; Change working directory to open buffer\nset nocompatible\nfiletype off</code></pre>\n\n\n<h2 id=\"line-numbers\">Line numbers</h2>\n<p>I cannot emphasise the importance of line numbers enough. Often when I'm \nhelping students in the lab debug their programs, the compiler is kind enough\nto mention the line number the error occurs at, and the student does not\nnotice it because they do not have line numbers turned on.</p>\n<p><strong>ALWAYS TURN LINE NUMBERS ON</strong></p>\n<p>They not only help in debugging, but are also a great way to quickly point out\na specific line to someone.</p>\n<pre class=\"codehilite\"><code>set number</code></pre>\n\n\n<h3 id=\"relative-line-numbers\">Relative line numbers</h3>\n<p>Recently, I found a nifty thing vim is (now) capable of doing - showing relative\nline numbers. Plain old simple line numbers begin numbering the first line from 1\nand then 2 and so on to the end of file. Relative line numbers are relative to the\nline the cursor is at. So the current line is <code>line 0</code> and the one above it and\none below it are <code>line 1</code> and so on. What this does in vim, is it makes motion\nsuper easy to go to. In large files, where lines are in hundreds, going above to\na particular function on screen means typing out the entire line number. With\nrelative line number, one only needs to type out the <em>relative</em> number with the\nmotion, such as <code>5j</code> to go 5 lines below the current one. Combined with the\nnormal line number option, it shows the actual line number on the current line,\nand relative line numbers on all other lines.</p>\n<pre class=\"codehilite\"><code>set relativenumber</code></pre>\n\n\n<h2 id=\"other-options\">Other options</h2>\n<h3 id=\"spellcheck\">Spellcheck</h3>\n<p>Vim can highlight spelling errors in various languages (depending on the language\ninstalled and available to use).</p>\n<pre class=\"codehilite\"><code>set spell</code></pre>\n\n\n<h3 id=\"search-and-highlight-matches\">Search and highlight matches</h3>\n<p>In vim, searches can be highlighted as they are typed. This makes it easy to\nsee what the matches are, and is a behviour consistent with other editors.</p>\n<pre class=\"codehilite\"><code>set showmatch\nset incsearch\nset hlsearch</code></pre>\n\n\n<h2 id=\"plugins\">Plugins</h2>\n<h3 id=\"solarized\">Solarized</h3>\n<p>Solarized comes as a handy plugin, available at \n<a href=\"https://github.com/altercation/vim-colors-solarized\">Github</a>.</p>\n<h3 id=\"ale\">Ale</h3>\n<p><a href=\"https://github.com/w0rp/ale\">Ale</a> is an asynchronous linting engine,\nwhich helps catch linting errors by showing them visually in vim. It is quite\nfast, lightweight, and works with most of the popular languages.</p>\n<h3 id=\"airline\">Airline</h3>\n<p><a href=\"https://github.com/vim-airline/vim-airline\">Airline</a> is a statusbar plugin\nthat adds colors to the tagbar (at bottom). It enables a visual mode to distinguish\nbetween normal, replace, visual, and insert mode. Additional themes are \navailable at <a href=\"https://github.com/vim-airline/vim-airline-themes\">Airline Themes</a>.</p>\n<h3 id=\"ctrlp\">CtrlP</h3>\n<p><a href=\"https://github.com/kien/ctrlp.vim\">CtrlP</a> allows opening files quickly using\n<code>Ctrl+P</code> through character matching in the filename. For e.g. to open a file\nat path <code>doc/abc/xyz.txt</code>, I can type <code>daxyz</code> and the file will come up\nin the listing. It is a fast and easy way to open files or buffers.</p>\n<h3 id=\"fugitive\">Fugitive</h3>\n<p><a href=\"https://github.com/tpope/vim-fugitive\">Fugitive</a> is a git wrapper that allows\nexecuting git commands over the currently opened file. It is very extensive,\nand allows a plethora of git commands. Plus, it is authored by Tim Pope.</p>\n<h3 id=\"surround\">Surround</h3>\n<p><a href=\"https://github.com/tpope/vim-surround\">Surround</a> allows manipulation of\nsurrounding brackets, quotes, tags, pretty much everything. </p>\n<h3 id=\"tagbar\">Tagbar</h3>\n<p><a href=\"https://majutsushi.github.io/tagbar/\">Tagbar</a> displays a sidebar consisting \nof CTags found in the file. Requires exuberant ctags. Selecting a tag\nallows quick movement to wherever it is defined in the file. For easier\nmanipulation, map the sidebar to show/hide using <code>F8</code>.</p>\n<h3 id=\"commentary\">Commentary</h3>\n<p><a href=\"https://github.com/tpope/vim-commentary\">Commentary</a> allows \nquick comment/uncomment using <code>gc</code>. Can be used with motion, making\nit possible to quickly comment an entire paragraph or visual selection.</p>\n<h3 id=\"multiple-cursors\">Multiple cursors</h3>\n<p><a href=\"https://github.com/terryma/vim-multiple-cursors\">Multiple Cursors</a> allows\ninserting multiple cursors just like Sublime Text, though it is not as\nsimple or as elegant to do it in Vim.</p>\n<h3 id=\"easymotion\">EasyMotion</h3>\n<p><a href=\"https://github.com/easymotion/vim-easymotion\">EasyMotion</a> allows quickly moving\nahead by word or through the next few lines using a single-letter motion.\nIt is surprisingly simple to use and makes short motion easy.</p>\n<h3 id=\"signature\">Signature</h3>\n<p><a href=\"https://github.com/kshenoy/vim-signature\">Signature</a> allows interacting\nwith vim marks, which are like bookmarks in a file. It allows alphabetical\ntraversal, removing marks, and all sorts of things which make navigating\nback and forth through files a little easier.</p>", "body": "> **dotfiles** available [here](https://github.com/coolharsh55/dotfiles)\r\n\r\n[Vim](http://www.vim.org/) is important to know because of the position\r\nit enjoys of being pre-installed on (nearly all) servers. For quick text\r\nediting, there's nothing better than it. Plus, it doesn't take up a lot\r\nof memory or CPU time, works well and consistently out of the box, and is\r\nvery *very* powerful. \r\n\r\nIn this post, I'll detail my vim configuration that has been gathered up over\r\nthe years.\r\n\r\n## Colorscheme\r\n\r\nVim contains several pre-installed colorschemes. However, I like to use\r\n[Solarized](https://github.com/altercation/vim-colors-solarized) with\r\nthe background set to dark. This matches my terminal colors, and allows\r\nme to work well into the dark night. Sometimes during the day, I use\r\nthe light variant as it has a better contrast against the hard sunlight.\r\n\r\n```\r\nset background=dark\r\ncolorscheme solarized\r\n```\r\n\r\n## Formatting\r\n\r\n### Whitespace\r\nWhitespace formatting is important because literring a file with a mix\r\nof tabs and spaces is a cardinal sin. I like to use a consisting tab\r\nof 4 spaces across all editors I use. Vim can automatically expand tabs\r\nto spaces, which is great.\r\n\r\n```\r\nset tabstop=4\r\nset softtabstop=4\r\nset expandtab\r\nset shiftwidth=4\r\n\r\n```\r\n\r\n### rulers and margins\r\nVim can show a visual line at a certain column to indicate text spilling\r\nover. It is also possible to automatically (hard or soft) wrap lines\r\nat that particular column. Usually, this is at 80 or 120 characters wide.\r\nI usually like to keep it to 80 columns.\r\n\r\n```\r\nset colorcolumn=80\r\nset ruler\r\n```\r\n\r\n### folding\r\nFolding means hiding certain text - say a function or a class and then\r\ndisplaying the minimum required information such as class name or \r\nfunction definition. In vim, it is possible to restrict folds at certain\r\nlevels or to set how the folds are calculated. Since I mostly work in\r\npython, I've set up the folding based on indent. I've also remapped\r\nfolding to the space key to make it super-easy to open and close folds.\r\n\r\n```\r\nset foldmethod=indent\r\nset foldlevel=99\r\n\" enable folding with spacebar\r\nnoremap <space> za\r\n```\r\n\r\n### Filetype\r\nVim can configure its settings according to the file type. Since I mostly\r\nwork with Python, which has indent-based syntax, vim knows to work with\r\nthe languge blocks in terms of *indents*.\r\n```\r\nset autoindent\r\nset fileformat=unix\r\nsyntax enable\r\nlet python_highlight_all=1\r\nfiletype indent on\r\nset encoding=utf-8\r\nset autochdir   \" Change working directory to open buffer\r\nset nocompatible\r\nfiletype off\r\n```\r\n\r\n## Line numbers\r\nI cannot emphasise the importance of line numbers enough. Often when I'm \r\nhelping students in the lab debug their programs, the compiler is kind enough\r\nto mention the line number the error occurs at, and the student does not\r\nnotice it because they do not have line numbers turned on.\r\n\r\n**ALWAYS TURN LINE NUMBERS ON**\r\n\r\nThey not only help in debugging, but are also a great way to quickly point out\r\na specific line to someone.\r\n\r\n```\r\nset number\r\n```\r\n\r\n### Relative line numbers\r\nRecently, I found a nifty thing vim is (now) capable of doing - showing relative\r\nline numbers. Plain old simple line numbers begin numbering the first line from 1\r\nand then 2 and so on to the end of file. Relative line numbers are relative to the\r\nline the cursor is at. So the current line is `line 0` and the one above it and\r\none below it are `line 1` and so on. What this does in vim, is it makes motion\r\nsuper easy to go to. In large files, where lines are in hundreds, going above to\r\na particular function on screen means typing out the entire line number. With\r\nrelative line number, one only needs to type out the *relative* number with the\r\nmotion, such as `5j` to go 5 lines below the current one. Combined with the\r\nnormal line number option, it shows the actual line number on the current line,\r\nand relative line numbers on all other lines.\r\n\r\n```\r\nset relativenumber\r\n```\r\n\r\n## Other options\r\n\r\n### Spellcheck\r\nVim can highlight spelling errors in various languages (depending on the language\r\ninstalled and available to use).\r\n\r\n```\r\nset spell\r\n```\r\n\r\n### Search and highlight matches\r\nIn vim, searches can be highlighted as they are typed. This makes it easy to\r\nsee what the matches are, and is a behviour consistent with other editors.\r\n\r\n```\r\nset showmatch\r\nset incsearch\r\nset hlsearch\r\n```\r\n\r\n## Plugins\r\n\r\n### Solarized\r\nSolarized comes as a handy plugin, available at \r\n[Github](https://github.com/altercation/vim-colors-solarized).\r\n\r\n### Ale\r\n[Ale](https://github.com/w0rp/ale) is an asynchronous linting engine,\r\nwhich helps catch linting errors by showing them visually in vim. It is quite\r\nfast, lightweight, and works with most of the popular languages.\r\n\r\n### Airline\r\n[Airline](https://github.com/vim-airline/vim-airline) is a statusbar plugin\r\nthat adds colors to the tagbar (at bottom). It enables a visual mode to distinguish\r\nbetween normal, replace, visual, and insert mode. Additional themes are \r\navailable at [Airline Themes](https://github.com/vim-airline/vim-airline-themes).\r\n\r\n### CtrlP\r\n[CtrlP](https://github.com/kien/ctrlp.vim) allows opening files quickly using\r\n`Ctrl+P` through character matching in the filename. For e.g. to open a file\r\nat path `doc/abc/xyz.txt`, I can type `daxyz` and the file will come up\r\nin the listing. It is a fast and easy way to open files or buffers.\r\n\r\n### Fugitive\r\n[Fugitive](https://github.com/tpope/vim-fugitive) is a git wrapper that allows\r\nexecuting git commands over the currently opened file. It is very extensive,\r\nand allows a plethora of git commands. Plus, it is authored by Tim Pope.\r\n\r\n### Surround\r\n[Surround](https://github.com/tpope/vim-surround) allows manipulation of\r\nsurrounding brackets, quotes, tags, pretty much everything. \r\n\r\n### Tagbar\r\n[Tagbar](https://majutsushi.github.io/tagbar/) displays a sidebar consisting \r\nof CTags found in the file. Requires exuberant ctags. Selecting a tag\r\nallows quick movement to wherever it is defined in the file. For easier\r\nmanipulation, map the sidebar to show/hide using `F8`.\r\n\r\n### Commentary\r\n[Commentary](https://github.com/tpope/vim-commentary) allows \r\nquick comment/uncomment using `gc`. Can be used with motion, making\r\nit possible to quickly comment an entire paragraph or visual selection.\r\n\r\n### Multiple cursors\r\n[Multiple Cursors](https://github.com/terryma/vim-multiple-cursors) allows\r\ninserting multiple cursors just like Sublime Text, though it is not as\r\nsimple or as elegant to do it in Vim.\r\n\r\n### EasyMotion\r\n[EasyMotion](https://github.com/easymotion/vim-easymotion) allows quickly moving\r\nahead by word or through the next few lines using a single-letter motion.\r\nIt is surprisingly simple to use and makes short motion easy.\r\n\r\n### Signature\r\n[Signature](https://github.com/kshenoy/vim-signature) allows interacting\r\nwith vim marks, which are like bookmarks in a file. It allows alphabetical\r\ntraversal, removing marks, and all sorts of things which make navigating\r\nback and forth through files a little easier.", "headerimage": "", "highlight": "0", "section": 5}, {"id": 11, "title": "Inheriting klip", "authors": "1", "date_created": "2017-05-26 15:03:00", "date_published": "2017-05-26 15:03:00", "date_updated": "2017-05-29 16:47:43", "is_published": "1", "short_description": "Inheriting the klip project, cleaning it up, and documenting it", "tags": "142,50,85", "slug": "inheriting-klip", "body_type": "markdown", "body_text": "<blockquote>\n<p>Project Source: <a href=\"https://github.com/coolharsh55/klip\">klip - Github</a></p>\n</blockquote>\n<h4 id=\"how-i-became-the-maintainer-of-klip\">How I became the maintainer of klip</h4>\n<p>I was not the original maintainer of <a href=\"https://github.com/coolharsh55/klip\">klip</a>.\nThe project was started by Github user <a href=\"https://github.com/emre\">emre</a> with a few\ncontributions by <a href=\"https://github.com/berkerpeksag\">berkerpeksag</a>. I came about it\nwhen searching for a <em>pypa</em> (python packaging authority, the place where third\nparty python libraries are hosted) package related to kindle annotations.</p>\n<p>I opened an <a href=\"https://github.com/coolharsh55/klip/issues/4\">issue</a> and emre said\nthat I could have the project as he wasn't really maintaining it anymore. This was\nthe first time I was being given responsibility of someone else's project, and it\nmade me quite excited. The transfer was smooth, and I was even given the project\non pypa. So here I was, with someone else's code, doing what I wanted to do.</p>\n<h4 id=\"moving-forward\">Moving forward</h4>\n<p>The first thing I did was to check if the code works with my Kindle <em>as it is</em>.\nIt did, so I did not need any immediate modifications. With such lazy thoughts,\nthe project stagnated for quite a few months. Recently, I took upon myself to\nupdate the documentation and to maintain it as much as I can. </p>\n<h2 id=\"structure-of-project\">Structure of project</h2>\n<p>There are two file, <code>devices.py</code> contains a class for each Kindle that has a\ndifferent annotation format; and <code>parser.py</code> which extracts the annotations.</p>\n<h3 id=\"devicespy\">devices.py</h3>\n<p>Each device is an instance of an abstract class called <code>KindleBase</code>\nwhich contains the fields and properties used in each annotation.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">class KindleBase(object):\n    noises = None\n    title = None\n    author_in_title = None\n    type_info = None\n    time_format = None\n    clip_type = None\n    page = None\n    location = None\n    added_on = None\n    content = None</code></pre>\n\n\n<p>Classes that inherit this base class define these attributes.\nThe project has classes that handle annotations for-</p>\n<ul>\n<li>Kindle 1-3 (<code>KindleOldGen</code>)</li>\n<li>Kindle 4 (<code>Kindle4</code>)</li>\n<li>Kindle Paperwhite (<code>KindlePaperwhite</code>)</li>\n<li>Kindle Touch (<code>KindleTouch</code>)</li>\n</ul>\n<p>As and when I come across any new form of Kindle (or annotation), I will\ncreate a new class for them and add it to the devices.\nThis keeps the parser free to do its job, which is to parse stuff.</p>\n<h3 id=\"parserpy\">parser.py</h3>\n<p><code>ClippingLoader</code> contains the parsing code in various functions.\nThe module contains two functions for parsing. The first, <code>load</code>,\ntakes data in the form of a string (read from a file, e.g.) and \nparses it. The second, <code>load_from_file</code>, takes a filepath and \nparses the contents of the file.</p>\n<h2 id=\"parsing-logic\">Parsing Logic</h2>\n<h3 id=\"seperating-chunks\">Seperating <code>chunks</code></h3>\n<p>As explained in the\n<a href=\"https://harshp.com/dev/projects/klip-a-kindle-annotations-parser/previous-project-kindle-annotations/\">previous post</a>,\nthe annotations are separated by a series of <code>=</code> characters. \nThe first task is to create <em>chunks</em> of annotations that can then be \nhandled individually. Python offers a handy mechanism to break text\nbased on a pattern using the <code>split</code> method.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">ENTRY_SEPERATOR = '=' * 10\nchunks = data.split(ENTRY_SEPERATOR)</code></pre>\n\n\n<h3 id=\"parsing-chunks\">Parsing chunks</h3>\n<blockquote>\n<p><code>ClippingLoader._parse</code></p>\n</blockquote>\n<p>Each chunk has at least 5 lines-</p>\n<ol>\n<li>seperator</li>\n<li>Title and Author</li>\n<li>Annotation type, location, timestamp</li>\n<li><em>blank line</em></li>\n<li>Text of annotation</li>\n</ol>\n<p>If there are less than 5 lines, then it is not the kind of annotation we need\nto address or handle. To extract each item, we pass the entire chunk to\nthe helper functions which use regex to extract relevant bits and then return it.</p>\n<h2 id=\"todo\">ToDo</h2>\n<ul>\n<li>auto-detect the Kindle model by matching all relevant regexes</li>\n<li>turn the annotation parser into a <code>kindle.js</code> library that can be run in browsers</li>\n<li>use the above script in heroku webapp for online clipping parsing</li>\n</ul>", "body": "> Project Source: [klip - Github](https://github.com/coolharsh55/klip)\r\n\r\n#### How I became the maintainer of klip\r\n\r\nI was not the original maintainer of [klip](https://github.com/coolharsh55/klip).\r\nThe project was started by Github user [emre](https://github.com/emre) with a few\r\ncontributions by [berkerpeksag](https://github.com/berkerpeksag). I came about it\r\nwhen searching for a *pypa* (python packaging authority, the place where third\r\nparty python libraries are hosted) package related to kindle annotations.\r\n\r\nI opened an [issue](https://github.com/coolharsh55/klip/issues/4) and emre said\r\nthat I could have the project as he wasn't really maintaining it anymore. This was\r\nthe first time I was being given responsibility of someone else's project, and it\r\nmade me quite excited. The transfer was smooth, and I was even given the project\r\non pypa. So here I was, with someone else's code, doing what I wanted to do.\r\n\r\n#### Moving forward\r\n\r\nThe first thing I did was to check if the code works with my Kindle *as it is*.\r\nIt did, so I did not need any immediate modifications. With such lazy thoughts,\r\nthe project stagnated for quite a few months. Recently, I took upon myself to\r\nupdate the documentation and to maintain it as much as I can. \r\n\r\n## Structure of project\r\n\r\nThere are two file, `devices.py` contains a class for each Kindle that has a\r\ndifferent annotation format; and `parser.py` which extracts the annotations.\r\n\r\n### devices.py\r\n\r\nEach device is an instance of an abstract class called `KindleBase`\r\nwhich contains the fields and properties used in each annotation.\r\n\r\n```python\r\nclass KindleBase(object):\r\n    noises = None\r\n    title = None\r\n    author_in_title = None\r\n    type_info = None\r\n    time_format = None\r\n    clip_type = None\r\n    page = None\r\n    location = None\r\n    added_on = None\r\n    content = None\r\n```\r\n\r\nClasses that inherit this base class define these attributes.\r\nThe project has classes that handle annotations for-\r\n\r\n* Kindle 1-3 (`KindleOldGen`)\r\n* Kindle 4 (`Kindle4`)\r\n* Kindle Paperwhite (`KindlePaperwhite`)\r\n* Kindle Touch (`KindleTouch`)\r\n\r\nAs and when I come across any new form of Kindle (or annotation), I will\r\ncreate a new class for them and add it to the devices.\r\nThis keeps the parser free to do its job, which is to parse stuff.\r\n\r\n### parser.py\r\n\r\n`ClippingLoader` contains the parsing code in various functions.\r\nThe module contains two functions for parsing. The first, `load`,\r\ntakes data in the form of a string (read from a file, e.g.) and \r\nparses it. The second, `load_from_file`, takes a filepath and \r\nparses the contents of the file.\r\n\r\n## Parsing Logic\r\n\r\n### Seperating `chunks`\r\nAs explained in the\r\n[previous post](https://harshp.com/dev/projects/klip-a-kindle-annotations-parser/previous-project-kindle-annotations/),\r\nthe annotations are separated by a series of `=` characters. \r\nThe first task is to create *chunks* of annotations that can then be \r\nhandled individually. Python offers a handy mechanism to break text\r\nbased on a pattern using the `split` method.\r\n\r\n```python\r\nENTRY_SEPERATOR = '=' * 10\r\nchunks = data.split(ENTRY_SEPERATOR)\r\n```\r\n\r\n### Parsing chunks\r\n\r\n> `ClippingLoader._parse`\r\n\r\nEach chunk has at least 5 lines-\r\n\r\n1. seperator\r\n2. Title and Author\r\n3. Annotation type, location, timestamp\r\n4. *blank line*\r\n5. Text of annotation\r\n\r\nIf there are less than 5 lines, then it is not the kind of annotation we need\r\nto address or handle. To extract each item, we pass the entire chunk to\r\nthe helper functions which use regex to extract relevant bits and then return it.\r\n\r\n## ToDo\r\n\r\n* auto-detect the Kindle model by matching all relevant regexes\r\n* turn the annotation parser into a `kindle.js` library that can be run in browsers\r\n* use the above script in heroku webapp for online clipping parsing", "headerimage": "", "highlight": "0", "section": 4}, {"id": 10, "title": "previous project - kindle annotations", "authors": "1", "date_created": "2017-05-26 14:28:00", "date_published": "2017-05-26 14:28:00", "date_updated": "2017-05-26 15:09:05", "is_published": "1", "short_description": "The previous project of parsing kindle annotations", "tags": "142,85,54", "slug": "previous-project-kindle-annotations", "body_type": "markdown", "body_text": "<h2 id=\"background\">Background</h2>\n<p>I own a <a href=\"https://en.wikipedia.org/wiki/Amazon_Kindle#Kindle_4\">Kindle 4</a>\nwhich mother gifted me 5-6 years back. Since then, I've read hundreds of\nbooks on it and saved thousands of annotations. I've lost all these\nannotations twice. One was when I accidentally emptied my entire Kindle,\nand the second time was when I did the same again. Since then, I've\ncome to realise that the annotations are stored in a single file called\n<code>clippings.txt</code> located in <code>Documents</code>. The format of this file changes\nwith each iteration of the Kindle, and sometimes with certain updates.</p>\n<p>This was the time I was under the influence of <em>regex</em>. Not to sound\nidiotic, but I liked the <em>power</em> of its <em>expressions</em>, and as is the\ncase, with a hammer in my hand, everything looked like a nail. So I\nengineered a way to parse each individual annotation out of the file\nby using regex. The project was in <code>python</code> and the only module used\nwas <code>re</code>. I engineered the solution using some mangled version of a \nstate machine that iterated over each line, and depending on what it\nhad parsed before, executed some action.</p>\n<h2 id=\"format-of-clippings\">Format of clippings</h2>\n<p>A typical annotation on the Kindle4 looks something like this -</p>\n<pre class=\"codehilite\"><code>==========\nThe Fountainhead (Ayn Rand)\n- Highlight Loc. 13169  | Added on Saturday, 26 July 14 21:37:48 GMT+01:00\n\nI could die for you. But I couldn\u2019t and wouldn\u2019t live for you.\u201d\n==========</code></pre>\n\n\n<p>All sections (or annotations) are seperated by a line populated with\nonly the character <code>=</code>. This means that whenever the parser encounters \na line with <code>=</code> in it, it assumes that this is the start of the annotation.\nThis is followed by the title of the book with the name of the author \nenclosed in brackets. After that comes the type of annotation, which can\nbe a <em>highlight</em> or a <em>bookmark</em>, with the location of that annotation in\nthe book and the date it was added on separated by <code>|</code>. This is followed\nby a blank line and then the text of the annotation.</p>\n<h2 id=\"state-machine\">State machine</h2>\n<p>The start state checks whether the line starts with a <code>=</code> character. If it \ndoes, it signals the start of the annotation. After that, it needs to check\nwhether the annotation is a <em>highlight</em>, which can be done by checking whether\nthe line starts with <code>- Highlight</code>. If it does, skip the blank line and gobble\nthe text of the annotation.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">{\n    'check_breakpoint': lambda x: x.startswith('='),\n    'check_is_highlight': lambda x: x.startswith('- Highlight'),\n    'book_info_regex': &quot;^([a-zA-Z']+\\s*[a-zA-Z\\._';\\:,\\s\\d]*)\\((.*)\\)$&quot;,\n    'highlight_regex': '^(.*)$',\n}</code></pre>\n\n\n<p>My naive previous self did not understand that the entire annotation could\nhave been extracted using a single regex expression. \nNevertheless, the code can be found at \n<a href=\"https://github.com/coolharsh55/kindle-annotations\">Github</a>.</p>", "body": "## Background\r\n\r\nI own a [Kindle 4](https://en.wikipedia.org/wiki/Amazon_Kindle#Kindle_4)\r\nwhich mother gifted me 5-6 years back. Since then, I've read hundreds of\r\nbooks on it and saved thousands of annotations. I've lost all these\r\nannotations twice. One was when I accidentally emptied my entire Kindle,\r\nand the second time was when I did the same again. Since then, I've\r\ncome to realise that the annotations are stored in a single file called\r\n`clippings.txt` located in `Documents`. The format of this file changes\r\nwith each iteration of the Kindle, and sometimes with certain updates.\r\n\r\nThis was the time I was under the influence of *regex*. Not to sound\r\nidiotic, but I liked the *power* of its *expressions*, and as is the\r\ncase, with a hammer in my hand, everything looked like a nail. So I\r\nengineered a way to parse each individual annotation out of the file\r\nby using regex. The project was in `python` and the only module used\r\nwas `re`. I engineered the solution using some mangled version of a \r\nstate machine that iterated over each line, and depending on what it\r\nhad parsed before, executed some action.\r\n\r\n## Format of clippings\r\n\r\nA typical annotation on the Kindle4 looks something like this -\r\n\r\n```\r\n==========\r\nThe Fountainhead (Ayn Rand)\r\n- Highlight Loc. 13169  | Added on Saturday, 26 July 14 21:37:48 GMT+01:00\r\n\r\nI could die for you. But I couldn\u2019t and wouldn\u2019t live for you.\u201d\r\n==========\r\n```\r\n\r\nAll sections (or annotations) are seperated by a line populated with\r\nonly the character `=`. This means that whenever the parser encounters \r\na line with `=` in it, it assumes that this is the start of the annotation.\r\nThis is followed by the title of the book with the name of the author \r\nenclosed in brackets. After that comes the type of annotation, which can\r\nbe a *highlight* or a *bookmark*, with the location of that annotation in\r\nthe book and the date it was added on separated by `|`. This is followed\r\nby a blank line and then the text of the annotation.\r\n\r\n## State machine\r\n\r\nThe start state checks whether the line starts with a `=` character. If it \r\ndoes, it signals the start of the annotation. After that, it needs to check\r\nwhether the annotation is a *highlight*, which can be done by checking whether\r\nthe line starts with `- Highlight`. If it does, skip the blank line and gobble\r\nthe text of the annotation.\r\n\r\n```python\r\n{\r\n    'check_breakpoint': lambda x: x.startswith('='),\r\n    'check_is_highlight': lambda x: x.startswith('- Highlight'),\r\n    'book_info_regex': \"^([a-zA-Z']+\\s*[a-zA-Z\\._';\\:,\\s\\d]*)\\((.*)\\)$\",\r\n    'highlight_regex': '^(.*)$',\r\n}\r\n```\r\n\r\nMy naive previous self did not understand that the entire annotation could\r\nhave been extracted using a single regex expression. \r\nNevertheless, the code can be found at \r\n[Github](https://github.com/coolharsh55/kindle-annotations).", "headerimage": "", "highlight": "0", "section": 4}, {"id": 9, "title": "Setting up a Facebook bot to respond with available GNIB/VISA appointments", "authors": "1", "date_created": "2017-05-25 20:27:00", "date_published": "2017-05-25 20:27:00", "date_updated": "2017-05-26 14:12:13", "is_published": "1", "short_description": "Facebook bot that responds to messages with appointments", "tags": "11,131,20,137,85", "slug": "setting-up-a-facebook-bot-to-respond-with-available-gnibvisa-appointments", "body_type": "markdown", "body_text": "<blockquote>\n<p><strong>recap:</strong> in the  previous posts, I set up a webapp that displayed the available\nappointments at <a href=\"https://gnibappt.herokuapp.com/\">https://gnibappt.herokuapp.com/</a>\nby making requests to the API for retrieving GNIB and VISA appointments.</p>\n<p><strong>source</strong>: hosted on Github <a href=\"https://github.com/coolharsh55/GNIBappointments/\">here</a></p>\n<p><strong>webapp</strong>: hosted on Heroku <a href=\"https://gnibappt.herokuapp.com/\">here</a>\nshows available timings for GNIB and Visa appointments</p>\n<p><strong>update:</strong> The Facebook bot requires to be reviewed by Facebook before it can be\nmade available publicly. I have submitted the app for review, with reply awaited.</p>\n</blockquote>\n<p>In this post, I explain how I set up a Facebook bot called\n<a href=\"https://www.facebook.com/GVisaBot/\">GVisaBot</a> that responds to texts with available\nappointments. It is quite easy and <em>free</em> to create a Facebook bot. All it needs is a developer\nkey, and a page, both of which can be created using your <em>existing</em> profile.</p>\n<h2 id=\"how-the-bot-works\">How the bot works</h2>\n<h3 id=\"facebook-bots-and-facebook-pages\">Facebook bots and Facebook Pages</h3>\n<p>Facebook bots are attached or associated with a Facebook Page, and contain a URL which\nFacebook uses to communicate with the bot. When the bot is set up, Facebook provides\na <code>PAGE_ACCESS_TOKEN</code> which is used to verify that the bot is allowed to access and \ninteract on <em>behalf</em> of the page. The second part is the <code>VERIFICATION_TOKEN</code> which\nis set by the developer to proof that the responses are coming from a verified party.</p>\n<h3 id=\"verification\">Verification</h3>\n<p>Verification is performed by sending a <code>GET</code> request to the bot URL, which must\nreturn the <code>hub.challenge</code> key only if the verification token sent by the request\nis the same one set by the developer. Messages are sent as a <code>POST</code> request to the\nURL, with the body containing a specific format that can contain attachements,\nlocations, images, and everything that can be sent in the Messenger. In this post,\nI only focus on the bits related with responding to text messages.</p>\n<h3 id=\"message-format\">Message Format</h3>\n<p>The data sent is a <code>JSON</code> string, which may need parsing/converting depending\non the programming language and web framework being used. The data contains\nthe key <code>entry</code> which is a list, and where each item is a dictionary of which the\nvalue under <code>messaging</code> is of concern.</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;entry&quot;: [\n        {\n            &quot;messaging&quot;: []\n        }\n    ]\n}</code></pre>\n\n\n<p>The <code>messaging</code> list contains further dictionaries, each containing one instance of a message\nfrom a sender. The keys <code>message</code> and <code>sender</code> are used to get the text message and reply\nback to the sender.</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;message&quot;: {\n        &quot;text&quot;: &quot;text of message&quot;\n    },\n    &quot;sender&quot;: {\n        &quot;id&quot;: &quot;id of sender&quot;\n    }\n}</code></pre>\n\n\n<h3 id=\"responding-back-to-the-sender\">Responding back to the sender</h3>\n<p>Each response back is a <code>POST</code> request made to \n<code>https://graph.facebook.com/v2.6/me/messages</code> with the parameters\n<code>access_token</code> containing the <code>PAGE_ACCESS_TOKEN</code> and request data hosting\na <code>JSON</code> dump of the form-</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;recipient&quot;: {\n        &quot;id&quot;: &quot;sender.id from above&quot;\n    },\n    &quot;message&quot;: {\n        &quot;text&quot;: &quot;text of response&quot;\n    }\n}</code></pre>\n\n\n<h2 id=\"setting-up-the-bot\">Setting up the bot</h2>\n<p>Needless to say, this will require that you have an existing Facebook account. </p>\n<blockquote>\n<p>Note: at this point, the bot must be up and online</p>\n</blockquote>\n<ul>\n<li>The first step is to select what Facebook Page the bot will be associated with, \nor <a href=\"https://www.facebook.com/pages/create\">create a new one</a>.</li>\n<li>After that, <a href=\"https://developers.facebook.com/quickstarts/?platform=web\">create a new App</a>\nassociated with that Page. </li>\n<li>Under the app dashboard, select <em>add a product</em> and under that select <em>Messenger</em>. </li>\n<li>Select the page, and get the <code>PAGE_ACCESS_TOKEN</code></li>\n<li>Select webhooks, and give it the URL the bot is hosted at. </li>\n<li>Type in a <code>VERIFICATION_TOKEN</code> that you used in coding the bot</li>\n<li>Select <em>messages</em> under subscriptions.</li>\n</ul>\n<p>That's it! Facebook will verify the URL responds with the correct verification token,\nand then you can use <em>message</em> on the page to interact with the bot.</p>\n<h3 id=\"responses\">Responses</h3>\n<p>Using a simple <code>if-then-else</code> conditional, I programmed the following responses:</p>\n<pre class=\"codehilite\"><code class=\"language-python\">if request in ['g', 'G', 'gnib', 'GNIB']:\n    return gnib_appointments\nelif request in ['v', 'V', 'visa', 'VISA']:\n    return visa_appointments\nelse:\n    return 'not valid'</code></pre>\n\n\n<h2 id=\"other-ideas\">Other ideas</h2>\n<ul>\n<li>The same principle can be extended to create services that can respond to texts</li>\n<li>Send an appointment upon request or as soon as it becomes available.</li>\n<li>It can also be used to create <em>Slack</em> bots, or use <em>Pushbullet</em> to get push notifications.</li>\n</ul>", "body": "> **recap:** in the  previous posts, I set up a webapp that displayed the available\r\nappointments at [https://gnibappt.herokuapp.com/](https://gnibappt.herokuapp.com/)\r\nby making requests to the API for retrieving GNIB and VISA appointments.\r\n\r\n> **source**: hosted on Github [here](https://github.com/coolharsh55/GNIBappointments/)\r\n\r\n> **webapp**: hosted on Heroku [here](https://gnibappt.herokuapp.com/)\r\nshows available timings for GNIB and Visa appointments\r\n\r\n> **update:** The Facebook bot requires to be reviewed by Facebook before it can be\r\nmade available publicly. I have submitted the app for review, with reply awaited.\r\n\r\nIn this post, I explain how I set up a Facebook bot called\r\n[GVisaBot](https://www.facebook.com/GVisaBot/) that responds to texts with available\r\nappointments. It is quite easy and *free* to create a Facebook bot. All it needs is a developer\r\nkey, and a page, both of which can be created using your *existing* profile.\r\n\r\n## How the bot works\r\n\r\n### Facebook bots and Facebook Pages\r\nFacebook bots are attached or associated with a Facebook Page, and contain a URL which\r\nFacebook uses to communicate with the bot. When the bot is set up, Facebook provides\r\na `PAGE_ACCESS_TOKEN` which is used to verify that the bot is allowed to access and \r\ninteract on *behalf* of the page. The second part is the `VERIFICATION_TOKEN` which\r\nis set by the developer to proof that the responses are coming from a verified party.\r\n\r\n### Verification\r\nVerification is performed by sending a `GET` request to the bot URL, which must\r\nreturn the `hub.challenge` key only if the verification token sent by the request\r\nis the same one set by the developer. Messages are sent as a `POST` request to the\r\nURL, with the body containing a specific format that can contain attachements,\r\nlocations, images, and everything that can be sent in the Messenger. In this post,\r\nI only focus on the bits related with responding to text messages.\r\n\r\n### Message Format\r\nThe data sent is a `JSON` string, which may need parsing/converting depending\r\non the programming language and web framework being used. The data contains\r\nthe key `entry` which is a list, and where each item is a dictionary of which the\r\nvalue under `messaging` is of concern.\r\n\r\n```json\r\n{\r\n\t\"entry\": [\r\n\t\t{\r\n\t\t\t\"messaging\": []\r\n\t\t}\r\n\t]\r\n}\r\n```\r\n\r\nThe `messaging` list contains further dictionaries, each containing one instance of a message\r\nfrom a sender. The keys `message` and `sender` are used to get the text message and reply\r\nback to the sender.\r\n\r\n```json\r\n{\r\n\t\"message\": {\r\n\t\t\"text\": \"text of message\"\r\n\t},\r\n\t\"sender\": {\r\n\t\t\"id\": \"id of sender\"\r\n\t}\r\n}\r\n``` \r\n\r\n### Responding back to the sender\r\n\r\nEach response back is a `POST` request made to \r\n`https://graph.facebook.com/v2.6/me/messages` with the parameters\r\n`access_token` containing the `PAGE_ACCESS_TOKEN` and request data hosting\r\na `JSON` dump of the form-\r\n\r\n```json\r\n{\r\n\t\"recipient\": {\r\n\t\t\"id\": \"sender.id from above\"\r\n\t},\r\n\t\"message\": {\r\n\t\t\"text\": \"text of response\"\r\n\t}\r\n}\r\n```\r\n\r\n## Setting up the bot\r\n\r\nNeedless to say, this will require that you have an existing Facebook account. \r\n\r\n> Note: at this point, the bot must be up and online\r\n\r\n* The first step is to select what Facebook Page the bot will be associated with, \r\nor [create a new one](https://www.facebook.com/pages/create).\r\n* After that, [create a new App](https://developers.facebook.com/quickstarts/?platform=web)\r\nassociated with that Page. \r\n* Under the app dashboard, select *add a product* and under that select *Messenger*. \r\n* Select the page, and get the `PAGE_ACCESS_TOKEN`\r\n* Select webhooks, and give it the URL the bot is hosted at. \r\n* Type in a `VERIFICATION_TOKEN` that you used in coding the bot\r\n* Select *messages* under subscriptions.\r\n\r\nThat's it! Facebook will verify the URL responds with the correct verification token,\r\nand then you can use *message* on the page to interact with the bot.\r\n\r\n\r\n### Responses\r\n\r\nUsing a simple `if-then-else` conditional, I programmed the following responses:\r\n\r\n```python\r\nif request in ['g', 'G', 'gnib', 'GNIB']:\r\n\treturn gnib_appointments\r\nelif request in ['v', 'V', 'visa', 'VISA']:\r\n\treturn visa_appointments\r\nelse:\r\n\treturn 'not valid'\r\n```\r\n\r\n## Other ideas\r\n\r\n* The same principle can be extended to create services that can respond to texts\r\n* Send an appointment upon request or as soon as it becomes available.\r\n* It can also be used to create *Slack* bots, or use *Pushbullet* to get push notifications.", "headerimage": "https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/GVisaBot_chat.png", "highlight": "0", "section": 3}, {"id": 8, "title": "Chrome extension for loading/saving form data", "authors": "1", "date_created": "2017-05-20 17:15:00", "date_published": "2017-05-20 17:15:00", "date_updated": "2017-05-20 17:40:22", "is_published": "1", "short_description": "Using a chrome extension to automatically populate GNIB forms", "tags": "140,141,137", "slug": "chrome-extension-for-loadingsaving-form-data", "body_type": "markdown", "body_text": "<blockquote>\n<p><strong>recap:</strong> In the previous posts, I describe retrieving GNIB and Visa \nappointments by manually querying\nthe API endpoints that the website uses. In this process, I describe the use of \n<code>bash/python</code> scripts\nand a Heroku webapp to display appointments.</p>\n<p><strong>source</strong>: hosted on Github <a href=\"https://github.com/coolharsh55/GNIBappointments/\">here</a></p>\n<p><strong>webapp</strong>: hosted on Heroku <a href=\"https://gnibappt.herokuapp.com/\">here</a>\nshows available timings for GNIB and Visa appointments</p>\n</blockquote>\n<p>In this post, I describe the workings of a Chrome extension that retrieves GNIB \nand Visa appointments\nwhen the user is present on those pages. The extension also provides a handy \ntoolbar that is rendered\non those pages to save and re-populate form data. This helps in filling out the \nform quickly once an \nappointment is available.</p>\n<h2 id=\"extension-goals\">Extension goals</h2>\n<ul>\n<li>Visible icon in the browser extension bar</li>\n<li>Checks appointments every 30mins or so in the background</li>\n<li>Notifies when a new appointment is available</li>\n<li>Allows quickly navigating to booking page</li>\n<li>Fills the form with information previously saved</li>\n<li>Works on both GNIB and Visa appointments</li>\n</ul>\n<h3 id=\"achievable-goals\">Achievable goals</h3>\n<p>Out of the listed goals above, few are quite easy (comparitively).\nThe icon can be made visible in the extension bar with not much effort.\nSimilarly, the background checks are simple using javascript's\n<code>setInterval</code> function. Similarly, notifications can be made using alert\nboxes or changing icon on the extension bar. And so on and so forth.</p>\n<p>However, one of the kickers in all this was the presence of CORS policy.\nWhich means that I cannot make requests to the API endpoint outside of the\nGNIB and Visa websites. So no more background queries against those two.\nSo for now, the extension does everything <em>except</em> background checks and\nnotifications.</p>\n<h2 id=\"chrome-extension-structure\">Chrome Extension structure</h2>\n<p>The chrome extension documentation can be found with a \n<a href=\"https://developer.chrome.com/extensions/getstarted\">tutorial</a> that helps\nget things started pretty quickly. </p>\n<h3 id=\"manifest\">manifest</h3>\n<p>The file <code>manifest.json</code> describes the metadata properties of the \nextension and is a JSON document that contains a prefixed set of keys. </p>\n<h4 id=\"icon\">icon</h4>\n<p>The icon of the extension is a <code>128x128</code> png file. Some free ones can be\nobtained <a href=\"http://www.flaticon.com/\">here</a>. </p>\n<h4 id=\"popup\">popup</h4>\n<p><img alt=\"\" src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/gnib_chrome_extension_popup.png\"></p>\n<p>This is the file that is executed when the extension is clicked on. It is a \nsimple HTML file that can contain arbitrary javascript to perform tasks.\nFor our purposes, we add two links for the GNIB and Visa appointment\npages.</p>\n<h4 id=\"content-scripts\">content scripts</h4>\n<p>This section describes javascript files that get executed based on the \ncontent or URL of the webpage. It allows matching/filtering websites\nbased on URL, and then specifying which files get executed.</p>\n<pre class=\"codehilite\"><code class=\"language-json\">&quot;content_scripts&quot;: [\n    {\n        &quot;matches&quot;: [&quot;https://burghquayregistrationoffice.inis.gov.ie/Website/AMSREG/AMSRegWeb.nsf/AppSelect?OpenForm&quot;],\n        &quot;js&quot;: [&quot;jquery.min.js&quot;,  &quot;sweetalert2.min.js&quot;, &quot;gnib_appointments.js&quot;, &quot;gnib_interface.js&quot;],\n        &quot;css&quot;: [&quot;interface.css&quot;, &quot;sweetalert2.min.css&quot;]\n    },\n    {\n        &quot;matches&quot;: [&quot;https://reentryvisa.inis.gov.ie/website/INISOA/IOA.nsf/AppointmentSelection?OpenForm&quot;],\n        &quot;js&quot;: [&quot;jquery.min.js&quot;,  &quot;sweetalert2.min.js&quot;, &quot;visa_appointments.js&quot;, &quot;visa_interface.js&quot;],\n        &quot;css&quot;: [&quot;interface.css&quot;, &quot;sweetalert2.min.css&quot;]\n    }\n  ]</code></pre>\n\n\n<p>Out of these, <code>jquery</code> and <code>sweetalert2</code> are external libraries which I copied because\nexternal requests may not work, or could slow everything down. \n<a href=\"https://jquery.com/\"><code>jquery</code></a> allows access to DOM elements and a quick way to perform interactions. <a href=\"https://limonte.github.io/sweetalert2/\"><code>sweetalert2</code></a> is an utility to create\nfancy popup boxes used to notify and for interactions.\nThe <code>*_appointments.js</code> are responsible for pulling the appointments, and the\n<code>*_interface.js</code> are responsible for interacting with the interface.</p>\n<h2 id=\"adding-an-interface-bar-on-page\">Adding an interface bar on page</h2>\n<p><img alt=\"extension interface\" src=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/gnib_chrome_extension_interface.png\"></p>\n<p>The extension adds a bar that sits on top and offers buttons for:</p>\n<ul>\n<li>checking appointments</li>\n<li>loading saved form data</li>\n<li>saving form data</li>\n</ul>\n<p>The bar is a simple <code>div</code> attached to the page using jQuery on page ready.\nIt is fixed at the top so that the user can browse the page with the bar available\nas a sort of an interface.</p>\n<pre class=\"codehilite\"><code class=\"language-javascript\">var div = $(\n    '&lt;div id=&quot;interface-sticky-header&quot;&gt;'\n    // ... buttons\n    +'&lt;/div&gt;')\n    .appendTo('body');</code></pre>\n\n\n<h3 id=\"checking-appointments\">Checking appointments</h3>\n<p>This is the same bit of code as covered in previous posts. \nUsing jQuery's <code>$(document).ready</code> function, we call the <code>check_appointments</code> function\nwhich will load the appointments and show available timings.</p>\n<h3 id=\"loading-and-saving-data\">Loading and Saving data</h3>\n<p>Chrome offers a way for extensions to store and sync data using the browser APIS.\nThis is done via <code>chrome.storage.sync.*</code> methods <code>get</code> and <code>set</code>. Instead of saving\nand retrieving individual key-values, we use a single <em>data packet</em> in the form of a\ndictionary variable.</p>\n<pre class=\"codehilite\"><code class=\"language-javascript\">// GNIB form fields\nvar category = $('#Category').val();\nvar subcategory = $('#SubCategory').val();\nvar gnib = $('#ConfirmGNIB').val();\nvar gnib_no = $('#GNIBNo').val();\nvar gnib_expiry = $('#GNIBExDT').val();\nvar user_declaration = $('#UsrDeclaration').val();\nvar given_name = $('#GivenName').val();\nvar surname = $('#SurName').val();\nvar dob = $('#DOB').val();\nvar nationality = $('#Nationality').val();\nvar email = $('#Email').val();\nvar email_confirm = $('#EmailConfirm').val();\nvar family_application = $('#FamAppYN').val();\nvar passport = $('#PPNoYN').val();\nvar passport_no = $('#PPNo').val();\n\nchrome.storage.sync.set({\n    data: {\n        &quot;category&quot;: category,\n        &quot;subcategory&quot;: subcategory,\n        &quot;gnib&quot;: gnib,\n        &quot;gnib_no&quot;: gnib_no,\n        &quot;gnib_expiry&quot;: gnib_expiry,\n        &quot;user_declaration&quot;: user_declaration,\n        &quot;given_name&quot;: given_name,\n        &quot;surname&quot;: surname,\n        &quot;dob&quot;: dob,\n        &quot;nationality&quot;: nationality,\n        &quot;email&quot;: email,\n        &quot;email_confirm&quot;: email_confirm,\n        &quot;family_application&quot;: family_application,\n        &quot;passport&quot;: passport,\n        &quot;passport_no&quot;: passport_no\n    }\n});</code></pre>\n\n\n<p>A good side effect of this is that the common fields between GNIB and Visa forms\nsuch as name and passport no are saved with the same identifier. This means that\nif you have filled a GNIB form before and saved that data, the common fields can be\npopulated in the Visa form with it.</p>", "body": "> **recap:** In the previous posts, I describe retrieving GNIB and Visa \r\nappointments by manually querying\r\nthe API endpoints that the website uses. In this process, I describe the use of \r\n`bash/python` scripts\r\nand a Heroku webapp to display appointments.\r\n\r\n> **source**: hosted on Github [here](https://github.com/coolharsh55/GNIBappointments/)\r\n\r\n> **webapp**: hosted on Heroku [here](https://gnibappt.herokuapp.com/)\r\nshows available timings for GNIB and Visa appointments\r\n\r\nIn this post, I describe the workings of a Chrome extension that retrieves GNIB \r\nand Visa appointments\r\nwhen the user is present on those pages. The extension also provides a handy \r\ntoolbar that is rendered\r\non those pages to save and re-populate form data. This helps in filling out the \r\nform quickly once an \r\nappointment is available.\r\n\r\n## Extension goals\r\n\r\n* Visible icon in the browser extension bar\r\n* Checks appointments every 30mins or so in the background\r\n* Notifies when a new appointment is available\r\n* Allows quickly navigating to booking page\r\n* Fills the form with information previously saved\r\n* Works on both GNIB and Visa appointments\r\n\r\n### Achievable goals\r\n\r\nOut of the listed goals above, few are quite easy (comparitively).\r\nThe icon can be made visible in the extension bar with not much effort.\r\nSimilarly, the background checks are simple using javascript's\r\n`setInterval` function. Similarly, notifications can be made using alert\r\nboxes or changing icon on the extension bar. And so on and so forth.\r\n\r\nHowever, one of the kickers in all this was the presence of CORS policy.\r\nWhich means that I cannot make requests to the API endpoint outside of the\r\nGNIB and Visa websites. So no more background queries against those two.\r\nSo for now, the extension does everything *except* background checks and\r\nnotifications.\r\n\r\n## Chrome Extension structure\r\n\r\nThe chrome extension documentation can be found with a \r\n[tutorial](https://developer.chrome.com/extensions/getstarted) that helps\r\nget things started pretty quickly. \r\n\r\n### manifest\r\n\r\nThe file `manifest.json` describes the metadata properties of the \r\nextension and is a JSON document that contains a prefixed set of keys. \r\n\r\n#### icon\r\n\r\nThe icon of the extension is a `128x128` png file. Some free ones can be\r\nobtained [here](http://www.flaticon.com/). \r\n\r\n#### popup\r\n\r\n![](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/gnib_chrome_extension_popup.png)\r\n\r\nThis is the file that is executed when the extension is clicked on. It is a \r\nsimple HTML file that can contain arbitrary javascript to perform tasks.\r\nFor our purposes, we add two links for the GNIB and Visa appointment\r\npages.\r\n\r\n#### content scripts\r\n\r\nThis section describes javascript files that get executed based on the \r\ncontent or URL of the webpage. It allows matching/filtering websites\r\nbased on URL, and then specifying which files get executed.\r\n\r\n```json\r\n\"content_scripts\": [\r\n    {\r\n        \"matches\": [\"https://burghquayregistrationoffice.inis.gov.ie/Website/AMSREG/AMSRegWeb.nsf/AppSelect?OpenForm\"],\r\n        \"js\": [\"jquery.min.js\",  \"sweetalert2.min.js\", \"gnib_appointments.js\", \"gnib_interface.js\"],\r\n        \"css\": [\"interface.css\", \"sweetalert2.min.css\"]\r\n    },\r\n    {\r\n        \"matches\": [\"https://reentryvisa.inis.gov.ie/website/INISOA/IOA.nsf/AppointmentSelection?OpenForm\"],\r\n        \"js\": [\"jquery.min.js\",  \"sweetalert2.min.js\", \"visa_appointments.js\", \"visa_interface.js\"],\r\n        \"css\": [\"interface.css\", \"sweetalert2.min.css\"]\r\n    }\r\n  ]\r\n```\r\n\r\nOut of these, `jquery` and `sweetalert2` are external libraries which I copied because\r\nexternal requests may not work, or could slow everything down. \r\n[`jquery`](https://jquery.com/) allows access to DOM elements and a quick way to perform interactions. [`sweetalert2`](https://limonte.github.io/sweetalert2/) is an utility to create\r\nfancy popup boxes used to notify and for interactions.\r\nThe `*_appointments.js` are responsible for pulling the appointments, and the\r\n`*_interface.js` are responsible for interacting with the interface.\r\n\r\n## Adding an interface bar on page\r\n\r\n![extension interface](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/gnib_chrome_extension_interface.png)\r\n\r\nThe extension adds a bar that sits on top and offers buttons for:\r\n\r\n* checking appointments\r\n* loading saved form data\r\n* saving form data\r\n\r\nThe bar is a simple `div` attached to the page using jQuery on page ready.\r\nIt is fixed at the top so that the user can browse the page with the bar available\r\nas a sort of an interface.\r\n\r\n```javascript\r\nvar div = $(\r\n\t'<div id=\"interface-sticky-header\">'\r\n \t// ... buttons\r\n\t+'</div>')\r\n\t.appendTo('body');\r\n```\r\n\r\n### Checking appointments\r\n\r\nThis is the same bit of code as covered in previous posts. \r\nUsing jQuery's `$(document).ready` function, we call the `check_appointments` function\r\nwhich will load the appointments and show available timings.\r\n\r\n### Loading and Saving data\r\n\r\nChrome offers a way for extensions to store and sync data using the browser APIS.\r\nThis is done via `chrome.storage.sync.*` methods `get` and `set`. Instead of saving\r\nand retrieving individual key-values, we use a single *data packet* in the form of a\r\ndictionary variable.\r\n\r\n```javascript\r\n// GNIB form fields\r\nvar category = $('#Category').val();\r\nvar subcategory = $('#SubCategory').val();\r\nvar gnib = $('#ConfirmGNIB').val();\r\nvar gnib_no = $('#GNIBNo').val();\r\nvar gnib_expiry = $('#GNIBExDT').val();\r\nvar user_declaration = $('#UsrDeclaration').val();\r\nvar given_name = $('#GivenName').val();\r\nvar surname = $('#SurName').val();\r\nvar dob = $('#DOB').val();\r\nvar nationality = $('#Nationality').val();\r\nvar email = $('#Email').val();\r\nvar email_confirm = $('#EmailConfirm').val();\r\nvar family_application = $('#FamAppYN').val();\r\nvar passport = $('#PPNoYN').val();\r\nvar passport_no = $('#PPNo').val();\r\n\r\nchrome.storage.sync.set({\r\n\tdata: {\r\n\t\t\"category\": category,\r\n\t\t\"subcategory\": subcategory,\r\n\t\t\"gnib\": gnib,\r\n\t\t\"gnib_no\": gnib_no,\r\n\t\t\"gnib_expiry\": gnib_expiry,\r\n\t\t\"user_declaration\": user_declaration,\r\n\t\t\"given_name\": given_name,\r\n\t\t\"surname\": surname,\r\n\t\t\"dob\": dob,\r\n\t\t\"nationality\": nationality,\r\n\t\t\"email\": email,\r\n\t\t\"email_confirm\": email_confirm,\r\n\t\t\"family_application\": family_application,\r\n\t\t\"passport\": passport,\r\n\t\t\"passport_no\": passport_no\r\n\t}\r\n});\r\n```\r\n\r\nA good side effect of this is that the common fields between GNIB and Visa forms\r\nsuch as name and passport no are saved with the same identifier. This means that\r\nif you have filled a GNIB form before and saved that data, the common fields can be\r\npopulated in the Visa form with it.", "headerimage": "https://s3-eu-west-1.amazonaws.com/harshp-media/dev/gnib_appointments/gnib_chrome_extension_swal.png", "highlight": "0", "section": 3}, {"id": 3, "title": "Reverse engineering an easy way to check GNIB appointments", "authors": "1", "date_created": "2017-05-19 17:09:00", "date_published": "2017-05-19 17:09:00", "date_updated": "2017-05-20 17:27:32", "is_published": "1", "short_description": "Using reverse engineering to easily pull appointments from the GNIB website", "tags": "137,134,138", "slug": "reverse-engineering-an-easy-way-to-check-gnib-appointments", "body_type": "markdown", "body_text": "<blockquote>\n<p><strong>source</strong>: hosted on Github <a href=\"https://github.com/coolharsh55/GNIBappointments/\">here</a></p>\n<p><strong>webapp</strong>: hosted on Heroku <a href=\"https://gnibappt.herokuapp.com/\">here</a>\nshows available timings for GNIB and Visa appointments</p>\n</blockquote>\n<p>As per the the <a href=\"http://www.inis.gov.ie/en/INIS/Pages/registration\">Irish Law</a>\nI must register myself with the local Police (or Garda). They issue a GNIB\ncard, which is proof that I am registered with them and am permitted to\nremain in Ireland. For this, I need to take an appointment online through\ntheir <a href=\"https://burghquayregistrationoffice.inis.gov.ie/\">website</a>. Seems like\na little extra effort but then that much was enough to make me stop working\non what I was supposed to be doing and instead look into how I can <em>save time</em>\non these appointment bookings.</p>\n<p>The caveat with this approach is that now everyone books online, and somehow\nthere almost <em>never</em> are any free appointments available. To just check whether\nthere are any appointments available (in a future date), I need to full a form\nwith 15 fields, which itself takes 5 minutes, \nonly to see a <em>\"no appointments available\"</em> message at the end. Frustrating.</p>\n<p>Through this post, I aim to document my efforts into reverse engineering\nGNIB appointment website and protocols to easily get available appointments.\nIt resulted in a bit of javascript code that when pasted into the browser\nconsole, prints the available appointments.\nThis is intended to be a first of many posts detailing my efforts to make \nthis a side-project and a tool that can be helpful for others.</p>\n<h2 id=\"digging-through-javascript-form-onclick\">Digging through javascript - Form <code>onClick</code></h2>\n<p>The first rule of automation is to familiar oneself with what is being automated.\nSo I set out to inspect what happens when I fill the form and hit the get \nappointments button. This involved digging through the javascript the site\nruns and following variables and AJAX requests around. Thankfully, Chrome makes\nthis really easy using <code>right click -&gt; inspect</code>.</p>\n<p>The button (id <code>btLook4App</code>) has an <code>onclick</code> value of <code>allowLook4App()</code>\ndefined in \n<a href=\"https://burghquayregistrationoffice.inis.gov.ie/Website/AMSREG/AMSRegWeb.nsf/AppForm.js\">Appform.js</a>.\nThe function calls another function called <code>get4DateAvailability()</code> which\nis where the appointments are retrieved using an AJAX request.\nThe url for this request is constructed dynamically using the statement\n<code>\"/\" + stPath + \"/(getApps4DTAvailability)?openpage\"</code> where \n<code>stPath</code> is <code>\"Website/AMSREG/AMSRegWeb.nsf\"</code>. One thing to note here is \nthe addition of <code>openpage</code> at the end of the URL - it is meant to be an\nempty GET parameter. Looking at the other parameters of the AJAX request,\none can figure out that it only sends information about </p>\n<ol>\n<li>Category</li>\n</ol>\n<p>Yup, that's all the information it sends from those 15 fields that were filled.\nThe whole requirement of needing the entire form to be filled and validated\nbefore sending requests is a <em>sham</em> (and a shame). </p>\n<h2 id=\"get-request\">GET request</h2>\n<p>The GET parameters can be summarised as:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;cat&quot;: &quot;value of element #Category&quot;,\n    &quot;sbcat&quot;: &quot;All&quot;,\n    &quot;typ: &quot;Renewal&quot;,\n    &quot;openpage&quot;: &quot;&quot;\n}</code></pre>\n\n\n<p>The kicker here is that the URL handling this GET request has (sanely) been\nconfigured to implement <a href=\"https://en.wikipedia.org/wiki/Cross-origin_resource_sharing\">CORS</a>\nwhich means that I cannot just query it from any other website.\nSo I opened the browser console and tried to see if my AJAX request worked\nby basically copying a minimal version of the original. It needs a variable\ncalled <code>dataThis</code> which is based on Category and a static string.</p>\n<pre class=\"codehilite\"><code class=\"language-js\">var dataThis = &quot;&amp;cat=Study&amp;sbcat=All&amp;typ=Renewal&quot;;\n$.ajax( {\n    type : &quot;GET&quot;,\n    url :   &quot;/&quot; + stPath + &quot;/(getApps4DTAvailability)?openpage&quot;,\n    data : dataThis,\n    success : function(data) {\n        console.log(data);\n    }\n});</code></pre>\n\n\n<p>which yields the JSON response of <code>Object {slots: \"[]\"}</code> which is NOTHING. </p>\n<h2 id=\"ajax-request-and-response\">AJAX request and response</h2>\n<p>So I knew I was on a wrong track. Further diggging through requests gave the new\nURL of <code>\"/\" + stPath + \"/(getAppsNear)?openpage\"</code> in function <code>getEarliestApps()</code>\nwhich is part of a hidden button that gets activated/shown only after the \nform is complete and validated. Querying this new URL using the following function yielded in the given response.</p>\n<pre class=\"codehilite\"><code class=\"language-js\">var dataThis = &quot;&amp;cat=Study&amp;sbcat=All&amp;typ=Renewal&quot;;\n$.ajax( {\n    type : &quot;GET&quot;,\n    url :   &quot;/&quot; + stPath + &quot;/(getAppsNear)?openpage&quot;,\n    data : dataThis,\n    success : function(data) {\n        console.log(data);\n    }\n});</code></pre>\n\n\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;slots&quot;: [\n        {\n            &quot;id&quot;: &quot;0A03221D91CF90848025811400317D02&quot;,\n            &quot;time&quot;: &quot;4 July 2017 - 10:00&quot;\n        },\n        {\n            &quot;id&quot;: &quot;3B7F286B73A976F08025812500318633&quot;,\n            &quot;time&quot;: &quot;21 July 2017 - 08:00&quot;\n        }\n    ]\n}</code></pre>\n\n\n<p>On a successful query, the response sends over available times in a list \nunder the key <code>slots</code> comprised of <code>id</code> and <code>time</code>, both of which are\nstrings. For my purpose, it was enough to extract the <code>time</code> values as\navailable appointments. Studying <code>getEarliestApps()</code> shows the conditions\nand ways to detect other possible conditions.</p>\n<ul>\n<li>If there is a field called <code>data.error</code> then the request has an error</li>\n<li>If there is a field called <code>data.empty</code> then there are no appointments currently available</li>\n<li>Just to be sure, I also check length of <code>data.slots</code> as it can be an empty list</li>\n</ul>\n<h2 id=\"a-quick-way-to-get-appointments\">A quick way to get appointments</h2>\n<p>This gave me a way to quickly check available appointments by opening the website and executing the AJAX request with the <code>success</code> function printing out the\navailable appointment times. Any error on the console means there was an\nerror with the request or there are no appointments available currently.</p>\n<pre class=\"codehilite\"><code class=\"language-js\">var dataThis = &quot;&amp;cat=Study&amp;sbcat=All&amp;typ=Renewal&quot;;\n$.ajax( {\n    type : &quot;GET&quot;,\n    url :   &quot;/&quot; + stPath + &quot;/(getAppsNear)?openpage&quot;,\n    data : dataThis,\n    success : function(data) {\n        for(appointment of data.slots) {\n            console.log(appointment.time);\n        }\n    }\n});</code></pre>\n\n\n<h2 id=\"future-plans\">Future plans</h2>\n<ul>\n<li>Turn this into a <strong>script</strong> that I can run anywhere (terminal/server) so that it\ncan be automated (<code>cron</code>)</li>\n<li>Create a <strong>chrome extension</strong> that does the AJAX request automatically periodically\nand then notifies using an icon the available appointments</li>\n</ul>", "body": "> **source**: hosted on Github [here](https://github.com/coolharsh55/GNIBappointments/)\r\n\r\n> **webapp**: hosted on Heroku [here](https://gnibappt.herokuapp.com/)\r\nshows available timings for GNIB and Visa appointments\r\n\r\nAs per the the [Irish Law](http://www.inis.gov.ie/en/INIS/Pages/registration)\r\nI must register myself with the local Police (or Garda). They issue a GNIB\r\ncard, which is proof that I am registered with them and am permitted to\r\nremain in Ireland. For this, I need to take an appointment online through\r\ntheir [website](https://burghquayregistrationoffice.inis.gov.ie/). Seems like\r\na little extra effort but then that much was enough to make me stop working\r\non what I was supposed to be doing and instead look into how I can *save time*\r\non these appointment bookings.\r\n\r\nThe caveat with this approach is that now everyone books online, and somehow\r\nthere almost *never* are any free appointments available. To just check whether\r\nthere are any appointments available (in a future date), I need to full a form\r\nwith 15 fields, which itself takes 5 minutes, \r\nonly to see a *\"no appointments available\"* message at the end. Frustrating.\r\n\r\nThrough this post, I aim to document my efforts into reverse engineering\r\nGNIB appointment website and protocols to easily get available appointments.\r\nIt resulted in a bit of javascript code that when pasted into the browser\r\nconsole, prints the available appointments.\r\nThis is intended to be a first of many posts detailing my efforts to make \r\nthis a side-project and a tool that can be helpful for others.\r\n\r\n## Digging through javascript - Form `onClick`\r\n\r\nThe first rule of automation is to familiar oneself with what is being automated.\r\nSo I set out to inspect what happens when I fill the form and hit the get \r\nappointments button. This involved digging through the javascript the site\r\nruns and following variables and AJAX requests around. Thankfully, Chrome makes\r\nthis really easy using `right click -> inspect`.\r\n\r\nThe button (id `btLook4App`) has an `onclick` value of `allowLook4App()`\r\ndefined in \r\n[Appform.js](https://burghquayregistrationoffice.inis.gov.ie/Website/AMSREG/AMSRegWeb.nsf/AppForm.js).\r\nThe function calls another function called `get4DateAvailability()` which\r\nis where the appointments are retrieved using an AJAX request.\r\nThe url for this request is constructed dynamically using the statement\r\n`\"/\" + stPath + \"/(getApps4DTAvailability)?openpage\"` where \r\n`stPath` is `\"Website/AMSREG/AMSRegWeb.nsf\"`. One thing to note here is \r\nthe addition of `openpage` at the end of the URL - it is meant to be an\r\nempty GET parameter. Looking at the other parameters of the AJAX request,\r\none can figure out that it only sends information about \r\n\r\n1. Category\r\n\r\nYup, that's all the information it sends from those 15 fields that were filled.\r\nThe whole requirement of needing the entire form to be filled and validated\r\nbefore sending requests is a *sham* (and a shame). \r\n\r\n## GET request\r\n\r\nThe GET parameters can be summarised as:\r\n\r\n```json\r\n{\r\n\t\"cat\": \"value of element #Category\",\r\n\t\"sbcat\": \"All\",\r\n\t\"typ: \"Renewal\",\r\n\t\"openpage\": \"\"\r\n}\r\n```\r\n\r\nThe kicker here is that the URL handling this GET request has (sanely) been\r\nconfigured to implement [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing)\r\nwhich means that I cannot just query it from any other website.\r\nSo I opened the browser console and tried to see if my AJAX request worked\r\nby basically copying a minimal version of the original. It needs a variable\r\ncalled `dataThis` which is based on Category and a static string.\r\n\r\n```js\r\nvar dataThis = \"&cat=Study&sbcat=All&typ=Renewal\";\r\n$.ajax( {\r\n\ttype : \"GET\",\r\n\turl :   \"/\" + stPath + \"/(getApps4DTAvailability)?openpage\",\r\n\tdata : dataThis,\r\n\tsuccess : function(data) {\r\n\t\tconsole.log(data);\r\n\t}\r\n});\r\n```\r\n\r\nwhich yields the JSON response of `Object {slots: \"[]\"}` which is NOTHING. \r\n\r\n## AJAX request and response\r\n\r\nSo I knew I was on a wrong track. Further diggging through requests gave the new\r\nURL of `\"/\" + stPath + \"/(getAppsNear)?openpage\"` in function `getEarliestApps()`\r\nwhich is part of a hidden button that gets activated/shown only after the \r\nform is complete and validated. Querying this new URL using the following function yielded in the given response.\r\n\r\n```js\r\nvar dataThis = \"&cat=Study&sbcat=All&typ=Renewal\";\r\n$.ajax( {\r\n\ttype : \"GET\",\r\n\turl :   \"/\" + stPath + \"/(getAppsNear)?openpage\",\r\n\tdata : dataThis,\r\n\tsuccess : function(data) {\r\n\t\tconsole.log(data);\r\n\t}\r\n});\r\n```\r\n\r\n```json\r\n{\r\n\t\"slots\": [\r\n\t\t{\r\n\t\t\t\"id\": \"0A03221D91CF90848025811400317D02\",\r\n\t\t\t\"time\": \"4 July 2017 - 10:00\"\r\n\t\t},\r\n\t\t{\r\n\t\t\t\"id\": \"3B7F286B73A976F08025812500318633\",\r\n\t\t\t\"time\": \"21 July 2017 - 08:00\"\r\n\t\t}\r\n\t]\r\n}\r\n```\r\n\r\nOn a successful query, the response sends over available times in a list \r\nunder the key `slots` comprised of `id` and `time`, both of which are\r\nstrings. For my purpose, it was enough to extract the `time` values as\r\navailable appointments. Studying `getEarliestApps()` shows the conditions\r\nand ways to detect other possible conditions.\r\n\r\n* If there is a field called `data.error` then the request has an error\r\n* If there is a field called `data.empty` then there are no appointments currently available\r\n* Just to be sure, I also check length of `data.slots` as it can be an empty list\r\n\r\n## A quick way to get appointments\r\n\r\nThis gave me a way to quickly check available appointments by opening the website and executing the AJAX request with the `success` function printing out the\r\navailable appointment times. Any error on the console means there was an\r\nerror with the request or there are no appointments available currently.\r\n\r\n```js\r\nvar dataThis = \"&cat=Study&sbcat=All&typ=Renewal\";\r\n$.ajax( {\r\n\ttype : \"GET\",\r\n\turl :   \"/\" + stPath + \"/(getAppsNear)?openpage\",\r\n\tdata : dataThis,\r\n\tsuccess : function(data) {\r\n\t\tfor(appointment of data.slots) {\r\n\t\t\tconsole.log(appointment.time);\r\n\t\t}\r\n\t}\r\n});\r\n```\r\n\r\n## Future plans\r\n\r\n* Turn this into a **script** that I can run anywhere (terminal/server) so that it\r\ncan be automated (`cron`)\r\n* Create a **chrome extension** that does the AJAX request automatically periodically\r\nand then notifies using an icon the available appointments", "headerimage": "", "highlight": "0", "section": 3}, {"id": 7, "title": "Retrieving Visa appointments", "authors": "1", "date_created": "2017-05-20 13:52:00", "date_published": "2017-05-20 13:52:00", "date_updated": "2017-05-20 17:27:24", "is_published": "1", "short_description": "Retrieving visa appointments similar to GNIB appointments", "tags": "137,138,139", "slug": "retrieving-visa-appointments", "body_type": "markdown", "body_text": "<blockquote>\n<p><strong>source</strong>: hosted on Github <a href=\"https://github.com/coolharsh55/GNIBappointments/\">here</a></p>\n<p><strong>webapp</strong>: hosted on Heroku <a href=\"https://gnibappt.herokuapp.com/\">here</a>\nshows available timings for GNIB and Visa appointments</p>\n</blockquote>\n<p>In Ireland, registering with Garda allows you a legal permission to stay in Ireland. But if you need to\nleave the country and travel for any reason, and need to come back, you need a visa. A single entry\nvisa allows returning once, while a multi-entry visa allows returning multiple times. Unfortunately,\nthe visa appointments are also made online in much the same as the GNIB ones. They require\nfilling out a form before viewing available dates and timings. Since in the previous posts, I managed\nto create scripts, and a webapp to easily view the timings without the hassle of filling out the forms,\nI sought to do the same for visa appointments as well.</p>\n<p>In this post, I describe how the system for visa appointments works, and reverse engineer a way to\nget the timings. They can be viewed online <a href=\"https://gnibappt.herokuapp.com/\">here</a> and the source\ncode can be accessed <a href=\"https://github.com/coolharsh55/GNIBappointments/\">here</a>.</p>\n<h2 id=\"dates-and-timings\">Dates and Timings</h2>\n<p>Although much of the system works in a similar manner, it is slightly different enough to require a\ndifferent code rather than just changing the API endpoint used to get appointments. In the visa\nsystem, there are available dates for appointments which are then queried to get available times.\nSo the system first queries the available dates, and then once the user selects a particular date,\nqueries for the available timings. </p>\n<p>The url for getting the available dates is <code>https://reentryvisa.inis.gov.ie/website/INISOA/IOA.nsf/(getDTAvail)</code>\nand the parameters required are:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;openagent&quot;: &quot;&quot;,\n    &quot;type&quot;: &quot;I&quot;\n}</code></pre>\n\n\n<p><code>openagent</code> is a null parameter, so it has no value or is empty. <code>type</code> signifies the visa appointment\nis for an individual. Like the GNIB system, much of this code is from \n<a href=\"https://reentryvisa.inis.gov.ie/website/INISOA/IOA.nsf/AppForm.js\">this</a> javascript file.</p>\n<p>Making the request provides dates as a list of strings of the form <code>DD/MM/YYYY</code>. These need to be passed\nas a parameter to the url <code>https://reentryvisa.inis.gov.ie/website/inisoa/ioa.nsf/(getapps4dt)</code>\nto get the timings for appointments on that particular date. The parameters for this request are:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;openagent&quot;: &quot;&quot;,\n    &quot;type&quot;: &quot;I&quot;,\n    &quot;num&quot;: 1,\n    &quot;dt&quot;: &quot;&lt;date&gt;&quot;\n}   </code></pre>\n\n\n<p>The <code>num</code> parameter specifies the number of people associated with the appointment, which for an\nindividual is <code>1</code>. The <code>dt</code> parameter specifies the date for which these appointments are requested.</p>\n<p>The response to this request is rather similar to the response of the GNIB request. It contains a field \ncalled <code>slots</code> that contains the timings for that particular date in the format:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;slots&quot;: [\n        {\n            &quot;id&quot;: &quot;string&quot;,\n            &quot;time&quot;: &quot;DD/MM/YYYY - HH:MM&quot;\n        }\n    ]\n}</code></pre>\n\n\n<p>The usual checks for <em>errors, null values, empty lists</em> etc. apply here. Updating the scripts, and the heroku\nwebapp to retrieve visa appointments is not much work beyond what was previously done.</p>\n<h2 id=\"future-work\">Future Work</h2>\n<ul>\n<li>Chrome extension</li>\n</ul>", "body": "> **source**: hosted on Github [here](https://github.com/coolharsh55/GNIBappointments/)\r\n\r\n> **webapp**: hosted on Heroku [here](https://gnibappt.herokuapp.com/)\r\nshows available timings for GNIB and Visa appointments\r\n\r\nIn Ireland, registering with Garda allows you a legal permission to stay in Ireland. But if you need to\r\nleave the country and travel for any reason, and need to come back, you need a visa. A single entry\r\nvisa allows returning once, while a multi-entry visa allows returning multiple times. Unfortunately,\r\nthe visa appointments are also made online in much the same as the GNIB ones. They require\r\nfilling out a form before viewing available dates and timings. Since in the previous posts, I managed\r\nto create scripts, and a webapp to easily view the timings without the hassle of filling out the forms,\r\nI sought to do the same for visa appointments as well.\r\n\r\nIn this post, I describe how the system for visa appointments works, and reverse engineer a way to\r\nget the timings. They can be viewed online [here](https://gnibappt.herokuapp.com/) and the source\r\ncode can be accessed [here](https://github.com/coolharsh55/GNIBappointments/).\r\n\r\n## Dates and Timings\r\n\r\nAlthough much of the system works in a similar manner, it is slightly different enough to require a\r\ndifferent code rather than just changing the API endpoint used to get appointments. In the visa\r\nsystem, there are available dates for appointments which are then queried to get available times.\r\nSo the system first queries the available dates, and then once the user selects a particular date,\r\nqueries for the available timings. \r\n\r\nThe url for getting the available dates is `https://reentryvisa.inis.gov.ie/website/INISOA/IOA.nsf/(getDTAvail)`\r\nand the parameters required are:\r\n\r\n```json\r\n{\r\n\t\"openagent\": \"\",\r\n\t\"type\": \"I\"\r\n}\r\n```\r\n\r\n`openagent` is a null parameter, so it has no value or is empty. `type` signifies the visa appointment\r\nis for an individual. Like the GNIB system, much of this code is from \r\n[this](https://reentryvisa.inis.gov.ie/website/INISOA/IOA.nsf/AppForm.js) javascript file.\r\n\r\nMaking the request provides dates as a list of strings of the form `DD/MM/YYYY`. These need to be passed\r\nas a parameter to the url `https://reentryvisa.inis.gov.ie/website/inisoa/ioa.nsf/(getapps4dt)`\r\nto get the timings for appointments on that particular date. The parameters for this request are:\r\n\r\n```json\r\n{\r\n\t\"openagent\": \"\",\r\n\t\"type\": \"I\",\r\n\t\"num\": 1,\r\n\t\"dt\": \"<date>\"\r\n}\t\r\n```\r\n\r\nThe `num` parameter specifies the number of people associated with the appointment, which for an\r\nindividual is `1`. The `dt` parameter specifies the date for which these appointments are requested.\r\n\r\nThe response to this request is rather similar to the response of the GNIB request. It contains a field \r\ncalled `slots` that contains the timings for that particular date in the format:\r\n\r\n```json\r\n{\r\n\t\"slots\": [\r\n\t\t{\r\n\t\t\t\"id\": \"string\",\r\n\t\t\t\"time\": \"DD/MM/YYYY - HH:MM\"\r\n\t\t}\r\n\t]\r\n}\r\n```\r\n\r\nThe usual checks for *errors, null values, empty lists* etc. apply here. Updating the scripts, and the heroku\r\nwebapp to retrieve visa appointments is not much work beyond what was previously done.\r\n\r\n## Future Work\r\n\r\n* Chrome extension", "headerimage": "", "highlight": "0", "section": 3}, {"id": 6, "title": "Heroku app to view GNIB appointments", "authors": "1", "date_created": "2017-05-20 13:33:00", "date_published": "2017-05-20 13:33:00", "date_updated": "2017-05-20 17:27:13", "is_published": "1", "short_description": "Using a free heroku app to easily view GNIB appointments", "tags": "131,137,81,85", "slug": "heroku-app-to-view-gnib-appointments", "body_type": "markdown", "body_text": "<blockquote>\n<p><strong>recap:</strong> In the previous posts, I described retrieving GNIB appointments\nthrough an endpoint using javascript in the browser console, a bash script,\nand a python script.</p>\n<p><strong>source</strong>: hosted on Github <a href=\"https://github.com/coolharsh55/GNIBappointments/\">here</a></p>\n<p><strong>webapp</strong>: hosted on Heroku <a href=\"https://gnibappt.herokuapp.com/\">here</a>\nshows available timings for GNIB and Visa appointments</p>\n</blockquote>\n<p>The good thing about a script is that they can be run pretty much anywhere,\nsuch as a dedicated web-server, or the phone (through some tedious efforts),\na raspberry pi, or through free services like <a href=\"https://heroku.com\">heroku</a>.</p>\n<p>In this post, I describe setting up the appointment script on a heroku instance.\nThe appointments are refreshed every 30mins (a resonable time), and can be\nviewed at any time using <a href=\"https://gnibappt.herokuapp.com/\">this</a> website.</p>\n<h2 id=\"heroku\">Heroku</h2>\n<p>Heroku is a PaaS service, which means that it takes care of everything\nand lets you focus only on the web browser part of it. It provides 50 free instances\nfor apps, which can be accessed using the address <em>https://app-name.herokuapp.com</em>.\nOh, and free HTTPS! Heroku provides app support for a large variety of languages,\nwhich (on the date of this post) include Node.js, Ruby, Python, Java, PHP, Go, Scala, and Clojure.\nThere is support for more languages, which can be checked through official documentation.</p>\n<h3 id=\"deployment\">Deployment</h3>\n<p>Publishing apps on heroku requires an account, which is free to sign up. The free tier gives\naccess to 1000 free hours of compute time which can be used in as many instances as you want,\nthough I think the number is limited to 50. Each app/instance or as heroku terms it - dyno - is\nautomatically suspended after 30 mins of inactivity. That means that it starts (and is a little slow)\nafter 30 mins of no-one using the app. Each dyno has access to 512MB of RAM (generous!) and\ncan perform operations on a single CPU (thread/worker).</p>\n<p>Deployment can be made by using the heroku toolbelt which are a set of utilities that help with\nversion control and pulling logs from the app instance. Or the app can be connected to a git\nrepository online such as Github so that commits are automatically deployed to the app.</p>\n<h3 id=\"procfile\">Procfile</h3>\n<p>A <code>Procfile</code> specifies the command and parameters for heroku to use when running the instance.\nThe procfile should be situated in the base directory of the app. For a simple, single dyno, free app,\nthe procfile specifies the command <code>web: command --arguments</code> as a way to specify that the dyno\nis of type <code>web</code> (as opposed to <code>worker</code>) and starting it will run <code>command</code> with the specified\narguments.</p>\n<h2 id=\"requirements\">requirements</h2>\n<p>Heroku supports third-party libraries which (in the case of <em>python</em>) can be specified using\n<code>requirements.txt</code>. Any library in this file will be installed before starting the app.</p>\n<h3 id=\"bottle\">Bottle</h3>\n<p><a href=\"https://bottlepy.org/docs/dev/\">Bottle</a> is a small, minimal python web framework that provides routing\nand templates and is very easy to run. Bottle is a single file library/module. Since the requirements\nof the GNIB project are simple, the project contains just one file - <code>app.py</code>. It uses much of the same\ncode as specified in the last post to perform the actual requests. </p>\n<h3 id=\"gunicorn\">Gunicorn</h3>\n<p><a href=\"http://gunicorn.org/\">Gunicorn</a> is a python WSGI HTTP server, which in plain-speak means that it\nhandles requests and lets bottle do the routing and actual work of generating page responses.\nThe following snippet runs gunicorn with bottle.</p>\n<pre class=\"codehilite\"><code class=\"language-python\"> run(\n    debug=True,\n    server='gunicorn', host='0.0.0.0', port=int(env.get(&quot;PORT&quot;, 5000)))</code></pre>\n\n\n<h2 id=\"checking-appointments-every-30-minutes\">Checking appointments every 30 minutes</h2>\n<p>Since our free app contains only a <code>web</code> worker without any background processes, we use a simple\n<code>if-then-else</code> conditional to check if it has been more than <em>30 mins</em> since we last checked appointments.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">from datetime import datetime\nlast_checked = None\n\ndef check():\n    if last_checked is None:\n        get_appointments()\n        last_checked = now\n    else:\n        now = datetime.now()\n        diff = now - last_checked\n        if diff.days &gt; 0 or diff.seconds &gt; 1800:\n            get_appointments()\n            last_checked = now</code></pre>\n\n\n<h2 id=\"future-work\">Future Work</h2>\n<ul>\n<li>Retrieve Visa appointments</li>\n<li>Chrome extension</li>\n</ul>", "body": "> **recap:** In the previous posts, I described retrieving GNIB appointments\r\nthrough an endpoint using javascript in the browser console, a bash script,\r\nand a python script.\r\n\r\n> **source**: hosted on Github [here](https://github.com/coolharsh55/GNIBappointments/)\r\n\r\n> **webapp**: hosted on Heroku [here](https://gnibappt.herokuapp.com/)\r\nshows available timings for GNIB and Visa appointments\r\n\r\nThe good thing about a script is that they can be run pretty much anywhere,\r\nsuch as a dedicated web-server, or the phone (through some tedious efforts),\r\na raspberry pi, or through free services like [heroku](https://heroku.com).\r\n\r\nIn this post, I describe setting up the appointment script on a heroku instance.\r\nThe appointments are refreshed every 30mins (a resonable time), and can be\r\nviewed at any time using [this](https://gnibappt.herokuapp.com/) website.\r\n\r\n## Heroku\r\n\r\nHeroku is a PaaS service, which means that it takes care of everything\r\nand lets you focus only on the web browser part of it. It provides 50 free instances\r\nfor apps, which can be accessed using the address *https://app-name.herokuapp.com*.\r\nOh, and free HTTPS! Heroku provides app support for a large variety of languages,\r\nwhich (on the date of this post) include Node.js, Ruby, Python, Java, PHP, Go, Scala, and Clojure.\r\nThere is support for more languages, which can be checked through official documentation.\r\n\r\n### Deployment\r\n\r\nPublishing apps on heroku requires an account, which is free to sign up. The free tier gives\r\naccess to 1000 free hours of compute time which can be used in as many instances as you want,\r\nthough I think the number is limited to 50. Each app/instance or as heroku terms it - dyno - is\r\nautomatically suspended after 30 mins of inactivity. That means that it starts (and is a little slow)\r\nafter 30 mins of no-one using the app. Each dyno has access to 512MB of RAM (generous!) and\r\ncan perform operations on a single CPU (thread/worker).\r\n\r\nDeployment can be made by using the heroku toolbelt which are a set of utilities that help with\r\nversion control and pulling logs from the app instance. Or the app can be connected to a git\r\nrepository online such as Github so that commits are automatically deployed to the app.\r\n\r\n### Procfile\r\n\r\nA `Procfile` specifies the command and parameters for heroku to use when running the instance.\r\nThe procfile should be situated in the base directory of the app. For a simple, single dyno, free app,\r\nthe procfile specifies the command `web: command --arguments` as a way to specify that the dyno\r\nis of type `web` (as opposed to `worker`) and starting it will run `command` with the specified\r\narguments.\r\n\r\n## requirements\r\n\r\nHeroku supports third-party libraries which (in the case of *python*) can be specified using\r\n`requirements.txt`. Any library in this file will be installed before starting the app.\r\n\r\n### Bottle\r\n\r\n[Bottle](https://bottlepy.org/docs/dev/) is a small, minimal python web framework that provides routing\r\nand templates and is very easy to run. Bottle is a single file library/module. Since the requirements\r\nof the GNIB project are simple, the project contains just one file - `app.py`. It uses much of the same\r\ncode as specified in the last post to perform the actual requests. \r\n\r\n### Gunicorn\r\n\r\n[Gunicorn](http://gunicorn.org/) is a python WSGI HTTP server, which in plain-speak means that it\r\nhandles requests and lets bottle do the routing and actual work of generating page responses.\r\nThe following snippet runs gunicorn with bottle.\r\n\r\n```python\r\n run(\r\n \tdebug=True,\r\n\tserver='gunicorn', host='0.0.0.0', port=int(env.get(\"PORT\", 5000)))\r\n```\r\n\r\n## Checking appointments every 30 minutes\r\n\r\nSince our free app contains only a `web` worker without any background processes, we use a simple\r\n`if-then-else` conditional to check if it has been more than *30 mins* since we last checked appointments.\r\n\r\n```python\r\nfrom datetime import datetime\r\nlast_checked = None\r\n\r\ndef check():\r\n\tif last_checked is None:\r\n\t\tget_appointments()\r\n\t\tlast_checked = now\r\n\telse:\r\n\t\tnow = datetime.now()\r\n\t\tdiff = now - last_checked\r\n\t\tif diff.days > 0 or diff.seconds > 1800:\r\n\t\t\tget_appointments()\r\n\t\t\tlast_checked = now\r\n```\r\n\r\n## Future Work\r\n\r\n* Retrieve Visa appointments\r\n* Chrome extension", "headerimage": "", "highlight": "0", "section": 3}, {"id": 5, "title": "Getting appointments using python", "authors": "1", "date_created": "2017-05-19 18:57:00", "date_published": "2017-05-19 18:57:00", "date_updated": "2017-05-20 17:27:08", "is_published": "1", "short_description": "Retrieving GNIB appointments using python", "tags": "137,85,136", "slug": "getting-appointments-using-python", "body_type": "markdown", "body_text": "<blockquote>\n<p><strong>recap:</strong> In the previous posts, I managed to get the available appointments\nusing <code>javascript</code> in the browser and through a <code>bash</code> script.</p>\n<p><strong>source</strong>: hosted on Github <a href=\"https://github.com/coolharsh55/GNIBappointments/\">here</a></p>\n<p><strong>webapp</strong>: hosted on Heroku <a href=\"https://gnibappt.herokuapp.com/\">here</a>\nshows available timings for GNIB and Visa appointments</p>\n</blockquote>\n<p>Following up with a <code>python</code> based script that uses the \n<a href=\"python-requests.org\">requests</a> library for the GET request and then\nparses the response to print available appointment times.</p>\n<h2 id=\"requests\">requests</h2>\n<p><code>requests</code> is a great library, which should be used more often by everyone,\nand one of the default installations on any system. The only reason it is \nnot in the standard library is because there is no real need for it to be.\nBesides, by being outside the standard collection, it can iterate at a \ndifferent pace. It makes requests really simple to read and make.</p>\n<p>Specifying parameters becomes easy as they are delcared as a collection.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">params = (\n    ('openpage', ''),  # BLANK\n    ('dt', ''),  # PARSED, but is always blank\n    ('cat', 'Study'),  # Category\n    ('sbcat', 'All'),  # Sub-Category\n    ('typ', 'Renewal'),  # Type\n)</code></pre>\n\n\n<p>Similarly, headers are a <code>dict</code> which allows us to specify the <code>key: value</code> pairs\nto be sent as headers.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">headers = {\n    'User-agent': 'script/python',\n    'Accept': '*/*',  # CORS\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Accept-Encoding': 'gzip, deflate, br',\n    'Origin': 'null',  # CORS\n    'Connection': 'keep-alive',\n}</code></pre>\n\n\n<p>Making the request is calling the URL with the parameters and headers passed to it.</p>\n<pre class=\"codehilite\"><code class=\"language-python\">response = requests.get(\n    'https://burghquayregistrationoffice.inis.gov.ie/'\n    + 'Website/AMSREG/AMSRegWeb.nsf/(getAppsNear)',\n    headers=headers, params=params)</code></pre>\n\n\n<h2 id=\"dealing-with-sslhttps\">Dealing with SSL/HTTPS</h2>\n<p>As the SSL/HTTPS is an issue with the GNIB endpoints, I've stumbled upon\nthe following solution. The requests library doesn't contain the particular\ncipher (or how the encryption is encoded). The particular cipher in use\ncan be gleamed by studying the output of <code>curl --verbose</code> and then finding the \ncorresponding entry in <a href=\"https://testssl.sh/openssl-rfc.mapping.html\">this</a>\ntable. For this particular case, it happened to be <code>DES-CBC3-SHA</code>.</p>\n<pre class=\"codehilite\"><code class=\"language-python\"># Looked up using curl --verbose\nrequests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':DES-CBC3-SHA'\n# disable SSL warning\nrequests.packages.urllib3.disable_warnings(\n    requests.packages.urllib3.exceptions.InsecureRequestWarning)</code></pre>\n\n\n<h2 id=\"response\">response</h2>\n<p>The response status code tells us whether the request was sucessfull or not.\nFor this, check the value of <code>response.status_code</code>, which should be <code>2xx</code>.</p>\n<h2 id=\"parsing-json\">Parsing JSON</h2>\n<p>This part is pretty much simple as covered in the previous post.\nSome additional checks for error conditions out of the way,\nthe appointments can be printed from the response.</p>\n<pre class=\"codehilite\"><code class=\"language-python\"># sanity checks\ndata = response.json()\n# error key is set\nif data.get('error', None) is not None:\n    raise Exception('ERROR: %s' % data['error'])\n\n# If there are no appointments, then the empty key is set\nif data.get('empty', None) is not None:\n    print('No appointments available')\n    sys.exit(0)\n\n# There are appointments, and are in the key 'slots'\ndata = data.get('slots', None)\nif data is None:\n    raise Exception('Data is NULL')\n\n# This should not happen, but a good idea to check it anyway\nif len(data) == 0:\n    print('No appointments available')\n    sys.exit(0)\n\n# print appointments\n# Format is:\n# {\n#   'id': 'str',\n#   'time': 'str'\n# }\nfor appointment in data:\n    print(appointment['time'])</code></pre>\n\n\n<h2 id=\"future-work\">Future Work</h2>\n<ul>\n<li>Online website that runs the script frequently and allows checking appointment\ntimings online</li>\n<li>Chrome extension that shows appointments</li>\n</ul>", "body": "> **recap:** In the previous posts, I managed to get the available appointments\r\nusing `javascript` in the browser and through a `bash` script.\r\n\r\n> **source**: hosted on Github [here](https://github.com/coolharsh55/GNIBappointments/)\r\n\r\n> **webapp**: hosted on Heroku [here](https://gnibappt.herokuapp.com/)\r\nshows available timings for GNIB and Visa appointments\r\n\r\nFollowing up with a `python` based script that uses the \r\n[requests](python-requests.org) library for the GET request and then\r\nparses the response to print available appointment times.\r\n\r\n## requests\r\n\r\n`requests` is a great library, which should be used more often by everyone,\r\nand one of the default installations on any system. The only reason it is \r\nnot in the standard library is because there is no real need for it to be.\r\nBesides, by being outside the standard collection, it can iterate at a \r\ndifferent pace. It makes requests really simple to read and make.\r\n\r\nSpecifying parameters becomes easy as they are delcared as a collection.\r\n\r\n```python\r\nparams = (\r\n    ('openpage', ''),  # BLANK\r\n    ('dt', ''),  # PARSED, but is always blank\r\n    ('cat', 'Study'),  # Category\r\n    ('sbcat', 'All'),  # Sub-Category\r\n    ('typ', 'Renewal'),  # Type\r\n)\r\n```\r\n\r\nSimilarly, headers are a `dict` which allows us to specify the `key: value` pairs\r\nto be sent as headers.\r\n\r\n```python\r\nheaders = {\r\n    'User-agent': 'script/python',\r\n    'Accept': '*/*',  # CORS\r\n    'Accept-Language': 'en-US,en;q=0.5',\r\n    'Accept-Encoding': 'gzip, deflate, br',\r\n    'Origin': 'null',  # CORS\r\n    'Connection': 'keep-alive',\r\n}\r\n```\r\n\r\nMaking the request is calling the URL with the parameters and headers passed to it.\r\n\r\n```python\r\nresponse = requests.get(\r\n    'https://burghquayregistrationoffice.inis.gov.ie/'\r\n    + 'Website/AMSREG/AMSRegWeb.nsf/(getAppsNear)',\r\n    headers=headers, params=params)\r\n```\r\n\r\n## Dealing with SSL/HTTPS\r\n\r\nAs the SSL/HTTPS is an issue with the GNIB endpoints, I've stumbled upon\r\nthe following solution. The requests library doesn't contain the particular\r\ncipher (or how the encryption is encoded). The particular cipher in use\r\ncan be gleamed by studying the output of `curl --verbose` and then finding the \r\ncorresponding entry in [this](https://testssl.sh/openssl-rfc.mapping.html)\r\ntable. For this particular case, it happened to be `DES-CBC3-SHA`.\r\n\r\n```python\r\n# Looked up using curl --verbose\r\nrequests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':DES-CBC3-SHA'\r\n# disable SSL warning\r\nrequests.packages.urllib3.disable_warnings(\r\n    requests.packages.urllib3.exceptions.InsecureRequestWarning)\r\n```\r\n\r\n## response\r\n\r\nThe response status code tells us whether the request was sucessfull or not.\r\nFor this, check the value of `response.status_code`, which should be `2xx`.\r\n\r\n## Parsing JSON\r\n\r\nThis part is pretty much simple as covered in the previous post.\r\nSome additional checks for error conditions out of the way,\r\nthe appointments can be printed from the response.\r\n\r\n```python\r\n# sanity checks\r\ndata = response.json()\r\n# error key is set\r\nif data.get('error', None) is not None:\r\n    raise Exception('ERROR: %s' % data['error'])\r\n\r\n# If there are no appointments, then the empty key is set\r\nif data.get('empty', None) is not None:\r\n    print('No appointments available')\r\n    sys.exit(0)\r\n\r\n# There are appointments, and are in the key 'slots'\r\ndata = data.get('slots', None)\r\nif data is None:\r\n    raise Exception('Data is NULL')\r\n\r\n# This should not happen, but a good idea to check it anyway\r\nif len(data) == 0:\r\n    print('No appointments available')\r\n    sys.exit(0)\r\n\r\n# print appointments\r\n# Format is:\r\n# {\r\n#   'id': 'str',\r\n#   'time': 'str'\r\n# }\r\nfor appointment in data:\r\n    print(appointment['time'])\r\n```\r\n\r\n## Future Work\r\n\r\n* Online website that runs the script frequently and allows checking appointment\r\ntimings online\r\n* Chrome extension that shows appointments", "headerimage": "", "highlight": "0", "section": 3}, {"id": 4, "title": "Getting appointments using bash", "authors": "1", "date_created": "2017-05-19 17:58:00", "date_published": "2017-05-19 17:58:00", "date_updated": "2017-05-20 17:26:56", "is_published": "1", "short_description": "Retrieving GNIB appointments using a bash scripr", "tags": "135,137,85,136", "slug": "getting-appointments-using-bash", "body_type": "markdown", "body_text": "<blockquote>\n<p><strong>recap:</strong> In the previous post, I managed to get the available appointments\nby pasting a bit of javascript that used GET to retrieve the available \nappointments through the browser console.</p>\n<p><strong>source</strong>: hosted on Github <a href=\"https://github.com/coolharsh55/GNIBappointments/\">here</a></p>\n<p><strong>webapp</strong>: hosted on Heroku <a href=\"https://gnibappt.herokuapp.com/\">here</a>\nshows available timings for GNIB and Visa appointments</p>\n</blockquote>\n<p>In this post, I describe further efforts that led to a <code>bash / python</code>\nscript that pulled the appointments. The advantages of a script is that\nit can be automated and run anywhere. So there are possibilities such as\nrunning the script on a server every <em>30 mins</em> and then sending an email\nor some form of notification when a new appointment becomes available.</p>\n<p>The script should not be too complicated and must be as simple as possible\nto run. Ideally something like this (actual output):</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">$harsh@XNMPRO:scripts &gt;./query_gnib_appointments.py\n4 July 2017 - 10:00\n21 July 2017 - 08:00</code></pre>\n\n\n<h2 id=\"script\">script</h2>\n<p>Bash is the defacto language scripts are written in (usually) and these days\nso is python. I also happen to have taken a <em>liking</em> to python, which is why\nI tend to use it for pretty much <em>everything</em>. </p>\n<h2 id=\"making-the-request\">making the request</h2>\n<p>That being said, bash uses <code>curl</code> which is an utility to make web requests.\nThe URL from the previous post was \n<code>https://burghquayregistrationoffice.inis.gov.ie/Website/AMSREG/AMSRegWeb.nsf/(getAppsNear)</code>.\nSuffixing it with the required GET parameters, we get\n<code>http://burghquayregistrationoffice.inis.gov.ie/Website/AMSREG/AMSRegWeb.nsf/(getAppsNear)?openpage=&amp;dt=&amp;cat=Study&amp;sbcat=All&amp;typ=Renewal</code>.\nFor brevity, I'm going to use just <code>URL</code> instead of the large wad of characters.</p>\n<h3 id=\"ssl-verification\">SSL verification</h3>\n<p>Using the redirect/location flag <code>-L</code>, the output produced was:</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">$harsh@XNMPRO:scripts &gt;curl -L &quot;URL&quot;\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\nMore details here: https://curl.haxx.se/docs/sslcerts.html\n\ncurl performs SSL certificate verification by default, using a &quot;bundle&quot;\n of Certificate Authority (CA) public keys (CA certs). If the default\n bundle file isn't adequate, you can specify an alternate file\n using the --cacert option.\nIf this HTTPS server uses a certificate signed by a CA represented in\n the bundle, the certificate verification probably failed due to a\n problem with the certificate (it might be expired, or the name might\n not match the domain name in the URL).\nIf you'd like to turn off curl's verification of the certificate, use\n the -k (or --insecure) option.</code></pre>\n\n\n<p>I'm not much knowledgeable about how SSL works at this level, so I\njust digged through the manpage to find the <code>-k</code> flag which disables\nSSL protection. Additionally, I used the <code>-s</code> flag to suppress progress\nbars from polluting the output. Using it yielded the response required:</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">$harsh@XNMPRO:scripts &gt;curl -L -k &quot;URL&quot;\n{&quot;slots&quot;:[{&quot;time&quot;:&quot;21 July 2017 - 08:00&quot;, &quot;id&quot;:&quot;5D2DE14F0B06EFF8802581250031862F&quot;}]}</code></pre>\n\n\n<h2 id=\"parsing-json\">Parsing JSON</h2>\n<p>If you can get away with manually eyeballing the response to get available\nappointment times, great. But I need <em>precisely</em> just the appointment timings.\nWhich meant that I needed some way to parse that JSON response. So many tools,\nand all of the pre-installed languages, all have JSON handling libraries. Take your pick. I found a good variety on \n<a href=\"http://stackoverflow.com/questions/1955505/parsing-json-with-unix-tools\">stackoverflow</a>.</p>\n<h3 id=\"jq\">jq</h3>\n<p>Working in shell scripts, <a href=\"https://stedolan.github.io/jq/\">jq</a> is a great tool\nfor working with JSON. It can be used as simply as:</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">$harsh@XNMPRO:scripts &gt;curl -L -k &quot;URL&quot; | jq &quot;.slots | .[] | .time&quot;\n&quot;21 July 2017 - 08:00&quot;</code></pre>\n\n\n<p><code>jq</code> uses syntax for selecting elements from JSON. The <code>.slots</code> instructs\nit to extract the value at that particular key. <code>|</code> (pipe) works as a filter\nand passes the data extracted from one part onto the next. <code>.[]</code> is an array\noperator, it acts as a <code>for ... each</code> on the array elements. <code>.time</code> then\nextracts the value for the appointment and prints it out.</p>\n<h3 id=\"grep\">grep</h3>\n<p>Using pattern matching, it is possible to only print out those lines\nthat contain the word \"time\".</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">$harsh@XNMPRO:scripts &gt;curl -L -k &quot;URL&quot; | grep -Po '&quot;time&quot;:.*?[^\\\\]&quot;,'\n&quot;time&quot;:&quot;21 July 2017 - 08:00&quot;,</code></pre>\n\n\n<h3 id=\"python\">python</h3>\n<p>After those two, it seems a bit silly to use python, but that's what I did\nanyway. In python, it is possible to execute a script inline using the\n<code>-c</code> command. The full script to parse out this particular JSON is:</p>\n<pre class=\"codehilite\"><code class=\"language-python\">import sys\nimport json\ndata = json.load(sys.stdin)\nif data.get(&quot;error&quot;, None) is not None:\n    raise Exception(&quot;ERROR: %s&quot; % data[&quot;error&quot;])\n# If there are no appointments, then the empty key is set\nif data.get(&quot;empty&quot;, None) is not None:\n    print(&quot;No appointments available&quot;)\n    sys.exit(0)\n# There are appointments, and are in the key &quot;slots&quot;\ndata = data.get(&quot;slots&quot;, None)\nif data is None:\n    raise Exception(&quot;Data is NULL&quot;)\n# This should not happen, but a good idea to check it anyway\nif len(data) == 0:\n    print(&quot;No appointments available&quot;)\n    sys.exit(0)\n# print appointments\n# Format is:\n# {\n#   &quot;id&quot;: &quot;str&quot;,\n#   &quot;time&quot;: &quot;str&quot;\n# }\nfor appointment in data:\n    print(appointment[&quot;time&quot;])</code></pre>\n\n\n<p>To use this, just put everything between triple-quotes <code>'''</code> as this\nconstitutes a multiline comment/statement in python.</p>\n<pre class=\"codehilite\"><code class=\"language-bash\">$harsh@XNMPRO:scripts &gt;curl -L -k &quot;URL&quot; | python '''\n# program goes here\n'''</code></pre>\n\n\n<h2 id=\"future-work\">Future work</h2>\n<ul>\n<li>Python script to do everything from request to parsing</li>\n<li>Chrome extension of some sorts</li>\n<li>Put the script up on a website for easier access to appointments</li>\n</ul>", "body": "> **recap:** In the previous post, I managed to get the available appointments\r\nby pasting a bit of javascript that used GET to retrieve the available \r\nappointments through the browser console.\r\n\r\n> **source**: hosted on Github [here](https://github.com/coolharsh55/GNIBappointments/)\r\n\r\n> **webapp**: hosted on Heroku [here](https://gnibappt.herokuapp.com/)\r\nshows available timings for GNIB and Visa appointments\r\n\r\nIn this post, I describe further efforts that led to a `bash / python`\r\nscript that pulled the appointments. The advantages of a script is that\r\nit can be automated and run anywhere. So there are possibilities such as\r\nrunning the script on a server every *30 mins* and then sending an email\r\nor some form of notification when a new appointment becomes available.\r\n\r\nThe script should not be too complicated and must be as simple as possible\r\nto run. Ideally something like this (actual output):\r\n\r\n```bash\r\n$harsh@XNMPRO:scripts >./query_gnib_appointments.py\r\n4 July 2017 - 10:00\r\n21 July 2017 - 08:00\r\n```\r\n## script\r\n\r\nBash is the defacto language scripts are written in (usually) and these days\r\nso is python. I also happen to have taken a *liking* to python, which is why\r\nI tend to use it for pretty much *everything*. \r\n\r\n## making the request\r\n\r\nThat being said, bash uses `curl` which is an utility to make web requests.\r\nThe URL from the previous post was \r\n`https://burghquayregistrationoffice.inis.gov.ie/Website/AMSREG/AMSRegWeb.nsf/(getAppsNear)`.\r\nSuffixing it with the required GET parameters, we get\r\n`http://burghquayregistrationoffice.inis.gov.ie/Website/AMSREG/AMSRegWeb.nsf/(getAppsNear)?openpage=&dt=&cat=Study&sbcat=All&typ=Renewal`.\r\nFor brevity, I'm going to use just `URL` instead of the large wad of characters.\r\n\r\n### SSL verification\r\n\r\nUsing the redirect/location flag `-L`, the output produced was:\r\n\r\n```bash\r\n$harsh@XNMPRO:scripts >curl -L \"URL\"\r\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\r\nMore details here: https://curl.haxx.se/docs/sslcerts.html\r\n\r\ncurl performs SSL certificate verification by default, using a \"bundle\"\r\n of Certificate Authority (CA) public keys (CA certs). If the default\r\n bundle file isn't adequate, you can specify an alternate file\r\n using the --cacert option.\r\nIf this HTTPS server uses a certificate signed by a CA represented in\r\n the bundle, the certificate verification probably failed due to a\r\n problem with the certificate (it might be expired, or the name might\r\n not match the domain name in the URL).\r\nIf you'd like to turn off curl's verification of the certificate, use\r\n the -k (or --insecure) option.\r\n```\r\n\r\nI'm not much knowledgeable about how SSL works at this level, so I\r\njust digged through the manpage to find the `-k` flag which disables\r\nSSL protection. Additionally, I used the `-s` flag to suppress progress\r\nbars from polluting the output. Using it yielded the response required:\r\n\r\n```bash\r\n$harsh@XNMPRO:scripts >curl -L -k \"URL\"\r\n{\"slots\":[{\"time\":\"21 July 2017 - 08:00\", \"id\":\"5D2DE14F0B06EFF8802581250031862F\"}]}\r\n```\r\n\r\n## Parsing JSON\r\n\r\nIf you can get away with manually eyeballing the response to get available\r\nappointment times, great. But I need *precisely* just the appointment timings.\r\nWhich meant that I needed some way to parse that JSON response. So many tools,\r\nand all of the pre-installed languages, all have JSON handling libraries. Take your pick. I found a good variety on \r\n[stackoverflow](http://stackoverflow.com/questions/1955505/parsing-json-with-unix-tools).\r\n\r\n### jq\r\n\r\nWorking in shell scripts, [jq](https://stedolan.github.io/jq/) is a great tool\r\nfor working with JSON. It can be used as simply as:\r\n\r\n```bash\r\n$harsh@XNMPRO:scripts >curl -L -k \"URL\" | jq \".slots | .[] | .time\"\r\n\"21 July 2017 - 08:00\"\r\n```\r\n\r\n`jq` uses syntax for selecting elements from JSON. The `.slots` instructs\r\nit to extract the value at that particular key. `|` (pipe) works as a filter\r\nand passes the data extracted from one part onto the next. `.[]` is an array\r\noperator, it acts as a `for ... each` on the array elements. `.time` then\r\nextracts the value for the appointment and prints it out.\r\n\r\n### grep\r\n\r\nUsing pattern matching, it is possible to only print out those lines\r\nthat contain the word \"time\".\r\n\r\n```bash\r\n$harsh@XNMPRO:scripts >curl -L -k \"URL\" | grep -Po '\"time\":.*?[^\\\\]\",'\r\n\"time\":\"21 July 2017 - 08:00\",\r\n```\r\n\r\n### python\r\n\r\nAfter those two, it seems a bit silly to use python, but that's what I did\r\nanyway. In python, it is possible to execute a script inline using the\r\n`-c` command. The full script to parse out this particular JSON is:\r\n\r\n```python\r\nimport sys\r\nimport json\r\ndata = json.load(sys.stdin)\r\nif data.get(\"error\", None) is not None:\r\n    raise Exception(\"ERROR: %s\" % data[\"error\"])\r\n# If there are no appointments, then the empty key is set\r\nif data.get(\"empty\", None) is not None:\r\n    print(\"No appointments available\")\r\n    sys.exit(0)\r\n# There are appointments, and are in the key \"slots\"\r\ndata = data.get(\"slots\", None)\r\nif data is None:\r\n    raise Exception(\"Data is NULL\")\r\n# This should not happen, but a good idea to check it anyway\r\nif len(data) == 0:\r\n    print(\"No appointments available\")\r\n    sys.exit(0)\r\n# print appointments\r\n# Format is:\r\n# {\r\n#   \"id\": \"str\",\r\n#   \"time\": \"str\"\r\n# }\r\nfor appointment in data:\r\n    print(appointment[\"time\"])\r\n```\r\n\r\nTo use this, just put everything between triple-quotes `'''` as this\r\nconstitutes a multiline comment/statement in python.\r\n\r\n```bash\r\n$harsh@XNMPRO:scripts >curl -L -k \"URL\" | python '''\r\n# program goes here\r\n'''\r\n```\r\n\r\n## Future work\r\n\r\n* Python script to do everything from request to parsing\r\n* Chrome extension of some sorts\r\n* Put the script up on a website for easier access to appointments", "headerimage": "", "highlight": "0", "section": 3}, {"id": 2, "title": "Full Stack Website Guide", "authors": "1", "date_created": "2017-05-17 20:11:00", "date_published": "2017-05-17 20:11:00", "date_updated": "2017-05-18 14:09:10", "is_published": "1", "short_description": "An overview of everything that goes into making a full stack website", "tags": "115,132,127,128,129,131,86,130,114,126,124,125,81,122,134,133,123,85", "slug": "full-stack-website-guide", "body_type": "markdown", "body_text": "<blockquote>\n<p><a href=\"https://s3-eu-west-1.amazonaws.com/harshp-media/dev/fullstack.png\">link to (full) image</a></p>\n</blockquote>\n<p>There is often the case about starting something where one wishes that\nthey had received some kind of advice or information about the topic.\nKind of an overview of what was what, and how everything fit together.\nThe '<em>big picture</em>'. This post is something similar on building a website.\nOver time, I have realised that there are a lot of components that are not\nnecessarily related, but are part of the big picture. Without following\nany particular tutorial, I just wanted to get an idea of what was what\nbefore actually starting anything. This post hopes to serve that itch\nby providing some brief explanations of how things stand. Hopefully,\nthis is helpful.</p>\n<p>Full-stack means two components - server (backend) and client (frontend). \nThe backend server is responsible for handling requests, \nwhich means deciding how the URL typed into the bar is handled \nand the correct page is sent back to the browser.\nThe frontend is responsible for taking the content and displaying it in the browser.\nThese are oversimplifications as the backend and frontend do a lot of other work as well.</p>\n<h2 id=\"domain\">Domain</h2>\n<p>The domain is the website, to put it concisely. \nIt is the part of the website that the browser goes looking for, and once it\nhas found it, asks it for specific pages.\nTO get a domain name, you can turn to any registrar, \nthough I am partial to <a href=\"https://www.gandi.net/\">gandi.net</a> as I have an account\nwith them and have found them to be competitively priced with excellent support.\nWith a domain name, you can set up subdomains such that <em>alpha.website.com </em>\nis handled by one server while <em>beta.website.com</em> is handled by another.</p>\n<h3 id=\"httpsssl\">HTTPS/SSL</h3>\n<p>This is the part where the domain name begins with <code>https://</code> instead of <code>http</code>.\nThe extra 's' at the end signifies secure, and informs that traffic to and from the \nwebsite is encrypted. In this day and age, there is NO reason to NOT use HTTPS.\n<a href=\"https://letsencrypt.org/\">LetsEncrypt</a> offers <strong>FREE</strong> SSL certificates, and they\nare a breeze to set up and renew. USE THEM. You may want to investigate this area\na little more if you are doing payments, shopping, or anything that requires extra\nsecurity on your website. But if it is just a simple website, then definitely\ngo and get that HTTPS in there. Again, it's free, so the only excuse you have\nis that you are lazy.</p>\n<h2 id=\"backend\">Backend</h2>\n<h3 id=\"choosing-the-infrastructure\">Choosing the infrastructure</h3>\n<p>There are two options depending on how much you want out of it\nand how much work you are willing to put. If you don't need the hassle of \nmanaging a server, you can choose <strong>PaaS (Platform as a Service)</strong> which\nmeans that someone else handles all the work of running a service for you.\nOr if you are comfortable handling your own server (which is a linux machine)\nthen you opt for <strong>IaaS (Infrastructure as a Service)</strong> where the company provides\nall the components and you combine them as you want.\nThink of it like PaaS being like a pre-made car that you can use as long as you\nknow driving and IaaS being like given the engine, chassis, wheels, and other\ncomponents where you have to engineer a car to your needs and then use it.</p>\n<h4 id=\"paas\">PaaS</h4>\n<p>The top two PaaS offerings are <a href=\"https://aws.amazon.com/de/elasticbeanstalk/\">Elastic Beanstalk</a> and\n<a href=\"https://cloud.google.com/appengine/\">Google App Engine</a>.\nOf these, Google App Engine is much more documented and has better pricing options.\nThe nice thing about PaaS is that you only have to focus on how your pages are \nbeing generated (more on that later), whereas with IaaS you have to worry about\nthings like how the database is stored and configured, how much RAM you are using,\nhow the web server is being utilised - so a lot of low level details. There is also\n<a href=\"https://www.heroku.com/\">heroku</a> which offers free instances with a caveat on\nhow much time they can be used for (in a day).</p>\n<h4 id=\"iaas\">IaaS</h4>\n<p>In IaaS, you get a virtual machine (server) which is mostly a linux box\nconfigured up to act as a processor with some specified RAM. \nI've been running my website (this one, yes) for more than a year now on AWS or \n<a href=\"https://aws.amazon.com/\">Amazon Web Services</a> or more specifically <strong>EC2</strong>. \nRecently, I had a go at porting it to <a href=\"https://cloud.google.com/\">Google Cloud</a> or <strong>Google Compute Engine</strong>.\nWith these, I can select a pre-configured VM (1 vCPU) and RAM (0.5GB) and the\nprovider will provision these for me.\nApart from these, there are also <a href=\"https://www.digitalocean.com/\">DigitalOcean</a>\nand <a href=\"https://amazonlightsail.com/\">Amazon Lightsail</a> which offer IaaS in\npre-configured setups which cannot be changed (or mixed and matched).</p>\n<h4 id=\"setting-up-the-domain\">Setting up the domain</h4>\n<p>When these instances are provisioned or provided, they are given an internal IP\naddress which makes them accessible from <em>within</em> the provider's network. \nThis internal IP can <em>change</em> which makes it non-ideal for web stuff because\nthe website needs a fixed address.\nTo get these working with our domain, we need a <em>static</em> IP address, which\nmeans that it won't change. In some providers, this <em>can</em> cost extra,\nbut most of them provide one free of charge with a IaaS instance.</p>\n<h3 id=\"web-server\">Web Server</h3>\n<p>Once you have an IaaS instance, you need to configure how the page requests\nare handled, what are valid URLs and so on. For this a web server such as \n<a href=\"https://httpd.apache.org/\">Apache</a> or <a href=\"https://nginx.org/en/\">Nginx</a> \n(pronounced engineX) come in handy. These sit in \nas the first recipient of requests that come in to the server. Traditionally,\nApache rules the roost of web servers. However, I use Nginx as it is easy to\nconfigure and set up.</p>\n<p>You can have multiple 'apps' for different URLs. For example,\nyou can configure <em>website.com/blog</em> to be handled by a different app than\n<em>website.com/project</em>. The interaction between the web server and a \nweb framework happens through a <em>port</em> or a <em>socket</em>. Generally,\nit is preferably to have sockets as they can be configured to have better\nsecurity in terms of access.</p>\n<h3 id=\"web-framework\">Web Framework</h3>\n<p>A web framework is a collection of utilities (or software, take your pick of a fancy term)\nwhich handles the page request <em>after</em> the web server has decided that the URL request\nis valid. This is where your <em>favourite</em> programming language comes in. \nThere exist web frameworks for pretty much all languages, even C.\nI use <a href=\"https://www.djangoproject.com/\">Django</a>, \nwhich is a <a href=\"https://www.python.org/\">python</a> web framework. I also use other\npython-based frameworks such as <a href=\"http://flask.pocoo.org/\">flask</a> and \n<a href=\"https://bottlepy.org/docs/dev/\">bottle</a>. </p>\n<p>The web framework is the place where you get control of the actual request\nand how to handle it. Things such as storing and retrieving requests from a database,\nor whether to return a 404 or text content. Some web frameworks are <em>heavy</em> or <em>featureful</em>\nin a way similar to how IaaS and PaaS compare. Some are bare bones and require you\nto add/mix components. \nOr there is a third <em>simpler</em> option of using a static HTML file generator that\nwill generate all the files once (so no random component) and then you serve\nit using any method from IaaS or PaaS.\nOne good static HTML generator is <a href=\"https://jekyllrb.com/\">Jekyll</a>.</p>\n<p>Start with the programming language. What's your favourite programming language?\nIs it java? Or perhaps ruby? Or python? Or c#? Whichever it is, that will decide\nwhat web framework you are using. If you're in this for a learning experience,\ngo for a minimal framework. Read around. See what people say. Most often,\npeople will comment (appreciatively) about how they find a minimal framework\nbetter because they can add just the features they want. For e.g. Django\ncomes with database ORM built in, which makes database handling a breeze.\nFlask, by comparison, does not come with any ORM. But if I wanted to, I could\nadd one using <a href=\"https://www.sqlalchemy.org/\">SQLAlchemy</a>. If you want to get up and running as quickly as\npossible, look for what 'beginners' should use - that's usually the most\neasy and quick to set up web framework.</p>\n<h3 id=\"database\">Database</h3>\n<h4 id=\"the-craze-for-nosql\">The craze for NoSQL</h4>\n<p>You <strong>don't </strong> need NoSQL. Let me repeat that again. YOU DON'T NEED NoSQL. \nUnless you really do, of course. It is more of a hyped term that has caught\non and people want to be on the gravy train. The thing with NoSQL is that \nit is good for really niche cases which 99% websites don't need. So stick\nwith a general RDBMS, and they're well oiled with web frameworks and\nweb servers and services, so you can work with them out of the box.</p>\n<h4 id=\"okay-which-database\">Okay, which database?</h4>\n<p><a href=\"https://www.postgresql.org/\">PostgreSQL</a>, of course. It is open, free, and fully compliant.\nIf for some reason, you feel the need for <em>MySQL</em>, please instead\nopt to use <a href=\"https://mariadb.org/\">MariaDB</a>, an open fork that is better IMHO.</p>\n<h2 id=\"frontend\">Frontend</h2>\n<p>Now comes the part where the website is displayed in a browser. \nThe frontend is essentially how you decide the website is displayed.\nMy philosophy is always to use something only when it is <strong>really</strong> needed.\nSo my first opinion would be to not decide on using <em>react</em> or <em>angular</em>\njust because everyone seems to be using it.</p>\n<p>Think of the basics. <code>HTML + CSS + JS</code>. HTML by itself means \n<a href=\"https://www.w3.org/TR/html5/\">HTML5</a> now.\nBeyond that, you don't need anything. The others, however, need better\nconsiderations.</p>\n<h3 id=\"css-frameworks\">CSS Frameworks</h3>\n<p>There's a repeating pattern here, of things that work great 'out-of-the-box',\nbut are 'big' and 'clunky'. In CSS, you have heavyweight frameworks such as \n<a href=\"http://getbootstrap.com/\">Bootstrap</a>, <a href=\"http://foundation.zurb.com/\">Foundation</a>, \nand <a href=\"https://semantic-ui.com/\">Semantic UI</a> which have enjoyed years of popularity\nand use. Apart from these, there are lightweight frameworks such as \n<a href=\"http://bulma.io/\">Bulma</a>, <a href=\"https://purecss.io/\">Pure</a>, \n<a href=\"http://getskeleton.com/\">Skeleton</a>, \n<a href=\"http://tachyons.io/\">Tachyons</a>, and so many others that work well at a\nminimalist level. Personally, I use Tachyons, which is a fancy way to write\ninline CSS. It works for my preferences, which is to enable writing CSS\nwhen I write blog posts. \nThere are other better posts describing how to select CSS frameworks, so I\nshall not presume about adding any wisdom to them.</p>\n<h3 id=\"js-frameworks\">JS Frameworks</h3>\n<p>The <strong>hot</strong> area of current trendy conversations. \nIt used to be <a href=\"https://jquery.com/\">JQuery</a> just a few years back. Now there are so many \noptions that it becomes confusing just how to differentiate their\naims and applications from each other.</p>\n<p>There is a good website, \n<a href=\"http://youmightnotneedjquery.com/\">you might not need jquery</a> that explains in\nquite an unusual way how JavaScript by itself is a powerful language\nand can do what you want concisely, or other smaller libraries\nthat offer just the right feature-set so that you don't have to use\njquery, which is 'heavy'. BTW, using plain javascript is now called\n<a href=\"http://vanilla-js.com/\">VanillaJS</a> because pure javascript just\n<em>had</em> to jump on the fancy naming bandwaggon (that's a gag/parody/satire).</p>\n<p>Frameworks such as <a href=\"https://facebook.github.io/react/\">React</a> \nand <a href=\"https://angularjs.org/\">Angular</a> solve a problem, so if you don't\nhave that problem, don't create another one by using them.\nThat being said, they are great to use. Another interesting\nframework is <a href=\"https://vuejs.org/\">Vue.js</a> which is lighter than React and Angular, and\noffers similar features.</p>\n<p>If all you want is a decent looking website / blog, then plain javascript\nand a simpler CSS framework should suffice. At most, you can go for\njquery (or the other minimal libraries that do the work you need).\nIf you have complex needs such as dynamic views, and shopping carts,\nand analytics, and dashboards; then it is a good idea to familiarise\nyourself with what the other frameworks do before taking the plunge.</p>\n<blockquote>\n<p><em>Footnote: This is not meant to be an exhaustive guide or tutorial. I wrote this down more as a documentation of what goes around whenever someone starts this discussion about full stack tools.</em></p>\n</blockquote>", "body": "> [link to (full) image](https://s3-eu-west-1.amazonaws.com/harshp-media/dev/fullstack.png)\r\n\r\nThere is often the case about starting something where one wishes that\r\nthey had received some kind of advice or information about the topic.\r\nKind of an overview of what was what, and how everything fit together.\r\nThe '_big picture_'. This post is something similar on building a website.\r\nOver time, I have realised that there are a lot of components that are not\r\nnecessarily related, but are part of the big picture. Without following\r\nany particular tutorial, I just wanted to get an idea of what was what\r\nbefore actually starting anything. This post hopes to serve that itch\r\nby providing some brief explanations of how things stand. Hopefully,\r\nthis is helpful.\r\n\r\nFull-stack means two components - server (backend) and client (frontend). \r\nThe backend server is responsible for handling requests, \r\nwhich means deciding how the URL typed into the bar is handled \r\nand the correct page is sent back to the browser.\r\nThe frontend is responsible for taking the content and displaying it in the browser.\r\nThese are oversimplifications as the backend and frontend do a lot of other work as well.\r\n\r\n\r\n## Domain\r\n\r\nThe domain is the website, to put it concisely. \r\nIt is the part of the website that the browser goes looking for, and once it\r\nhas found it, asks it for specific pages.\r\nTO get a domain name, you can turn to any registrar, \r\nthough I am partial to [gandi.net](https://www.gandi.net/) as I have an account\r\nwith them and have found them to be competitively priced with excellent support.\r\nWith a domain name, you can set up subdomains such that *alpha.website.com *\r\nis handled by one server while *beta.website.com* is handled by another.\r\n\r\n### HTTPS/SSL\r\n\r\nThis is the part where the domain name begins with `https://` instead of `http`.\r\nThe extra 's' at the end signifies secure, and informs that traffic to and from the \r\nwebsite is encrypted. In this day and age, there is NO reason to NOT use HTTPS.\r\n[LetsEncrypt](https://letsencrypt.org/) offers **FREE** SSL certificates, and they\r\nare a breeze to set up and renew. USE THEM. You may want to investigate this area\r\na little more if you are doing payments, shopping, or anything that requires extra\r\nsecurity on your website. But if it is just a simple website, then definitely\r\ngo and get that HTTPS in there. Again, it's free, so the only excuse you have\r\nis that you are lazy.\r\n\r\n## Backend\r\n\r\n### Choosing the infrastructure\r\n\r\nThere are two options depending on how much you want out of it\r\nand how much work you are willing to put. If you don't need the hassle of \r\nmanaging a server, you can choose **PaaS (Platform as a Service)** which\r\nmeans that someone else handles all the work of running a service for you.\r\nOr if you are comfortable handling your own server (which is a linux machine)\r\nthen you opt for **IaaS (Infrastructure as a Service)** where the company provides\r\nall the components and you combine them as you want.\r\nThink of it like PaaS being like a pre-made car that you can use as long as you\r\nknow driving and IaaS being like given the engine, chassis, wheels, and other\r\ncomponents where you have to engineer a car to your needs and then use it.\r\n\r\n#### PaaS\r\n\r\nThe top two PaaS offerings are [Elastic Beanstalk](https://aws.amazon.com/de/elasticbeanstalk/) and\r\n[Google App Engine](https://cloud.google.com/appengine/).\r\nOf these, Google App Engine is much more documented and has better pricing options.\r\nThe nice thing about PaaS is that you only have to focus on how your pages are \r\nbeing generated (more on that later), whereas with IaaS you have to worry about\r\nthings like how the database is stored and configured, how much RAM you are using,\r\nhow the web server is being utilised - so a lot of low level details. There is also\r\n[heroku](https://www.heroku.com/) which offers free instances with a caveat on\r\nhow much time they can be used for (in a day).\r\n\r\n#### IaaS\r\n\r\nIn IaaS, you get a virtual machine (server) which is mostly a linux box\r\nconfigured up to act as a processor with some specified RAM. \r\nI've been running my website (this one, yes) for more than a year now on AWS or \r\n[Amazon Web Services](https://aws.amazon.com/) or more specifically **EC2**. \r\nRecently, I had a go at porting it to [Google Cloud](https://cloud.google.com/) or **Google Compute Engine**.\r\nWith these, I can select a pre-configured VM (1 vCPU) and RAM (0.5GB) and the\r\nprovider will provision these for me.\r\nApart from these, there are also [DigitalOcean](https://www.digitalocean.com/)\r\nand [Amazon Lightsail](https://amazonlightsail.com/) which offer IaaS in\r\npre-configured setups which cannot be changed (or mixed and matched).\r\n\r\n#### Setting up the domain\r\nWhen these instances are provisioned or provided, they are given an internal IP\r\naddress which makes them accessible from *within* the provider's network. \r\nThis internal IP can *change* which makes it non-ideal for web stuff because\r\nthe website needs a fixed address.\r\nTo get these working with our domain, we need a *static* IP address, which\r\nmeans that it won't change. In some providers, this *can* cost extra,\r\nbut most of them provide one free of charge with a IaaS instance.\r\n\r\n### Web Server\r\n\r\nOnce you have an IaaS instance, you need to configure how the page requests\r\nare handled, what are valid URLs and so on. For this a web server such as \r\n[Apache](https://httpd.apache.org/) or [Nginx](https://nginx.org/en/) \r\n(pronounced engineX) come in handy. These sit in \r\nas the first recipient of requests that come in to the server. Traditionally,\r\nApache rules the roost of web servers. However, I use Nginx as it is easy to\r\nconfigure and set up.\r\n\r\nYou can have multiple 'apps' for different URLs. For example,\r\nyou can configure *website.com/blog* to be handled by a different app than\r\n*website.com/project*. The interaction between the web server and a \r\nweb framework happens through a *port* or a *socket*. Generally,\r\nit is preferably to have sockets as they can be configured to have better\r\nsecurity in terms of access.\r\n\r\n### Web Framework\r\n\r\nA web framework is a collection of utilities (or software, take your pick of a fancy term)\r\nwhich handles the page request *after* the web server has decided that the URL request\r\nis valid. This is where your *favourite* programming language comes in. \r\nThere exist web frameworks for pretty much all languages, even C.\r\nI use [Django](https://www.djangoproject.com/), \r\nwhich is a [python](https://www.python.org/) web framework. I also use other\r\npython-based frameworks such as [flask](http://flask.pocoo.org/) and \r\n[bottle](https://bottlepy.org/docs/dev/). \r\n\r\nThe web framework is the place where you get control of the actual request\r\nand how to handle it. Things such as storing and retrieving requests from a database,\r\nor whether to return a 404 or text content. Some web frameworks are *heavy* or *featureful*\r\nin a way similar to how IaaS and PaaS compare. Some are bare bones and require you\r\nto add/mix components. \r\nOr there is a third *simpler* option of using a static HTML file generator that\r\nwill generate all the files once (so no random component) and then you serve\r\nit using any method from IaaS or PaaS.\r\nOne good static HTML generator is [Jekyll](https://jekyllrb.com/).\r\n\r\nStart with the programming language. What's your favourite programming language?\r\nIs it java? Or perhaps ruby? Or python? Or c#? Whichever it is, that will decide\r\nwhat web framework you are using. If you're in this for a learning experience,\r\ngo for a minimal framework. Read around. See what people say. Most often,\r\npeople will comment (appreciatively) about how they find a minimal framework\r\nbetter because they can add just the features they want. For e.g. Django\r\ncomes with database ORM built in, which makes database handling a breeze.\r\nFlask, by comparison, does not come with any ORM. But if I wanted to, I could\r\nadd one using [SQLAlchemy](https://www.sqlalchemy.org/). If you want to get up and running as quickly as\r\npossible, look for what 'beginners' should use - that's usually the most\r\neasy and quick to set up web framework.\r\n\r\n### Database\r\n\r\n#### The craze for NoSQL\r\n\r\nYou **don't ** need NoSQL. Let me repeat that again. YOU DON'T NEED NoSQL. \r\nUnless you really do, of course. It is more of a hyped term that has caught\r\non and people want to be on the gravy train. The thing with NoSQL is that \r\nit is good for really niche cases which 99% websites don't need. So stick\r\nwith a general RDBMS, and they're well oiled with web frameworks and\r\nweb servers and services, so you can work with them out of the box.\r\n\r\n#### Okay, which database?\r\n\r\n[PostgreSQL](https://www.postgresql.org/), of course. It is open, free, and fully compliant.\r\nIf for some reason, you feel the need for *MySQL*, please instead\r\nopt to use [MariaDB](https://mariadb.org/), an open fork that is better IMHO.\r\n\r\n## Frontend\r\n\r\nNow comes the part where the website is displayed in a browser. \r\nThe frontend is essentially how you decide the website is displayed.\r\nMy philosophy is always to use something only when it is **really** needed.\r\nSo my first opinion would be to not decide on using *react* or *angular*\r\njust because everyone seems to be using it.\r\n\r\nThink of the basics. `HTML + CSS + JS`. HTML by itself means \r\n[HTML5](https://www.w3.org/TR/html5/) now.\r\nBeyond that, you don't need anything. The others, however, need better\r\nconsiderations.\r\n\r\n### CSS Frameworks\r\n\r\nThere's a repeating pattern here, of things that work great 'out-of-the-box',\r\nbut are 'big' and 'clunky'. In CSS, you have heavyweight frameworks such as \r\n[Bootstrap](http://getbootstrap.com/), [Foundation](http://foundation.zurb.com/), \r\nand [Semantic UI](https://semantic-ui.com/) which have enjoyed years of popularity\r\nand use. Apart from these, there are lightweight frameworks such as \r\n[Bulma](http://bulma.io/), [Pure](https://purecss.io/), \r\n[Skeleton](http://getskeleton.com/), \r\n[Tachyons](http://tachyons.io/), and so many others that work well at a\r\nminimalist level. Personally, I use Tachyons, which is a fancy way to write\r\ninline CSS. It works for my preferences, which is to enable writing CSS\r\nwhen I write blog posts. \r\nThere are other better posts describing how to select CSS frameworks, so I\r\nshall not presume about adding any wisdom to them.\r\n\r\n### JS Frameworks\r\n\r\nThe **hot** area of current trendy conversations. \r\nIt used to be [JQuery](https://jquery.com/) just a few years back. Now there are so many \r\noptions that it becomes confusing just how to differentiate their\r\naims and applications from each other.\r\n\r\nThere is a good website, \r\n[you might not need jquery](http://youmightnotneedjquery.com/) that explains in\r\nquite an unusual way how JavaScript by itself is a powerful language\r\nand can do what you want concisely, or other smaller libraries\r\nthat offer just the right feature-set so that you don't have to use\r\njquery, which is 'heavy'. BTW, using plain javascript is now called\r\n[VanillaJS](http://vanilla-js.com/) because pure javascript just\r\n*had* to jump on the fancy naming bandwaggon (that's a gag/parody/satire).\r\n\r\nFrameworks such as [React](https://facebook.github.io/react/) \r\nand [Angular](https://angularjs.org/) solve a problem, so if you don't\r\nhave that problem, don't create another one by using them.\r\nThat being said, they are great to use. Another interesting\r\nframework is [Vue.js](https://vuejs.org/) which is lighter than React and Angular, and\r\noffers similar features.\r\n\r\nIf all you want is a decent looking website / blog, then plain javascript\r\nand a simpler CSS framework should suffice. At most, you can go for\r\njquery (or the other minimal libraries that do the work you need).\r\nIf you have complex needs such as dynamic views, and shopping carts,\r\nand analytics, and dashboards; then it is a good idea to familiarise\r\nyourself with what the other frameworks do before taking the plunge.\r\n\r\n> _Footnote: This is not meant to be an exhaustive guide or tutorial. I wrote this down more as a documentation of what goes around whenever someone starts this discussion about full stack tools._", "headerimage": "https://s3-eu-west-1.amazonaws.com/harshp-media/dev/fullstack.png", "highlight": "0", "section": 2}, {"id": 1, "title": "Hello, World!", "authors": "1", "date_created": "2017-04-25 15:32:00", "date_published": "2017-04-25 15:32:00", "date_updated": "2017-04-25 15:40:22", "is_published": "1", "short_description": "First post, and it *had* to be a hello world program.", "tags": "113", "slug": "hello-world", "body_type": "markdown", "body_text": "<pre><code class=\"language-python\">\nprint(\"Hello, World!\")</code><pre>", "body": "<pre><code class=\"language-python\">\r\nprint(\"Hello, World!\")</code><pre>", "headerimage": "", "highlight": "0", "section": 1}]