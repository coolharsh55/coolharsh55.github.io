<!DOCTYPE html>
<html
    lang="en"
    prefix="schema: http://schema.org/ ">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>AI Cards: Towards an Applied Framework for Machine-Readable AI and Risk Documentation Inspired by the EU AI Act</title>
    <meta name="description" content=""/>
    <meta name="schema:name" content="AI Cards: Towards an Applied Framework for Machine-Readable AI and Risk Documentation Inspired by the EU AI Act">
    <meta name="schema:description" content="AI Cards is a novel holistic framework for representing intended uses of AI systems by encompassing information regarding technical specifications, context of use, and risk management, both in human- and machine-readable formats.">
    <meta name="schema:datePublished" content="item.schema_datePublished">
    <meta name="schema:keywords" content="AI-Act,ISO,risk,semantic-web,">
    <meta name="schema:author" content="https://harshp.com/me">
    <meta name="schema:identifier" content="https://harshp.com/research/publications/065-AI-Cards-Risk-Documentation-AI-Act">
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@coolharsh55">
    <meta name="twitter:creator" content="@coolharsh55">
    <meta property="og:url" content="">
    <meta property="og:title" content="AI Cards: Towards an Applied Framework for Machine-Readable AI and Risk Documentation Inspired by the EU AI Act">
    <meta property="og:description" content="AI Cards is a novel holistic framework for representing intended uses of AI systems by encompassing information regarding technical specifications, context of use, and risk management, both in human- and machine-readable formats.">
    <link rel="stylesheet" href="/css/sitebase.css" />
</head>
<body>
    <header><nav>
        <a href="/" property="schema:isPartOf" typeof="schema:Website">harshp.com</a> 
| <a href="/research">research</a> | <a href="/research/publications">publications</a>    </nav></header>
    <main>
    <article typeof="https://harshp.com/code/vocab#FullPaper https://harshp.com/code/vocab#RenderedItem https://schema.org/ScholarlyArticle " resource="https://harshp.com/research/publications/065-AI-Cards-Risk-Documentation-AI-Act">
        <h1 property="schema:name schema:headline">AI Cards: Towards an Applied Framework for Machine-Readable AI and Risk Documentation Inspired by the EU AI Act</h1>
<div id="description">
	<small>
	<time datetime="2024-09-01T00:00:00">2024-09-01T00:00:00</time>
    <i>Conference</i>
    <br/>
    <span class='note'>to be presented</span>
    Annual Privacy Forum (APF)    <br/>
    &#9997;<i>
    Delaram Golpayegani*
    ,
    Other Author(s)
    ,
    <u>Harshvardhan J. Pandit</u>
    ,
    Declan O&#39;Sullivan
    ,
    Dave Lewis
    </i>
    <br/>
        &#x1f513;copies:
        <a href="https://doi.org/10.48550/arXiv.2406.18211">arXiv</a>
        , <a href="https://harshp.com/research/publications/065-AI-Cards-Risk-Documentation-AI-Act">harshp.com</a>
        , <a href="https://osf.io/6dxgt">OSF</a>
    	<br/>
        &#128230;resources:
        <a href="https://github.com/delaramGlp/aicards">repo</a>
    <br/>
    AI Cards is a novel holistic framework for representing intended uses of AI systems by encompassing information regarding technical specifications, context of use, and risk management, both in human- and machine-readable formats.
    </small>
</div>
        <div id="content" property="schema:articleBody">
        <link rel="stylesheet" href="/css/toc.css" />
<div id="toc"></div>

<h2>Abstract</h2>

<h2 id="introduction">Introduction</h2>
<p>Artificial Intelligence (AI) has become a centrepiece of reshaping
many aspects of individual life, society, and public and private sectors
for the better <span class="citation"
data-cites="gruetzemacher2022transformative balahur2022data maragno2023exploring"><a
href="#ref-gruetzemacher2022transformative"
role="doc-biblioref">[1]</a>–<a href="#ref-maragno2023exploring"
role="doc-biblioref">[3]</a></span>. Yet, concerns have been raised
regarding the ethical implications and potential risks associated with
the use of AI, such as biases in decision-making algorithms <span
class="citation" data-cites="miron2021evaluating araujo2020ai"><a
href="#ref-miron2021evaluating" role="doc-biblioref">[4]</a>, <a
href="#ref-araujo2020ai" role="doc-biblioref">[5]</a></span> and privacy
issues <span class="citation"
data-cites="zhang2021ethics hupont2022landscape"><a
href="#ref-zhang2021ethics" role="doc-biblioref">[6]</a>, <a
href="#ref-hupont2022landscape" role="doc-biblioref">[7]</a></span>.
These concerns have prompted a global wave of trustworthy AI guidelines
<span class="citation" data-cites="OECD hlegtai"><a href="#ref-OECD"
role="doc-biblioref">[8]</a>, <a href="#ref-hlegtai"
role="doc-biblioref">[9]</a></span> and some preliminary regulatory
efforts aimed at mitigating the potential harms of AI, ensuring its
development and use are aligned with safety standards and respect human
rights and freedoms. The European Union’s (EU) AI Act <span
class="citation" data-cites="aiact"><a href="#ref-aiact"
role="doc-biblioref">[10]</a></span><a href="#fn1" class="footnote-ref"
id="fnref1" role="doc-noteref"><sup>1</sup></a> is the first
comprehensive legal framework on AI, through which AI systems are
subjected to a set of regulatory obligations according to their risk
level—that can be, from highest to lowest, <em>unacceptable</em>,
<em>high-risk</em>, <em>limited-risk</em>, or <em>minimal-risk</em>.</p>
<p>With the risk level being the yardstick for determining the
regulatory requirements that AI systems need to satisfy, the AI Act is
intent on promoting secure, trustworthy, and ethical use of AI, with a
particular emphasis on management and mitigation of potential harms that
high-risk AI systems may cause. The risk management system provisions,
set in Art. 9, thereby play a pivotal role in the implementation of the
Act. Effective AI risk management requires information regarding the AI
system, its incorporating components, its context of use, and its
potential risks to be maintained and communicated in the form of
<em>documented information</em>. While maintaining and sharing
information regarding AI systems and their risks promote transparency
and in turn trustworthiness <span class="citation"
data-cites="hupont2023documentinghighrisk"><a
href="#ref-hupont2023documentinghighrisk"
role="doc-biblioref">[11]</a></span>, it is additionally a legal
obligation for providers of high-risk AI systems to draw up technical
documentation to demonstrate compliance with the Act’s requirements
(Art. 11). The elements of technical documentation, which includes risk
management system, are described in Annex IV at a high-level. However,
to serve its purpose technical documentation needs to be extensive and
detailed. Being limited to defining essential requirements, the AI Act
relies on European <em>harmonised standards</em> for technical
specifications to help with implementation and enforcement of its legal
requirements, including technical documentation <span class="citation"
data-cites="mazzini2023proposal tartaro2023towards"><a
href="#ref-mazzini2023proposal" role="doc-biblioref">[12]</a>, <a
href="#ref-tartaro2023towards" role="doc-biblioref">[13]</a></span>.
Notably, the European Commission’s draft standardisation request <span
class="citation" data-cites="EC2022request"><a href="#ref-EC2022request"
role="doc-biblioref">[14]</a></span> refrains from calling for European
standards in relation to data, model, or AI system documentation <span
class="citation" data-cites="golpayegani2023high"><a
href="#ref-golpayegani2023high" role="doc-biblioref">[15]</a></span>.
Consequently, the European Standardisation Organisations’ work plan does
not include standards specifically addressing these aspects <span
class="citation" data-cites="soler2023analysis"><a
href="#ref-soler2023analysis" role="doc-biblioref">[16]</a></span>. In
parallel, the existing body of work on AI, model, and data documentation
approaches pays little attention to documentation for legal compliance,
fails to provide a strong connection between the documented information
and responsible AI implications, and does not take into consideration
aspects related to risk management <span class="citation"
data-cites="hupont2023documentinghighrisk heger2022understanding"><a
href="#ref-hupont2023documentinghighrisk" role="doc-biblioref">[11]</a>,
<a href="#ref-heger2022understanding"
role="doc-biblioref">[17]</a></span>.</p>
<p>Currently, there is a critical need for technical documentation to
support and be in sync with the AI system development and usage
practices. Moreover, with the involvement of several entities across the
AI value chain, the documentation must be generated and maintained in a
manner that ensures consistency and interoperability. This is also
crucial for investigation of technical documentation by AI auditors and
conformity assessment bodies for compliance checking and certification.
The motivation for this work is therefore to address this current need
for consistent, uniform, and interoperable specifications to support
effective implementation of the AI Act. This paper aims to address the
current lack of unified technical documentation practices aligned with
the AI Act, with a threefold contribution:</p>
<ol>
<li><p>we provide an in-depth analysis of the provisions of the EU AI
Act in regard to documentation with a focus on <em>technical
documentation</em> and <em>risk management system documentation</em>
(Section <a href="#AIA_documentation" data-reference-type="ref"
data-reference="AIA_documentation">3</a>);</p></li>
<li><p>we propose <strong><em>AI Cards</em></strong>—a novel framework
providing a human-readable overview of a given use of an AI system and
its risks (Section <a href="#AICards" data-reference-type="ref"
data-reference="AICards">4</a>); and</p></li>
<li><p>we present a machine-readable specification for AI Cards that
enables generating, maintaining, and updating documentation in sync with
AI development. It further supports querying and sharing information
needed for tasks such as compliance checking, comparing multiple AI
specifications for purposes such as AI procurement, and exchanging
information across the value chain (Section <a
href="#semantic_representation" data-reference-type="ref"
data-reference="semantic_representation">4.3</a>).</p></li>
</ol>
<p>We demonstrate a practical application of AI Cards using an
illustrative example of an AI-based proctoring system (Section <a
href="#use-case" data-reference-type="ref"
data-reference="use-case">5</a>), validate its usefulness through a
survey (Section <a href="#validation" data-reference-type="ref"
data-reference="validation">6</a>), and discuss its benefits (Section <a
href="#benefits" data-reference-type="ref"
data-reference="benefits">7</a>).</p>
<h2 id="relatedwork">Related Work</h2>
<h3
id="previous-studies-on-the-ai-acts-documentation-and-risk-management-requirements">Previous
Studies on the AI Act’s Documentation and Risk Management
Requirements</h3>
<p>Since the release of the AI Act’s proposal by the European Commission
in April 2021, the research community, international organisations, and
industrial actors have been exploring the new avenues it opened by
analysing the AI Act’s contents for suggesting clarifications,
identifying potential gaps, or giving critique. In this section, we
refer to some existing studies on the provisions of the AI Act in regard
to documentation and risk management.</p>
<p>Panigutti et al. explore the role of the AI Act’s transparency and
documentation requirements in addressing the opacity of high-risk AI
systems <span class="citation" data-cites="panigutti2023role"><a
href="#ref-panigutti2023role" role="doc-biblioref">[18]</a></span>.
Gyevnara et al. discuss compliance-oriented transparency required to
satisfy the AI Act’s requirements in regard to risk and quality
management systems <span class="citation"
data-cites="gyevnara2023get"><a href="#ref-gyevnara2023get"
role="doc-biblioref">[19]</a></span>. Schuett presents a comprehensive
analysis of the AI Act’s risk management provisions, without delving
into the details of risk management system documentation <span
class="citation" data-cites="schuett2023risk"><a
href="#ref-schuett2023risk" role="doc-biblioref">[20]</a></span>. Soler
et al.’s assessment of international standards in regard to addressing
AI Act’s requirements shows that these standards are insufficient to
meet the provisions of the AI Act related to risk management <span
class="citation" data-cites="soler2023analysis"><a
href="#ref-soler2023analysis" role="doc-biblioref">[16]</a></span>.</p>
<p>In regard to technical documentation (Art. 11 and Annex IV), the most
comprehensive studies, to the best of our knowledge, are the work of
Hupont et al. <span class="citation"
data-cites="hupont2023documentinghighrisk"><a
href="#ref-hupont2023documentinghighrisk"
role="doc-biblioref">[11]</a></span>, which identifies 20 information
elements needed in documentation of AI systems and their constituting
datasets, and the analysis of Annex IV provided by Golpayegani et al.
<span class="citation" data-cites="golpayegani2022airo"><a
href="#ref-golpayegani2022airo" role="doc-biblioref">[21]</a></span>,
which identifies 50 information elements. Nevertheless, both studies
remain short in being fully comprehensive in terms of covering both
technical and risk management system documentation requirements laid
down in Art. 11, Annex IV, and Art. 9. Building on our previous work,
presented in <span class="citation" data-cites="golpayegani2022airo"><a
href="#ref-golpayegani2022airo" role="doc-biblioref">[21]</a></span> and
<span class="citation" data-cites="hupont2023documentinghighrisk"><a
href="#ref-hupont2023documentinghighrisk"
role="doc-biblioref">[11]</a></span>, this paper provides an in-depth
analysis of technical documentation with a focus on risk management
system documentation.</p>
<h3
id="alignment-of-existing-documentation-practices-with-the-ai-act">Alignment
of Existing Documentation Practices with the AI Act</h3>
<p>Compliance with the AI Act’s documentation requirements requires
guidelines, standards, tools, and formats to assist with generation,
communication, and auditing of technical documentation. With
transparency being widely recognised as a key factor in implementing
trustworthy AI <span class="citation" data-cites="hlegtai"><a
href="#ref-hlegtai" role="doc-biblioref">[9]</a></span>, several
documentation frameworks proposed by the AI community, such as
Datasheets for Datasets <span class="citation"
data-cites="gebru2021datasheets"><a href="#ref-gebru2021datasheets"
role="doc-biblioref">[22]</a></span> and Model Cards <span
class="citation" data-cites="mitchell2019model"><a
href="#ref-mitchell2019model" role="doc-biblioref">[23]</a></span>, have
become <em>de-facto</em> practices. As these documentation approaches
are widely-adopted, a key question is the extent to which they could be
leveraged for regulatory compliance tasks. This question is investigated
by the studies mentioned in the following. Pistilli et al. discuss the
potential of Model Cards as a compliance tool and anticipate its
adoption—among other existing documentation practices originated from
the AI community—for compliance with the AI Act’s documentation
obligations <span class="citation" data-cites="pistilli2023stronger"><a
href="#ref-pistilli2023stronger" role="doc-biblioref">[24]</a></span>.
The work by Hupont et al. <span class="citation"
data-cites="hupont2023documentinghighrisk"><a
href="#ref-hupont2023documentinghighrisk"
role="doc-biblioref">[11]</a></span>, which investigates the 6 most
widely-used AI and data documentation approaches for their alignment
with the implementation of documentation provisions, concludes that AI
Factsheets <span class="citation" data-cites="arnold2019factsheets"><a
href="#ref-arnold2019factsheets" role="doc-biblioref">[25]</a></span>
offers a higher overall degree of information coverage, followed by
Model Cards and the AI Classification Framework proposed by the
Organization for Economic Cooperation and Development (OECD) <span
class="citation" data-cites="OECD"><a href="#ref-OECD"
role="doc-biblioref">[8]</a></span>. The research also demonstrates that
while data-related information elements are well-covered by most
documentation approaches, particularly Datasheets for Datasets <span
class="citation" data-cites="gebru2021datasheets"><a
href="#ref-gebru2021datasheets" role="doc-biblioref">[22]</a></span>,
the Dataset Nutrition Label <span class="citation"
data-cites="holland2020dataset"><a href="#ref-holland2020dataset"
role="doc-biblioref">[26]</a></span>, and the Accountability for Machine
Learning framework <span class="citation"
data-cites="hutchinson2021towards"><a href="#ref-hutchinson2021towards"
role="doc-biblioref">[27]</a></span>, they still do not cover technical
information needs related to AI systems. This finding is further
strengthened in a follow-up comparative analysis of 36 AI system, model,
and/or dataset documentation practices, which unravels the overall
alignment of documentation practices with transparency requirements of
the EU AI Act and other recent EU data and AI initiatives, and spots a
gap in representing the information related to AI systems in its
entirety and its context of use <span class="citation"
data-cites="micheli2023landscape"><a href="#ref-micheli2023landscape"
role="doc-biblioref">[28]</a></span>. Interestingly, to date, the only
documentation methodology conceived from design for the AI Act is
<em>Use Case Cards</em> <span class="citation"
data-cites="hupont2023usecasecards"><a
href="#ref-hupont2023usecasecards" role="doc-biblioref">[29]</a></span>,
which primarily focuses on documenting the intended use of AI systems to
assess their risk level as per the AI Act, without mentioning technical
information and risk management details.</p>
<h3 id="sota-machine-readable">Machine-Readable Data, Model, and AI
System Documentation</h3>
<p>While documentation approaches are acknowledged as instruments for
improving transparency, and in turn enhancing trustworthiness, there has
been little attention to the barriers in generation, maintenance,
assessment, and exchange of conventional text-based documentation.
Providing machine-readable specifications is an idea taken up by some
recent work to support adaptable and interoperable documentation. In the
area of data documentation, Open Datasheets<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>,
proposed by Roman et al., provides a metadata framework for documenting
open datasets in a machine-readable manner <span class="citation"
data-cites="roman2023open"><a href="#ref-roman2023open"
role="doc-biblioref">[30]</a></span>. For machine-understandable
documentation of machine learning models, Model Card Report Ontology
(MCRO)<a href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>, developed by Amith et al., offers
the metadata for the content of Model Card reports <span
class="citation" data-cites="amith2022semanticmodelcards"><a
href="#ref-amith2022semanticmodelcards"
role="doc-biblioref">[31]</a></span>. Linked Model and Data Cards (LMDC)
present a schema for integration of Model and Data Cards in a data space
to provide a holistic view of a model or an AI service <span
class="citation" data-cites="donald2023semantic"><a
href="#ref-donald2023semantic" role="doc-biblioref">[32]</a></span>. The
Realising Accountable Intelligent Systems (RAInS) ontology<a href="#fn4"
class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>,
created by Naja et al., models the information relevant to AI
accountability traces <span class="citation" data-cites="naja2022kg"><a
href="#ref-naja2022kg" role="doc-biblioref">[33]</a></span>.</p>
<p>In the context of documentation for regulatory compliance, the W3C
Data Privacy Vocabulary (DPV) <span class="citation"
data-cites="pandit2024data"><a href="#ref-pandit2024data"
role="doc-biblioref">[34]</a></span><a href="#fn5" class="footnote-ref"
id="fnref5" role="doc-noteref"><sup>5</sup></a> is developed based on
the requirements of the EU GDPR (General Data Protection Regulation) and
has been applied for representing Data Protection Impact Assessment
(DPIA) information <span class="citation" data-cites="pandit2022dpia"><a
href="#ref-pandit2022dpia" role="doc-biblioref">[35]</a></span>,
documenting data breach reports <span class="citation"
data-cites="pandit2023breach"><a href="#ref-pandit2023breach"
role="doc-biblioref">[36]</a></span>, and representing Register of
Processing Activities (ROPA) <span class="citation"
data-cites="ryan2022dpcat"><a href="#ref-ryan2022dpcat"
role="doc-biblioref">[37]</a></span>. In our previous work, we proposed
the AI Risk Ontology (AIRO)<a href="#fn6" class="footnote-ref"
id="fnref6" role="doc-noteref"><sup>6</sup></a> for describing AI
systems and their risks based on the AI Act and ISO 31000 family of
standards <span class="citation" data-cites="golpayegani2022airo"><a
href="#ref-golpayegani2022airo" role="doc-biblioref">[21]</a></span>. In
this work, AIRO is employed to provide the basis for the
machine-readable representation of the AI Cards. However, it is
important to note that ISO 31000 family of standards is not aligned with
the AI Act due to some fundamental differences <span class="citation"
data-cites="soler2023analysis"><a href="#ref-soler2023analysis"
role="doc-biblioref">[16]</a></span>. AIRO is used while awaiting future
standards to be developed in response to the European Commission’s
standardisation request. Therefore, it needs to be updated with future
publication of harmonised standards related to AI risk management.</p>
<h2 id="AIA_documentation">Provisions of the EU AI Act Regarding
Documentation of AI Systems</h2>
<p>Documentation of AI systems is highly tied to the principles of
transparency and accountability <span class="citation"
data-cites="hupont2023usecasecards naja2022kg"><a
href="#ref-hupont2023usecasecards" role="doc-biblioref">[29]</a>, <a
href="#ref-naja2022kg" role="doc-biblioref">[33]</a></span>, providing
an appropriate degree of information to different stakeholders to enable
several types of assessments. In a similar vein, documentation is
essential for assessing legal compliance. Figure <a
href="#fig:aiact_key_documentation" data-reference-type="ref"
data-reference="fig:aiact_key_documentation">1</a> shows the AI Act’s
documentation requirements for high-risk AI systems and illustrates how
they are related to each other. Within the AI Act, <em>technical
documentation</em> and <em>quality management system documentation</em>
are key documents in the conformity assessment procedure, containing
detailed information required for self- and third-party assessments.
Given the central role of technical documentation and presence of
related guidance on its content (Annex IV), this work focuses on
identifying its concrete information elements. However, among its
incorporating documents, our analysis expands on <em>risk management
system documentation</em> considering that the risk-centred nature of
the AI Act makes the risk management system requirement (Art. 9) taking
the lead in ensuring that the potential risks of high-risk AI systems
are reduced to an acceptable level. In the following, we focus on
identifying the information elements required to be featured in
technical (Section <a href="#technical_documentation"
data-reference-type="ref"
data-reference="technical_documentation">3.1</a>) and risk management
(Section <a href="#risk_management_documentation"
data-reference-type="ref"
data-reference="risk_management_documentation">3.2</a>)
documentation.</p>
<figure id="fig:aiact_key_documentation">
<img src="img/065-documentation.png">
<figcaption>High-risk AI systems’ documentation requirements as per the
AI Act.</figcaption>
</figure>
<h3 id="technical_documentation">Article 11 and Annex IV - Technical
Documentation</h3>
<p>As discussed earlier, technical documentation is a key artefact for
demonstrating compliance with the AI Act. According to Art. 11, it is
the <em>high-risk AI provider</em> who should draw up the technical
documentation <em>ex-ante</em> and keep it <em>updated</em>. From this
obligation, three critical challenges arise: (i) when development of AI
systems includes integrating third-party components into the system,
e.g. training data and pre-trained models, high-risk AI providers need
to dispense details regarding these components in technical
documentation, e.g. information about how data is labelled (Annex
IV(2)(d)). Thus, generating technical documentation to fulfill the
requirements of the AI Act can become a collective activity, which
requires communication and exchange of often sensitive and confidential
information. This opens up a myriad of questions concerning trust,
accountability, and liability, that lie beyond the scope of this
discussion. (ii) Real-world testing environments and sandboxes assist in
determining what might be difficult to ascertain ex-ante. However,
specificities of the <em>context of use</em>, which significantly
influence AI risks and impacts, might not be revealed until the system’s
deployment. This suggests a need for ex-ante involvement of potential
users and ex-post feedback loops. (iii) Finally, keeping technical
documentation up-to-date and re-assessing legal compliance are
hard-to-manage tasks. A key question here is “what changes do trigger
the update process?”. A conspicuous, albeit not comprehensive, answer is
<em>substantial modifications</em>—changes that affect the system’s
compliance with the Act or its intended purpose (Art. 3(23)). However,
lack of clarity regarding what modifications are deemed as substantial
adds to the complexities of this challenge.</p>
<p>To help with the generation and auditing of technical documentation,
the minimum set of its incorporating information elements is outlined in
Annex IV, which is subject to the European Commission’s potential
amendments (Art. 11(3)). Annex IV serves as a primary template wherein
information elements are described with varying degrees of detail, with
the majority articulated at a high-level. This implies a need for
harmonised standards to support the implementation of Art. 11. However,
as mentioned earlier, to the best of our knowledge there is no ongoing
standardisation activities to address this need. Aiming to provide
clarification on the content of technical documentation, we conducted an
in-depth analysis of Annex IV, with a particular emphasis on the risk
management system. The analysis is an initial step in establishing a
collective understanding of the technical documentation’s content and
could inspire the development of prospective European standards,
guidelines, and templates.</p>
<h3 id="risk_management_documentation">Article 9 - Risk Management
System Documentation</h3>
<p>The AI Act will rely on harmonised standards to operationalise legal
requirements at the technical level, and risk management system will be
no exception. At the time of writing, however, European Standardisation
Organisations are still working on deliverables planned in response to
the European Commission’s standardisation request. Furthermore, existing
ISO/IEC standards on AI risk management and AI management system, namely
ISO/IEC 23894<a href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a> and ISO/IEC 42001<a href="#fn8"
class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>,
have been found insufficient in meeting the requirements of the AI Act
<span class="citation" data-cites="soler2023analysis"><a
href="#ref-soler2023analysis" role="doc-biblioref">[16]</a></span>.</p>
<p>Indeed, within the current AI standardisation realm, the two main
international standards covering AI risk management aspects that have
been analysed in regard to AI risk management in the technical report
published by the European Commission’s Joint Research Centre <span
class="citation" data-cites="soler2023analysis"><a
href="#ref-soler2023analysis" role="doc-biblioref">[16]</a></span> are:
(i) <strong>ISO/IEC 42001 “Information technology — Artificial
intelligence — Management system”</strong> that offers a framework to
assist with implementation of AI management systems, and (ii)
<strong>ISO/IEC 23894 “Artificial intelligence — Guidance on risk
management”</strong>, which offers practical guidance on managing risks,
albeit defined as the effect of uncertainty, without relation to
potential harms to individuals. Due to this and other reasons, the
requirements defined in these standards are not useful to comply with
the provisions of the AI Act in regard to risk management <span
class="citation" data-cites="soler2023analysis"><a
href="#ref-soler2023analysis" role="doc-biblioref">[16]</a></span>,
however they may be useful to identify relevant technical information
elements for the development of AI Cards while we await harmonised
standards for the AI Act. In this section, we elaborate on information
elements for the documentation of an AI risk management system by
obtaining insights from these two standards.</p>
<p>We used the overall structure of AI management systems, which follows
the ISO/IEC’s <em>harmonised structure</em> for management system
standards<a href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a>, as a resource for extracting the
key organisational activities within an AI risk management system. For
each activity, documentation needs, in terms of information elements,
were identified from both ISO/IEC 42001 and 23894 (for a summary of the
analysis see Figure <a href="#fig:AIRMS" data-reference-type="ref"
data-reference="fig:AIRMS">2</a>). These information elements can be
categorised into four overall groups:</p>
<ul>
<li><p>Information about the <em>context of the AI system and the
organisation</em>, for example, the AI system’s intended purpose and the
role of the organisation in relation to the system.</p></li>
<li><p>Details of the <em>risk management system</em> in place, e.g. the
policies, responsibilities, and resources required for implementation of
the risk management system itself. This category of information is
relevant to the ISO/IEC 23894’s AI risk management framework, whose
intention is to help with AI governance and integration of AI risk
management activities into an organisation’s existing
processes.</p></li>
<li><p>Documentation of <em>risk management processes</em> across
different phases: planning (ex-ante), operation (ongoing), and
post-operation (ex-post).</p></li>
<li><p><em>Results of AI risk management</em>, which can be represented
in artefacts produced throughout the risk management process, e.g. risk
assessment documentation that lists AI risks identified, their
likelihood, severity, sources, consequences, and impacts.</p></li>
</ul>
<figure id="fig:AIRMS">
<img src="img/065-AIRMS.png">
<figcaption>Information elements identified for AI risk management
system documentation (note they are inspired by ISO/IEC 42001 and 23894,
and therefore will be updated according to the AI Act’s future official
harmonised standards).</figcaption>
</figure>
<h2 id="AICards">AI Cards</h2>
<p>Extensiveness of technical documentation and confidentiality of its
content may hinder collaboration and communication with AI
stakeholders—many of whom are not necessarily technical AI experts. To
address this challenge, we propose the <strong>AI Cards
framework</strong>, as a structured information sheet, which offers an
overview of a use of an AI system and its related trustworthy AI
concerns by inclusion of crucial information about technical
specification, context of use, risks, and compliance. With an intuitive
representation, the AI Cards framework introduces a simple, transparent,
and comprehensible summary card laying out a holistic picture of a
specific AI use case and its risks without disclosing sensitive
information. The framework encompasses machine-readable specifications
that enhance interoperability, enable automation, and support
extensibility.</p>
<h3 id="development-process">Development Process</h3>
<p>In shaping the framework, we first specified its requirements, which
are summarised in Table <a href="#tab:design" data-reference-type="ref"
data-reference="tab:design">1</a>. Based on the defined scope, we
re-examined our analysis of Annex IV. The information elements within
the scope were considered as candidates for inclusion in the AI Cards.
However, many of them were too detailed to be represented in a summary
card. Therefore, we propose condensed views for these information
elements through visual aids.</p>
<p>The AI Cards framework was defined and iteratively refined in
consultation with researchers involved in digital policymaking within
the European Commission and further validated by conducting an online
anonymous survey with law and technology researchers (see Section <a
href="#validation" data-reference-type="ref"
data-reference="validation">6</a> for more details).</p>
<div id="tab:design">
<table>
<caption>AI Cards framework requirements</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Purpose</th>
<th style="text-align: left;">Providing an overview of technical
documentation that effectively conveys key information</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Scope</td>
<td style="text-align: left;"><p>In the scope: AI system, as a whole,
its context of use, and information relevant to trustworthy AI concerns
including risk management system.</p>
<p>Out of the scope: organisational processes, details of management
systems, and documentation of AI components, including data and model.
The framework is horizontal and does not take sector-specific nuances
into account.</p></td>
</tr>
<tr class="even">
<td style="text-align: left;">Key audience</td>
<td style="text-align: left;">AI users, end-users, subjects, providers,
developers, auditors, and policymakers</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Representation formats</td>
<td style="text-align: left;">An easy-to-understand visual
human-readable representation accompanied with a machine-readable
specification</td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:design" label="tab:design"></span></p>
<h3 id="information-elements">Information Elements</h3>
<p>Figure <a href="#fig:AICards" data-reference-type="ref"
data-reference="fig:AICards">3</a> shows a visualisation of the AI Card
which condenses the information elements in 9 sections. To enable
representing AI Cards in a time series and to allow version control
required to reflect the evolving nature of AI systems, the Card’s
metadata includes essential information describing its
<strong>version</strong>, <strong>issuance date</strong>,
<strong>language</strong>, <strong>publisher</strong>, and
<strong>contact</strong> information. Further, the <strong>URL of the
machine-readable specification</strong> is included.</p>
<figure id="fig:AICards">
<img src="img/065-AICards.png">
<figcaption>Visual representation of AI Cards.</figcaption>
</figure>
<h3 id="general-information">(1) General Information</h3>
<p>This section provides the key information about the AI system
including its <strong>version</strong>, <strong>modality</strong>, e.g.
standalone software or safety component of a product, main <strong>AI
techniques</strong> used, <strong>provider(s)</strong>, and
<strong>developer(s)</strong>.</p>
<h3 id="intended-use">(2) Intended Use</h3>
<p>The AI Act put a considerable emphasis on intended use of the system,
given its profound effect on risks and impacts. For describing the
intended use, we propose using the combination of the 5 concepts
identified in <span class="citation" data-cites="golpayegani2023high"><a
href="#ref-golpayegani2023high" role="doc-biblioref">[15]</a></span>,
which are:</p>
<ul>
<li><p>the <strong>domain</strong> in which the system is intended to be
used within,</p></li>
<li><p>the <strong>purpose</strong>, i.e. end goal, of using the system
in a specific context,</p></li>
<li><p>the <strong>AI capability</strong> that enables realisation of
the purpose,</p></li>
<li><p>the <strong>AI deployer</strong> which is the entity using the AI
system,</p></li>
<li><p>the <strong>AI subject</strong> which is the entity subjected to
the outputs, e.g. decisions, generated by the AI system.</p></li>
</ul>
<p>Noteworthy, the subsequent sections heavily rely of the information
represented in this section, considering that they should be defined in
the view of the intended use.</p>
<h3 id="key-components">(3) Key Components</h3>
<p>Many AI systems are built through integration of multiple AI models,
datasets, general-purpose AI systems, and other software elements, each
of which has an effect on the system’s behaviour and in turn its risks
<span class="citation" data-cites="micheli2023landscape"><a
href="#ref-micheli2023landscape" role="doc-biblioref">[28]</a></span>.
This section provides the system’s <em>high-level architecture</em> in
terms of incorporating <strong>components</strong>. For each component,
its <strong>name</strong>, <strong>version</strong>, and <strong>link to
documentation or ID</strong> are presented. For detailed information
about components, we rely on their documentation provided presumably by
the components’ providers in the form of <strong>information
sheets</strong>, e.g. Datasheets, Model Cards, and AI Factsheets.</p>
<h3 id="data-processing">(4) Data Processing</h3>
<p>AI systems that do not process data, if they exist at all, are rare.
Within the EU digital acquis, the GDPR <span class="citation"
data-cites="gdpr"><a href="#ref-gdpr"
role="doc-biblioref">[38]</a></span>, which protects the fundamental
right to privacy by regulating <em>personal data</em>, is applicable to
those AI systems that process natural persons’ data. Therefore, having
knowledge of whether a given use of an AI system involves processing of
personal data is crucial to correctly interpret the resulting legal
compliance obligations. This section specifies inclusion of processing
<strong>personal data</strong> and shows the <strong>category</strong>
of the data, e.g. biometric data, and represents whether data protection
impact assessment <strong>(DPIA)</strong> is conducted. It further
specifies inclusion of <strong>non-personal</strong>,
<strong>anonymised</strong>, and <strong>licenced</strong> data.</p>
<h3 id="human-involvement">(5) Human Involvement</h3>
<p>Involvement can take different forms depending on the phase of AI
development, the role of human actors, and the system’s <strong>level of
automation</strong>—which has a range from fully autonomous to fully
human-controlled according to ISO/IEC 22989<a href="#fn10"
class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.
The level of automation also has a substantial effect on the safeguards,
including human oversight measures, required for controlling AI risks.
This section provides an overview of involvement of two specific human
actors: <strong>AI end-users</strong>, who use the AI system’s output
and <strong>AI subjects</strong>, who are subjected to the outputs of
the system. For these actors, we look into the following aspects of
involvement:</p>
<ul>
<li><p><strong>Intended involvement</strong>: represents whether the
involvement of a specific actor is as intended. An example of an
<em>intended</em> AI subject in an AI-based proctoring system is a
student sitting an online test. In this case, other persons present in
the room are <em>unintended</em> AI subjects.</p></li>
<li><p><strong>Active involvement</strong>: shows whether a specific
actor actively interacts with the AI system.</p></li>
<li><p><strong>Informed involvement</strong>: represents whether a
specific actor was informed that an AI system is in place; for example,
in cases where a decision affecting a person’s education is made using
AI-based solutions.</p></li>
<li><p><strong>Control over AI outputs</strong>: shows the extent to
which AI subjects and end-users have control over AI outputs, in
particular decisions made by the AI system and their impacts. Inspired
by OECD’s modes of operationality <span class="citation"
data-cites="OECD"><a href="#ref-OECD"
role="doc-biblioref">[8]</a></span>, we consider the following six
levels of control:</p>
<ul>
<li><p>An AI subject/end-user can opt in the system’s output.</p></li>
<li><p>An AI subject/end-user can opt out of the system’s
output.</p></li>
<li><p>An AI subject/end-user can challenge the system’s
output.</p></li>
<li><p>An AI subject/end-user can correct the system’s output.</p></li>
<li><p>An AI subject/end-user can reverse the system’s output
ex-post.</p></li>
<li><p>An AI subject/end-user cannot opt out of the system’s
output.</p></li>
</ul></li>
</ul>
<h3 id="risk-profile">(6) Risk Profile</h3>
<p>This section provides a high-level summary of risk management
results, which includes an overview of <strong>likelihood</strong>,
<strong>severity</strong> and <strong>residual risk</strong> associated
with risks that have impact on areas of <strong>health and
safety</strong>, <strong>fundamental rights</strong>,
<strong>society</strong>, and <strong>environment</strong>. Further,
this section shows whether any technical, monitoring (human oversight),
cybersecurity, transparency, and logging measures applied to control the
risks.</p>
<h3 id="quality">(7) Quality</h3>
<p>With many AI incidents rooted in poor quality, ensuring that the
system is of high quality by participating in AI regulatory sandboxes,
benchmarking, and performing tests is necessary. We propose illustrating
key <strong>AI system qualities</strong> using a radar chart. We
recommend including <strong>Accuracy</strong>,
<strong>robustness</strong>, and <strong>cybersecurity</strong>, which
are the key qualities explicitly mentioned in the Act (Art. 15).
Further, the relevant <em>product quality</em> and <em>quality in
use</em>, described by ISO/IEC 25059 on AI SQuaRE (Systems and software
Quality Requirements and Evaluation)<a href="#fn11" class="footnote-ref"
id="fnref11" role="doc-noteref"><sup>11</sup></a> can be considered to
be included in the quality section.</p>
<h3 id="pre-determined-changes">(8) Pre-determined Changes</h3>
<p>This section provides a list of <strong>pre-determined
changes</strong> to the system and its performance in terms of
<strong>subject</strong> and <strong>frequency</strong> of change as
well as the potential <strong>impacts of change on performance and
risks</strong>.</p>
<h3 id="regulations-certification">(9) Regulations &amp;
Certification</h3>
<p>This section lists the main digital <strong>regulations</strong> the
AI system is compliant with, key <strong>standards</strong> to which the
system or the provider(s) conform, and <strong>codes of conduct</strong>
followed in development or use of the AI system.</p>
<h3 id="semantic_representation">Machine-Readable Specification</h3>
<p>The visual representation assists stakeholders in gaining an
understanding of an AI system, its context, and the associated
trustworthy AI concerns without delving into the extensive details of
technical documentation. However, these are not sufficient to support
some of the desirable features of documentation, including search and
tracking capabilities, conducting meta-analysis, comparing multiple AI
systems, and automating generation and update of documentation <span
class="citation" data-cites="heger2022understanding"><a
href="#ref-heger2022understanding" role="doc-biblioref">[17]</a></span>.
To include these features, the AI Cards framework supports
machine-readable representation of information by leveraging the
standards, methods, and tools provided by the World Wide Web Consortium
(W3C)<a href="#fn12" class="footnote-ref" id="fnref12"
role="doc-noteref"><sup>12</sup></a>, motivated by the body of work
discussed in Section <a href="#sota-machine-readable"
data-reference-type="ref" data-reference="sota-machine-readable">2.3</a>
as well as the rise in uptake of open data formats for documentation,
reporting, and sharing information by the AI community, e.g.
HuggingFace’s use of JSON Model Cards<a href="#fn13"
class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>,
and by the EU, e.g. machine-readable regulatory reporting <span
class="citation" data-cites="eu2022mrer"><a href="#ref-eu2022mrer"
role="doc-biblioref">[39]</a></span>, DCAT-AP open data portals<a
href="#fn14" class="footnote-ref" id="fnref14"
role="doc-noteref"><sup>14</sup></a>, and EU vocabularies and
ontologies<a href="#fn15" class="footnote-ref" id="fnref15"
role="doc-noteref"><sup>15</sup></a>. The machine-readable
representation, which relies on Semantic Web technologies, fosters
openness and interoperability—both essential for exchanging information
across the AI value chain. This representation is extensible and
therefore enables accommodating sector-specific information requirements
and allows adaptation to highly-anticipated guidelines from authorities
including the AI office, the potential amendments to the Act via
delegated Acts, and case law. Grounded on formal logic, it also assists
in ensuring that the information is complete, correct, and
verifiable.</p>
<p>For provision of machine-readable specifications for AI Cards, an
<em>ontology</em> is needed to provide a common semantic model that
explicitly represents terms, their definitions, and the semantic
relations between them, ensuring consistency and interoperability. For
this, building upon our previous work <span class="citation"
data-cites="golpayegani2022airo"><a href="#ref-golpayegani2022airo"
role="doc-biblioref">[21]</a></span>, we further extend AIRO to support
modelling information elements featured within the AI Cards. This
extension borrows concepts and relations from Data Quality Vocabulary
(DQV)<a href="#fn16" class="footnote-ref" id="fnref16"
role="doc-noteref"><sup>16</sup></a> <span class="citation"
data-cites="albertoni2021dqv"><a href="#ref-albertoni2021dqv"
role="doc-biblioref">[40]</a></span> for expressing AI quality and Data
Privacy Vocabulary (DPV)<a href="#fn17" class="footnote-ref"
id="fnref17" role="doc-noteref"><sup>17</sup></a> <span class="citation"
data-cites="pandit2024data"><a href="#ref-pandit2024data"
role="doc-biblioref">[34]</a></span> for modelling personal data
processing. Figure <a href="#fig:semantic_model"
data-reference-type="ref" data-reference="fig:semantic_model">4</a>
depicts key classes and relations required for representing information
featured in each section of the AI Card.</p>
<figure id="fig:semantic_model">
<img src="img/065-extended-airo.jpeg">
<figcaption>An extension of AIRO for AI Cards.</figcaption>
</figure>
<p>With the ontology providing the schema, the information incorporated
in the AI Cards can be represented as an RDF graph. This opens the door
to leveraging the power of Semantic Web standards and technologies
across a variety of tasks, among them are:</p>
<ul>
<li><p>Querying to retrieve information about an AI system and
information related to demonstration and investigation of legal
compliance, for example verifying presence of measures to address
potential AI impacts on the fundamental right to non-discrimination in
support of compliance with Art. 9 of the AI Act. The information
retrieval is implemented using <code>SELECT</code> queries in the
SPARQL<a href="#fn18" class="footnote-ref" id="fnref18"
role="doc-noteref"><sup>18</sup></a>, which the W3C standardised query
language, as shown in <span class="citation"
data-cites="golpayegani2022airo"><a href="#ref-golpayegani2022airo"
role="doc-biblioref">[21]</a></span>.</p></li>
<li><p>Updating information about an AI application, for example when a
mitigation measure is no longer effective and is replaced with a new
measure, which is implemented using <code>DELETE/INSERT</code> SPARQL
queries.</p></li>
<li><p>Integrating information provided in documentation of
incorporating components (demonstrated in the “Key Componets” section of
the AI Card), for example a Model Card that documents a system’s
incorporating model, when provided in standardised linked data formats
such as JSON-LD or RDF.</p></li>
<li><p>Reasoning that might support conformity assessment and legal
compliance tasks, for example defining semantic rules to suggest the
regulatory risk profile associated with an AI application. Semantic
rule-checking using the Shapes Constraint Language (SHACL)<a
href="#fn19" class="footnote-ref" id="fnref19"
role="doc-noteref"><sup>19</sup></a>, which is a W3C standardised
language, is valuable in validation of information against a set of
rules (see prior work on determining high-risk AI applications using
SHACL <span class="citation" data-cites="golpayegani2023high"><a
href="#ref-golpayegani2023high"
role="doc-biblioref">[15]</a></span>).</p></li>
<li><p>Expressing intended use policies in agreements between different
parties in the AI value chain, for example agreements between AI
providers and deployers describing conditions of using or modifying an
AI application. In this, the Open Digital Rights Language (ODRL)<a
href="#fn20" class="footnote-ref" id="fnref20"
role="doc-noteref"><sup>20</sup></a>—the W3C recommended language for
representing policies—can be used for describing intended and precluded
uses as <em>permission</em> and <em>prohibition</em> statements
respectively.</p></li>
</ul>
<h2 id="use-case">Proof-of-Concept: AI Cards for an AI-Based Proctoring
System</h2>
<p>To demonstrate the potential scalability, and applicability of the
proposed framework, and inspired by the use cases described in <span
class="citation" data-cites="panigutti2023role"><a
href="#ref-panigutti2023role" role="doc-biblioref">[18]</a></span> and
<span class="citation" data-cites="hupont2023usecasecards"><a
href="#ref-hupont2023usecasecards" role="doc-biblioref">[29]</a></span>,
we take an AI-based student proctoring tool called <em>Proctify</em> as
an illustrative proof-of-concept. It is important to note that this
system is merely selected to demonstrate the applicability of the AI
Cards framework in real-world cases. The AI Act generally considers such
proctoring systems as <em>high-risk</em> in the education domain when
used for monitoring and detecting suspicious behaviour of students
during tests <span class="citation" data-cites="hupont2022landscape"><a
href="#ref-hupont2022landscape" role="doc-biblioref">[7]</a></span>.</p>
<p>Proctify is intended to detect suspicious behaviour during online
exams by analysing facial behaviour from a student’s facial video
captured throughout the exam using a webcam. Prior to this, students
have explicitly consented to be recorded during the exam and informed
that they must be alone in the room. The system incorporates a graphic
interface displaying an analysis of the student’s face including the
head pose, gaze direction, and face landmarks’ positions. This extracted
information is then provided as an input to <em>SusBehavedModel</em>,
which has been trained in-house by the system’s provider using
<em>SusBehavedDataset</em>, to determine whether the student is
displaying suspicious behaviour, e.g. looking away from the screen,
leaving the room, or a third person detected in the room. Detection of
suspicious behaviour raises an alarm in the interface to inform and let
the human oversight actors, e.g. human instructors, take appropriate
actions, e.g. communicating with the student.</p>
<p>Throughout the risk management process, the risks and impacts of the
system are identified and assessed by the provider, including the
following: the system may have lower accuracy for students with darker
skin tones and a higher rate of false-positive alarms for students
wearing glasses. Further, false-negatives and false-positives are more
frequent for students with health issues or disabilities that affect
their facial behaviour. There is also a chance of over-reliance of human
instructors on the system’s output (automation bias). These events have
the potential to negatively impact students’ <em>mental health</em>,
<em>future career</em>, and their rights to <em>dignity</em> and
<em>non-discrimination</em>. Some of the measures applied to address the
system’s risks and impacts are: ensuring the dataset is representative
and diverse in demographic terms, conducting rigours and frequent
testing of accuracy, assigning expert human proctors, and creating clear
protocols to act upon when an alarm is raised. The Proctify’s AI Card is
visualised in Figure <a href="#fig:AICard_proctify"
data-reference-type="ref" data-reference="fig:AICard_proctify">5</a> and
its machine-readable specification is available online<a href="#fn21"
class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>.
As shown in the figure, the visual representation of the AI Card
provides a summary of risk management information without disclosing the
details. However, the machine-readable specification is capable of
modelling the details.</p>
<figure id="fig:AICard_proctify">
<img src="img/065-proctify.png">
<figcaption>An example of AI Card for an AI-based proctoring
system.</figcaption>
</figure>
<h2 id="validation">Validation</h2>
<p>As mentioned earlier, to identify essential information elements of
the AI Cards framework, we consulted experts involved in EU digital
policymaking, in particular the AI Act, whose backgrounds are in AI
transparency and documentation, explainability, cybersecurity,
standardisation, and Semantic Web. Throughout the development process,
our analysis of the AI Act’s provisions, the visual representation of AI
Cards, and its machine-readable specification were discussed to ensure
alignment with EU digital policies, adoption of correct terminology, and
suitability of the AI Cards framework for addressing common concerns of
AI stakeholders. After reaching a solid structure for the AI Cards, an
online anonymous survey has been conducted to assess the usefulness of
the AI Cards framework.</p>
<h3 id="survey-design">Survey Design</h3>
<p>In addition to questions regarding the respondent’s background, the
survey includes questions about usefulness of (i) (only) the visual
representation, (ii) (only) the machine-readable representation, and
(iii) the overall AI Cards framework (both human- and machine-readable
representations). In the latter, to assess how stakeholders envisage
usability of the AI Card the System Usability Scale (SUS) <span
class="citation" data-cites="brooke1996sus"><a href="#ref-brooke1996sus"
role="doc-biblioref">[41]</a></span> is used. As the SUS is originally
designed for assessing perceived usability, minor wording changes were
applied to fit it with the purpose of AI Cards’ assessment and to
provide more clarity. Table <a href="#tab:sus" data-reference-type="ref"
data-reference="tab:sus">2</a> shows the list of SUS-based questions for
assessing the AI Cards’ usability, each answered on a 5-point Likert
scale ranging from 1 (strongly disagree) to 5 (strongly agree).</p>
<h3 id="recruitment">Recruitment</h3>
<p>In the first phase, a student cohort of participants (N=23) were
recruited from the Data Governance module in Dublin City University
attended by students enrolled in Masters of Art (MA) in Data Protection
and Privacy (MDPP) and the European Master in Law, Data and Artificial
Intelligence (EMILDAI). It is important to highlight that, because of
the nature of this module, some of the participants (30 percent) have
notable positions in public organisations, industry, and NGOs. It should
be noted that a second phase of evaluation with policymakers and
industry stakeholders is currently underway.</p>
<h3 id="preliminaries">Preliminaries</h3>
<p>Prior to carrying out the survey, it was reviewed and approved by the
ethics committee within the school of computer science and statistics of
Trinity College Dublin. To start, in a preliminary interactive session,
information about the AI Act and the AI Cards was presented to
participants and they were asked to sign the informed consent form.
Then, they could start completing the survey. It should be noted that
the participation in the survey was optional.</p>
<h3 id="findings">Findings</h3>
<p>23 responses were collected in this first phase of the evaluation
from the student cohort. Regarding the usefulness of the visual
representation, inclusion of the following information was suggested in
the survey results:</p>
<ul>
<li><p>information related to the registry process,</p></li>
<li><p>link to the GDPR compliance including summary of the requirements
for data processing and retention period of data if personal data
processed,</p></li>
<li><p>limitations of the system,</p></li>
<li><p>information regarding tests including tests, bugs, issues, and
date of testing,</p></li>
<li><p>a legend or key to explain the abbreviations.</p></li>
</ul>
<p>In the context of implementation and enforcement of the AI Act, the
participants agreed on the usefulness of the human-readable
representation in compliance checking, creating technical and risk
management system documentation, and creating guidelines. The
participants further referred to the following potential uses of the AI
Cards: <em>ensuring traceability and transparency</em>, <em>monitoring
the system</em>, <em>reviewing periods for pre-determined changes</em>,
<em>raising awareness</em>, and <em>explaining the core concepts to
stakeholders with different levels of AI literacy and technical
knowledge</em>. In regard to usefulness of AI Cards as a tool to
facilitate exchange of information regarding an AI system and its risks,
while all participants acknowledged its usefulness to some extent for
communication within the AI development ecosystem and with authorities,
a minority (4 out of 23) did not perceive it valuable for communication
with the public.</p>
<p>Regarding machine-readable specifications, there was general
consensus on its usefulness for automated tools, establishing a common
language, and structuring the EU registry of AI systems. Further, the
participants indicated that the machine-readable specifications could
assist in <em>determining the legal risk category</em>, <em>providing a
better understanding of models</em>, and <em>building models with better
compliance capabilities</em>.</p>
<p>Regarding the potential target users of the framework, one
participant referred to <em>“AI Brokers for comparing AI Systems - would
require more metrics but a good start for comparison”</em> and another
participant mentioned <em>“Members of the public, in fostering awareness
and understanding of AI systems”</em>. The results of SUS usability test
are shown in Table <a href="#tab:sus" data-reference-type="ref"
data-reference="tab:sus">2</a>, where due to the mixed tone of the items
a higher score for odd-numbered items and a lower score for
even-numbered items are desired. The average SUS score, calculated using
the formula proposed in <span class="citation"
data-cites="lewis2018sus"><a href="#ref-lewis2018sus"
role="doc-biblioref">[42]</a></span>, is 66.30.</p>
<div id="tab:sus">
<table>
<caption>SUS questions used for evaluating usability of AI
Cards</caption>
<thead>
<tr class="header">
<th style="text-align: left;">No.</th>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Score (mean)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: left;">I think that AI stakeholders would like to
use the AI Cards framework frequently.</td>
<td style="text-align: left;">4.21</td>
</tr>
<tr class="even">
<td style="text-align: left;">2</td>
<td style="text-align: left;">I find the AI Cards framework
unnecessarily complex.</td>
<td style="text-align: left;">2.13</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: left;">I think the AI Cards framework is easy to
use.</td>
<td style="text-align: left;">3.96</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">I think that AI stakeholders would need
the support of a technical person to be able to use the AI Cards
framework.</td>
<td style="text-align: left;">3.35</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5</td>
<td style="text-align: left;">I find the various aspects (i.e.
information elements and human- and machine-readable formats) in the AI
Cards framework are well integrated.</td>
<td style="text-align: left;">4.22</td>
</tr>
<tr class="even">
<td style="text-align: left;">6</td>
<td style="text-align: left;">I think there is too much inconsistency in
the AI Cards framework.</td>
<td style="text-align: left;">1.83</td>
</tr>
<tr class="odd">
<td style="text-align: left;">7</td>
<td style="text-align: left;">I would imagine that most people would
learn to use the AI Cards framework very quickly.</td>
<td style="text-align: left;">3.91</td>
</tr>
<tr class="even">
<td style="text-align: left;">8</td>
<td style="text-align: left;">I find the AI Cards framework very
cumbersome to use.</td>
<td style="text-align: left;">2.26</td>
</tr>
<tr class="odd">
<td style="text-align: left;">9</td>
<td style="text-align: left;">I feel very confident using the AI Cards
framework.</td>
<td style="text-align: left;">3.43</td>
</tr>
<tr class="even">
<td style="text-align: left;">10</td>
<td style="text-align: left;">I need to learn a lot of things before I
could get going with the AI Cards framework.</td>
<td style="text-align: left;">3.30</td>
</tr>
</tbody>
</table>
</div>
<p><span id="tab:sus" label="tab:sus"></span></p>
<p>Another key aspect in evaluation of effectiveness and usefulness of
an AI documentation framework is the extent of its adoption by the AI
community, as evidenced in widely-adopted documentation approaches, e.g.
Datasheets <span class="citation" data-cites="gebru2021datasheets"><a
href="#ref-gebru2021datasheets" role="doc-biblioref">[22]</a></span>,
Model Cards <span class="citation" data-cites="mitchell2019model"><a
href="#ref-mitchell2019model" role="doc-biblioref">[23]</a></span>, and
Factsheets <span class="citation" data-cites="arnold2019factsheets"><a
href="#ref-arnold2019factsheets" role="doc-biblioref">[25]</a></span>.
Given that the AI Act was only adopted recently and the final text has
not been published yet, the extent of AI Cards’ adoption will need to be
assessed over time. As indicated by one the participants, <em>“The real
world application would be interesting to see and may require changing
based on different Member State authorities requiring other information
be made available for documentation purposes”</em>.</p>
<h2 id="benefits">Benefits and Potential Applications</h2>
<p>The AI Cards framework distinguishes itself from existing
documentation approaches through:</p>
<ul>
<li><p>Its <strong>alignment with the EU AI Act’s provisions</strong> in
regard to technical and risk management system documentation. It should
be noted that though the framework is designed based on the requirements
of the EU AI Act, its modular and semantic nature makes it scalable to
be applied to document a wide range of AI applications, regardless of
the jurisdiction they are used in.</p></li>
<li><p>Its dual approach towards information representation, which makes
the framework <strong>accessible</strong> to both humans and machines,
facilitates communication, promotes <strong>interoperability</strong>,
and enables automation.</p></li>
<li><p>Its holistic <strong>AI system-focused</strong> approach, which
allows inclusion of information regarding the context of use, risk
management, and compliance in addition to technical specifications.
While most existing documentation practices address potential risks or
ethical issues at a very high and unstructured level <span
class="citation" data-cites="hupont2023documentinghighrisk"><a
href="#ref-hupont2023documentinghighrisk"
role="doc-biblioref">[11]</a></span>, an AI Card draws a picture of the
<strong>risk management system</strong> that is already established by
illustrating an overview of identified risks and applied mitigation
measures.</p></li>
<li><p>Its <strong>modular and future-proof</strong> design that enables
reconfiguration and extension of the framework to address documentation
requirements arising from forthcoming AI regulations, policies, and
standards.</p></li>
</ul>
<p>AI Cards’ machine-readable specification adds an extra level of
potential for application. In terms of documentation management, this
specification facilitates frequent modification, version control, and
integration of data from different sources. Further, it lays the ground
for development of supporting tools and RegTech solutions. Within the
context of the EU AI Act, these supporting tools can be employed to
assist <em>AI providers</em> in documentation and exchange of
information required for compliance, help <em>conformity assessment
bodies</em> in tasks related to auditing and certification, and assist
<em>AI deployers</em> in conducting fundamental rights impact assessment
(FRIA). Further, AI Cards might become useful for the <em>AI Office</em>
in the development of automated Semantic Web-based tools to assist with
FRIA, which has an overlap with technical documentation in terms of
information requirements. Currently, there is little guidance for how to
conduct risk assessment on the impact of AI systems on fundamental
rights and the state of the art in taking fundamental rights seriously
in the context of AI technical standardisation is new and fast evolving.
Therefore, the locating, consumption and reuse of examples of AI risks
assessment and accompanying technical documentation is urgently needed
to establish a public knowledge base from which AI providers and
deployers, especially less well-resourced SMEs and public bodies, can
draw legal certainty. Such a sharing of AI risks and the documentation
in AI card may therefore accelerate regulatory learning both between
regulatory bodies and between prospective AI providers and deployers.
Where confidentiality concern may impede the open access sharing of such
information, official regulatory learning structures such as sandboxes
and testing with human subjects could use this mechanism to maximise
sharing and learning between participating stakeholders. Shared
searchable repositories of AI risks and corresponding AI Cards may also
offer support in identifying risks of reasonably foreseeable misuse of
AI systems.</p>
<p>The semantic model, provided as an open-source ontology, can function
as a basis for a pan-European AI vocabulary<a href="#fn22"
class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>
to help establish a common language across different and
multidisciplinary actors involved in the AI ecosystem. This consistency
in the language, accompanied by a machine-readable representation,
promotes interoperability and streamlines the information exchange
required for incident reporting, compliance checking, and sharing best
practices. It also promotes broader participation of stakeholders needed
in development of standards for the AI Act <span class="citation"
data-cites="veale2021demystifying"><a href="#ref-veale2021demystifying"
role="doc-biblioref">[43]</a></span>. In addition, it can be utilised
for structuring the EU database of high-risk AI systems (Art. 60) and
incident reports (Art. 62). As an added advantage, the semantic model
can further be improved to evolve into a multilingual ontology
supporting official EU languages.</p>
<p>In a broader context, the semantic model is helpful for AI providers
and users that operate in different jurisdictions in addressing
challenges of cross-border compliance and interoperability by providing
an extensible and adaptable structure to maintain information. Provided
as an open resource, the schema can be reused and enhanced by the AI
community to fit their needs in regard to documentation and risk
management.</p>
<h2 id="conclusion-and-further-work">Conclusion and Further Work</h2>
<p>With this paper, we took an initial step to address the current lack
of standardised and interoperable AI documentation practices in
alignment with the EU AI Act. We proposed AI Cards, a novel framework
for documentation of uses of AI systems in two complementary human- and
machine-readable representations. We envision this contribution as a
valuable input for future EU policymaking and standardisation efforts
related to technical documentation, such as Implementing Acts, European
Commission-issued guidelines, and European standards. We also hope that
this work promotes use of standardised and interoperable Semantic
Web-based formats in AI documentation.</p>
<p>In an ongoing effort to enhance AI Cards’ usability and wider
applicability, an in-depth user experience study (second phase of the
evaluation) involving AI practitioners, auditors, standardisation
experts, and policymakers is underway. Our future work also takes the
direction towards development of automated tools for generating AI Cards
from users’ input and existing AI documents provided in textual formats.
Further, alignment of AI Cards with documentation and reporting
requirements of EU digital regulations, including the GDPR, Digital
Services Act (DSA) <span class="citation" data-cites="dsa"><a
href="#ref-dsa" role="doc-biblioref">[44]</a></span>, Interoperability
Act <span class="citation" data-cites="interoperabilityact"><a
href="#ref-interoperabilityact" role="doc-biblioref">[45]</a></span>,
and Data Governance Act (DGA) <span class="citation" data-cites="dga"><a
href="#ref-dga" role="doc-biblioref">[46]</a></span>, as well as
well-known risk management frameworks, e.g. NIST Artificial Intelligence
Risk Management Framework (AI RMF 1.0) <span class="citation"
data-cites="airmf"><a href="#ref-airmf"
role="doc-biblioref">[47]</a></span>, is a key future step in refining
the AI Cards.</p>
<div class="credits">
<hr/>
<p>This project has received funding from the European Union’s Horizon
2020 research and innovation programme under the Marie Skłodowska-Curie
grant agreement No 813497 (PROTECT ITN), as part of the ADAPT SFI Centre
for Digital Media Technology is funded by Science Foundation Ireland
through the SFI Research Centres Programme and is co-funded under the
European Regional Development Fund (ERDF) through
Grant#13/RC/2106_P2.</p>
<hr/>
<p>The views expressed in this article are purely those of the authors
and should not, under any circumstances, be regarded as an official
position of the European Commission.</p>
<hr/>
<h2 class="unnumbered" id="bibliography">References</h2>
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-gruetzemacher2022transformative" class="csl-entry"
role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">R.
Gruetzemacher and J. Whittlestone, <span>“The transformative potential
of artificial intelligence,”</span> <em>Futures</em>, vol. 135, p.
102884, 2022. </div>
</div>
<div id="ref-balahur2022data" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">A.
Balahur <em>et al.</em>, <span>“Data quality requirements for inclusive,
non-biased and trustworthy <span>AI</span>,”</span> 2022. </div>
</div>
<div id="ref-maragno2023exploring" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">G.
Maragno, L. Tangi, L. Gastaldi, and M. Benedetti, <span>“Exploring the
factors, affordances and constraints outlining the implementation of
artificial intelligence in public sector organizations,”</span>
<em>International Journal of Information Management</em>, vol. 73, p.
102686, 2023. </div>
</div>
<div id="ref-miron2021evaluating" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">M.
Miron, S. Tolan, E. Gómez, and C. Castillo, <span>“Evaluating causes of
algorithmic bias in juvenile criminal recidivism,”</span> <em>Artificial
Intelligence and Law</em>, vol. 29, no. 2, pp. 111–147, 2021. </div>
</div>
<div id="ref-araujo2020ai" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">T.
Araujo, N. Helberger, S. Kruikemeier, and C. H. De Vreese, <span>“In
<span>AI</span> we trust? Perceptions about automated decision-making by
artificial intelligence,”</span> <em>AI &amp; society</em>, vol. 35, pp.
611–623, 2020. </div>
</div>
<div id="ref-zhang2021ethics" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">Y.
Zhang, M. Wu, G. Y. Tian, G. Zhang, and J. Lu, <span>“Ethics and privacy
of artificial intelligence: Understandings from bibliometrics,”</span>
<em>Knowledge-Based Systems</em>, vol. 222, p. 106994, 2021. </div>
</div>
<div id="ref-hupont2022landscape" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">I.
Hupont, S. Tolan, H. Gunes, and E. Gómez, <span>“The landscape of facial
processing applications in the context of the european <span>AI</span>
act and the development of trustworthy systems,”</span> <em>Scientific
Reports</em>, vol. 12, no. 1, p. 10688, 2022. </div>
</div>
<div id="ref-OECD" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div
class="csl-right-inline">OECD, <span>“<span class="nocase">OECD
Framework for Classification of AI Systems: a tool for effective AI
policies</span>.”</span> Available: <a
href="https://oecd.ai/en/classification"
class="uri">https://oecd.ai/en/classification</a>, 2022. </div>
</div>
<div id="ref-hlegtai" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div
class="csl-right-inline">European Commission and Directorate-General for
Communications Networks, Content and Technology, <em>Ethics guidelines
for trustworthy <span>AI</span></em>. Publications Office, 2019
[Online]. Available: <a
href="https://data.europa.eu/doi/10.2759/346720">https://data.europa.eu/doi/10.2759/346720</a></div>
</div>
<div id="ref-aiact" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div
class="csl-right-inline"><span>“Proposal for a regulation of the
european parliament and of the council laying down harmonised rules on
artificial intelligence (artificial intelligence act) and amending
certain union legislative acts) and amending certain union legislative
acts.”</span> European Commission, April 2021 [Online]. Available: <a
href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206">https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206</a></div>
</div>
<div id="ref-hupont2023documentinghighrisk" class="csl-entry"
role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">I.
Hupont, M. Micheli, B. Delipetrev, E. Gómez, and J. S. Garrido,
<span>“Documenting high-risk <span>AI</span>: A european regulatory
perspective,”</span> <em>Computer</em>, vol. 56, no. 5, pp. 18–27, 2023.
</div>
</div>
<div id="ref-mazzini2023proposal" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">G.
Mazzini and S. Scalzo, <span>“The proposal for the artificial
intelligence act: Considerations around some key concepts,”</span>
<em>Camardi (a cura di), La via europea per l’Intelligenza
artificiale</em>, 2023. </div>
</div>
<div id="ref-tartaro2023towards" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">A.
Tartaro, <span>“Towards european standards supporting the AI act:
Alignment challenges on the path to trustworthy <span>AI</span>,”</span>
in <em>Proceedings of the AISB convention</em>, 2023, pp. 98–106. </div>
</div>
<div id="ref-EC2022request" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">E.
Commission, <span>“Draft standardisation request to the european
standardisation organisations in support of safe and trustworthy
artificial intelligence,”</span> 2022. [Online]. Available: <a
href="https://ec.europa.eu/docsroom/documents/52376">https://ec.europa.eu/docsroom/documents/52376</a>.
[Accessed: 07-Dec-2023]</div>
</div>
<div id="ref-golpayegani2023high" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">D.
Golpayegani, H. J. Pandit, and D. Lewis, <span>“To be high-risk, or not
to be—semantic specifications and implications of the <span>AI</span>
act’s high-risk <span>AI</span> applications and harmonised
standards,”</span> in <em>Proceedings of the 2023 ACM conference on
fairness, accountability, and transparency</em>, 2023, pp. 905–915.
</div>
</div>
<div id="ref-soler2023analysis" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">J.
Soler Garrido <em>et al.</em>, <span>“Analysis of the preliminary AI
standardisation work plan in support of the AI act,”</span> Joint
Research Centre (Seville site), 2023. </div>
</div>
<div id="ref-heger2022understanding" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">A.
K. Heger, L. B. Marquis, M. Vorvoreanu, H. Wallach, and J. Wortman
Vaughan, <span>“Understanding machine learning practitioners’ data
documentation perceptions, needs, challenges, and desiderata,”</span>
<em>Proceedings of the ACM on Human-Computer Interaction</em>, vol. 6,
no. CSCW2, pp. 1–29, 2022. </div>
</div>
<div id="ref-panigutti2023role" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">C.
Panigutti <em>et al.</em>, <span>“The role of explainable
<span>AI</span> in the context of the <span>AI</span> act,”</span> in
<em>Proceedings of the 2023 ACM conference on fairness, accountability,
and transparency</em>, 2023, pp. 1139–1150. </div>
</div>
<div id="ref-gyevnara2023get" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">B.
Gyevnara, N. Fergusona, and B. Schaferb, <span>“Get your act together: A
comparative view on transparency in the <span>AI</span> act and
technology,”</span> <em>arXiv preprint arXiv:2302.10766</em>, 2023.
</div>
</div>
<div id="ref-schuett2023risk" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">J.
Schuett, <span>“Risk management in the artificial intelligence
act,”</span> <em>European Journal of Risk Regulation</em>, pp. 1–19,
2023. </div>
</div>
<div id="ref-golpayegani2022airo" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">D.
Golpayegani, H. J. Pandit, and D. Lewis, <span>“<span>AIRO</span>: An
ontology for representing <span>AI</span> risks based on the proposed
<span>EU</span> <span>AI</span> act and <span>ISO</span> risk management
standards,”</span> <em>Towards a Knowledge-Aware <span>AI</span></em>,
pp. 51–65, 2022. </div>
</div>
<div id="ref-gebru2021datasheets" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">T.
Gebru <em>et al.</em>, <span>“Datasheets for datasets,”</span>
<em>Communications of the ACM</em>, vol. 64, no. 12, pp. 86–92, 2021.
</div>
</div>
<div id="ref-mitchell2019model" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">M.
Mitchell <em>et al.</em>, <span>“Model cards for model
reporting,”</span> in <em>Proceedings of the conference on fairness,
accountability, and transparency</em>, 2019, pp. 220–229. </div>
</div>
<div id="ref-pistilli2023stronger" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">G.
Pistilli, C. Muñoz Ferrandis, Y. Jernite, and M. Mitchell,
<span>“Stronger together: On the articulation of ethical charters, legal
tools, and technical documentation in ML,”</span> in <em>Proceedings of
the 2023 ACM conference on fairness, accountability, and
transparency</em>, 2023, pp. 343–354. </div>
</div>
<div id="ref-arnold2019factsheets" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">M.
Arnold <em>et al.</em>, <span>“<span>FactSheets</span>: Increasing trust
in <span>AI</span> services through supplier’s declarations of
conformity,”</span> <em>IBM Journal of Research and Development</em>,
vol. 63, no. 4/5, pp. 6–1, 2019. </div>
</div>
<div id="ref-holland2020dataset" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">S.
Holland, A. Hosny, S. Newman, J. Joseph, and K. Chmielinski, <span>“The
dataset nutrition label,”</span> <em>Data Protection and Privacy</em>,
vol. 12, no. 12, p. 1, 2020. </div>
</div>
<div id="ref-hutchinson2021towards" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">B.
Hutchinson <em>et al.</em>, <span>“Towards accountability for machine
learning datasets: Practices from software engineering and
infrastructure,”</span> in <em>ACM conference on fairness,
accountability, and transparency</em>, 2021, pp. 560–575. </div>
</div>
<div id="ref-micheli2023landscape" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">M.
Micheli, I. Hupont, B. Delipetrev, and J. Soler-Garrido, <span>“The
landscape of data and <span>AI</span> documentation approaches in the
european policy context,”</span> <em>Ethics and Information
Technology</em>, vol. 25, no. 4, p. 56, 2023. </div>
</div>
<div id="ref-hupont2023usecasecards" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline">I.
Hupont, D. Fernández-Llorca, S. Baldassarri, and E. Gómez, <span>“Use
case cards: A use case reporting framework inspired by the european
<span>AI</span> act,”</span> <em>Ethics and Information Technology</em>,
vol. 26, no. 2, 2024. </div>
</div>
<div id="ref-roman2023open" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">A.
C. Roman <em>et al.</em>, <span>“Open datasheets: Machine-readable
documentation for open datasets and responsible <span>AI</span>
assessments,”</span> <em>arXiv preprint arXiv:2312.06153</em>, 2023.
</div>
</div>
<div id="ref-amith2022semanticmodelcards" class="csl-entry"
role="listitem">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">M.
T. Amith <em>et al.</em>, <span>“Toward a standard formal semantic
representation of the model card report,”</span> <em>BMC
bioinformatics</em>, vol. 23, no. 6, pp. 1–18, 2022. </div>
</div>
<div id="ref-donald2023semantic" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">A.
Donald <em>et al.</em>, <span>“Towards a semantic approach for linked
dataspace, model and data cards,”</span> in <em>Companion proceedings of
the ACM web conference 2023</em>, 2023, pp. 1468–1473. </div>
</div>
<div id="ref-naja2022kg" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">I.
Naja, M. Markovic, P. Edwards, W. Pang, C. Cottrill, and R. Williams,
<span>“Using knowledge graphs to unlock practical collection,
integration, and audit of <span>AI</span> accountability
information,”</span> <em>IEEE Access</em>, vol. 10, pp. 74383–74411,
2022. </div>
</div>
<div id="ref-pandit2024data" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">H.
J. Pandit, B. Esteves, G. P. Krog, P. Ryan, D. Golpayegani, and J.
Flake, <span>“Data privacy vocabulary (<span>DPV</span>)–version
2,”</span> <em>arXiv preprint arXiv:2404.13426</em>, 2024. </div>
</div>
<div id="ref-pandit2022dpia" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">H.
J. Pandit, <span>“A semantic specification for data protection impact
assessments (<span>DPIA</span>),”</span> in <em>Towards a
knowledge-aware AI: SEMANTiCS 2022—proceedings of the 18th international
conference on semantic systems, 13-15 september 2022, vienna,
austria</em>, IOS Press, 2022, pp. 36–50. </div>
</div>
<div id="ref-pandit2023breach" class="csl-entry" role="listitem">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">H.
J. Pandit, P. Ryan, G. P. Krog, M. Crane, and R. Brennan, <span>“Towards
a semantic specification for <span>GDPR</span> data breach
reporting,”</span> in <em>Legal knowledge and information systems</em>,
IOS Press, 2023, pp. 131–136. </div>
</div>
<div id="ref-ryan2022dpcat" class="csl-entry" role="listitem">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline">P.
Ryan, R. Brennan, and H. J. Pandit, <span>“<span>DPCat</span>:
Specification for an interoperable and machine-readable data processing
catalogue based on <span>GDPR</span>,”</span> <em>Information</em>, vol.
13, no. 5, p. 244, 2022. </div>
</div>
<div id="ref-gdpr" class="csl-entry" role="listitem">
<div class="csl-left-margin">[38] </div><div
class="csl-right-inline"><span>“Regulation (EU) 2016/679 of the european
parliament and of the council of 27 april 2016 on the protection of
natural persons with regard to the processing of personal data and on
the free movement of such data, and repealing directive 95/46/EC
(general data protection regulation),”</span> <em>Official Journal of
the European Union</em>, vol. L119. 2016 [Online]. Available: <a
href="http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ:L:2016:119:TOC">http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ:L:2016:119:TOC</a></div>
</div>
<div id="ref-eu2022mrer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[39] </div><div
class="csl-right-inline">European Commission and Directorate-General for
Financial Stability, Financial Services and Capital Markets Union,
<em><a href="https://doi.org/doi/10.2874/036007">MRER proof of concept –
assessing the feasibility of machine-readable and executable reporting
for EMIR</a></em>. Publications Office of the European Union, 2022.
</div>
</div>
<div id="ref-albertoni2021dqv" class="csl-entry" role="listitem">
<div class="csl-left-margin">[40] </div><div class="csl-right-inline">R.
Albertoni and A. Isaac, <span>“Introducing the data quality vocabulary
(<span>DQV</span>),”</span> <em>Semantic Web</em>, vol. 12, no. 1, pp.
81–97, 2021. </div>
</div>
<div id="ref-brooke1996sus" class="csl-entry" role="listitem">
<div class="csl-left-margin">[41] </div><div class="csl-right-inline">J.
Brooke, <span>“<span>SUS</span>-a <span>‘quick and dirty’</span>
usability scale,”</span> <em>Usability evaluation in industry</em>, pp.
189–194, 1996. </div>
</div>
<div id="ref-lewis2018sus" class="csl-entry" role="listitem">
<div class="csl-left-margin">[42] </div><div class="csl-right-inline">J.
R. Lewis, <span>“The system usability scale: Past, present, and
future,”</span> <em>International Journal of Human–Computer
Interaction</em>, vol. 34, no. 7, pp. 577–590, 2018, doi: <a
href="https://doi.org/10.1080/10447318.2018.1455307">10.1080/10447318.2018.1455307</a>.
</div>
</div>
<div id="ref-veale2021demystifying" class="csl-entry" role="listitem">
<div class="csl-left-margin">[43] </div><div class="csl-right-inline">M.
Veale and F. Zuiderveen Borgesius, <span>“Demystifying the draft
<span>EU</span> artificial intelligence act—analysing the good, the bad,
and the unclear elements of the proposed approach,”</span> <em>Computer
Law Review International</em>, vol. 22, no. 4, pp. 97–112, 2021. </div>
</div>
<div id="ref-dsa" class="csl-entry" role="listitem">
<div class="csl-left-margin">[44] </div><div
class="csl-right-inline"><span>“Regulation (EU) 2022/2065 of the
european parliament and of the council of 19 october 2022 on a single
market for digital services and amending directive 2000/31/EC (digital
services act).”</span> October 2022 [Online]. Available: <a
href="http://data.europa.eu/eli/reg/2022/2065/oj">http://data.europa.eu/eli/reg/2022/2065/oj</a></div>
</div>
<div id="ref-interoperabilityact" class="csl-entry" role="listitem">
<div class="csl-left-margin">[45] </div><div
class="csl-right-inline"><span>“Proposal for a regulation of the
european parliament and of the council laying down measures for a high
level of public sector interoperability across the union (interoperable
europe act).”</span> European Commission, November 2022 [Online].
Available: <a
href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52022PC0720">https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52022PC0720</a></div>
</div>
<div id="ref-dga" class="csl-entry" role="listitem">
<div class="csl-left-margin">[46] </div><div
class="csl-right-inline"><span>“Regulation (EU) 2022/868 of the european
parliament and of the council of 30 may 2022 on european data governance
and amending regulation (EU) 2018/1724 (data governance act).”</span>
May 2022 [Online]. Available: <a
href="http://data.europa.eu/eli/reg/2022/868/oj">http://data.europa.eu/eli/reg/2022/868/oj</a></div>
</div>
<div id="ref-airmf" class="csl-entry" role="listitem">
<div class="csl-left-margin">[47] </div><div
class="csl-right-inline">National Institute of Standards and Technology,
<span>“Artificial intelligence risk management framework (<span>AI RMF
1.0</span>).”</span> 2023 [Online]. Available: <a
href="https://doi.org/10.6028/NIST.AI.100-1">https://doi.org/10.6028/NIST.AI.100-1</a></div>
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>For this work, we examined multiple AI Act mandates
published since April 2021, in particular the agreed provisional text.
However, the references to the AI Act within this paper shall be
interpreted as references to the European Commission’s proposal; as at
the time of writing, the final content of the AI Act is not published in
the official journal of the European Union.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a
href="https://github.com/microsoft/opendatasheets-framework"
class="uri">https://github.com/microsoft/opendatasheets-framework</a><a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://github.com/UTHealth-Ontology/MCRO"
class="uri">https://github.com/UTHealth-Ontology/MCRO</a><a
href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://w3id.org/rains"
class="uri">https://w3id.org/rains</a><a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://w3id.org/dpv/"
class="uri">https://w3id.org/dpv/</a><a href="#fnref5"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><a href="https://w3id.org/airo"
class="uri">https://w3id.org/airo</a><a href="#fnref6"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><a href="https://www.iso.org/standard/77304.html"
class="uri">https://www.iso.org/standard/77304.html</a><a href="#fnref7"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><a href="https://www.iso.org/standard/81230.html"
class="uri">https://www.iso.org/standard/81230.html</a><a href="#fnref8"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a
href="https://www.iso.org/sites/directives/current/consolidated/index.html"
class="uri">https://www.iso.org/sites/directives/current/consolidated/index.html</a><a
href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p><a href="https://www.iso.org/standard/74296.html"
class="uri">https://www.iso.org/standard/74296.html</a><a
href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><a href="https://www.iso.org/standard/80655.html"
class="uri">https://www.iso.org/standard/80655.html</a><a
href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p><a href="https://www.w3.org/"
class="uri">https://www.w3.org/</a><a href="#fnref12"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p><a href="https://huggingface.co/docs/hub/model-cards"
class="uri">https://huggingface.co/docs/hub/model-cards</a><a
href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p><a
href="https://op.europa.eu/en/web/eu-vocabularies/dcat-ap"
class="uri">https://op.europa.eu/en/web/eu-vocabularies/dcat-ap</a><a
href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p><a
href="https://op.europa.eu/en/web/eu-vocabularies/controlled-vocabularies"
class="uri">https://op.europa.eu/en/web/eu-vocabularies/controlled-vocabularies</a><a
href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p><a href="https://www.w3.org/ns/dqv"
class="uri">https://www.w3.org/ns/dqv</a><a href="#fnref16"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p><a href="https://w3id.org/dpv/"
class="uri">https://w3id.org/dpv/</a><a href="#fnref17"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p><a href="https://www.w3.org/TR/sparql11-query/"
class="uri">https://www.w3.org/TR/sparql11-query/</a><a href="#fnref18"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p><a href="https://www.w3.org/TR/shacl/"
class="uri">https://www.w3.org/TR/shacl/</a><a href="#fnref19"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p><a href="https://www.w3.org/TR/odrl-model/"
class="uri">https://www.w3.org/TR/odrl-model/</a><a href="#fnref20"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p><a
href="https://github.com/DelaramGlp/airo/blob/main/usecase/proctify.ttl"
class="uri">https://github.com/DelaramGlp/airo/blob/main/usecase/proctify.ttl</a><a
href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>See existing examples of EU vocabularies here: <a
href="https://op.europa.eu/en/web/eu-vocabularies"
class="uri">https://op.europa.eu/en/web/eu-vocabularies</a><a
href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

<section>
    <h4>Prior Publication Attempts</h4>
    <p>This paper was published after 2 attempts. Before being accepted at this venue, it was submitted to: FAccT</p>
</section>        </div>
    </article>
    </main>
    <footer>
        <a href="/me">About Me</a> | <a href="/contact">Contact</a> | <a rel="me" href="https://eupolicy.social/@harsh">Mastodon</a> | privacy policy n/a | license: <a class="no-reformat" rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">CC bY-NC 4.0</a><br/>
        Made using <a href="https://www.w3.org/TR/rdf11-concepts/">RDF</a>, <a href="https://www.w3.org/TR/sparql11-query/">SPARQL</a>, and <a href="https://www.python.org/">Python</a> - <a href="https://github.com/coolharsh55/harshp.com/">source on GitHub</a>
    </footer>
    <script src="/js/utils.js"></script>
</body>
</html>